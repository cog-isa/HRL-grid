# ¬ данном коде реализован алгоритм из статьи Automatic Discovery of Subgoals inReinforcement Learning using Diverse Density:

https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1017&context=cs_faculty_pubs

¬ mcgovern.py описаны два класса, класс Game с описанием среды, и класс agent с описанием параметров и свойств агента

## «апускаетс€ скрипт запуском файла train.py

ѕри его открытии сразу начинаетс€ обучение с параметрами, описанными в train.py, при желании их можно изменить.
ѕри обучении всплывает окно "Grid World", где показываетс€ текущии стадии обучени€.

∆елтым обозначаетс€ цель, куда должен прийти агент, красным подцель, котора€ находитс€ через Diverse Density.

—иним обозначаетс€ сам агент.

¬ консоле пишутс€ сообщени€ типа 4 3 position [0, 1, 2, 3] [-0.18281843807147813, -0.18395445100610527, -0.17562076331583998, -0.18189855164936664] 
√де 4 3 position это текуща€ позици€ агента, [0, 1, 2, 3] это все доступные действи€ из этой позиции и последний массив содержит Q-values, исход€ из которых и выбираетс€ следующее действие

ƒействие 0 - вверх
	 1 - вниз
	 2 - влево
	 3 - вправо


¬ train.py дл€ создани€ агента с сеткой 10 на 10 (можно поставить любую) создаетс€ 

	ag = agent(10,10)

«атем надо указать стенку, она описываетс€ 3 параметрами, 1 - на каком стобце эта стенка 2 - с какой строчки начинаетс€ просвет, 3 - на какой строчке заканчиваетс€ просвет

	ag.stenka(5,4,6)

ѕосле задаютс€ координаты цели 
	
	ag.tcel(8,8)

ƒалее нужно инициализировать начальные значени€ Q-table дл€ выше выбранной среды

	ag.init2()

ѕосле этого агент ag готов дл€ дальнейшего обучени€
