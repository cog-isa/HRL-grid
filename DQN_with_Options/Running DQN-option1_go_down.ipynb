{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "import plotting\n",
    "\n",
    "import dqn\n",
    "from dqn_utils import *\n",
    "#from atari_wrappers import *\n",
    "#from environments.arm_env.arm_env import ArmEnv\n",
    "from arm_env_dqn_go_down import ArmEnvDQN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def arm_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=256,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def arm_learn(env, session, num_timesteps):\n",
    "    # This is just a rough estimate\n",
    "    num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "    lr_multiplier = 1.0\n",
    "    lr_schedule = PiecewiseSchedule([\n",
    "                                         (0,                   1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                    ],\n",
    "                                    outside_value=5e-5 * lr_multiplier)\n",
    "    optimizer = dqn.OptimizerSpec(\n",
    "        constructor=tf.train.AdamOptimizer,\n",
    "        kwargs=dict(epsilon=1e-4),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "\n",
    "    def stopping_criterion(env, t):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return t >= num_timesteps\n",
    "\n",
    "    exploration_schedule = PiecewiseSchedule(\n",
    "        [\n",
    "            (0, 1.0),\n",
    "            (8e3, 0.3),\n",
    "            (num_iterations, 0.01),\n",
    "        ], outside_value=0.01\n",
    "    )\n",
    "\n",
    "    dqn.learn(\n",
    "        env,\n",
    "        q_func=arm_model,\n",
    "        optimizer_spec=optimizer,\n",
    "        session=session,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=1000000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        learning_starts=5000,\n",
    "        learning_freq=1,\n",
    "        frame_history_len=1,\n",
    "        target_update_freq=200,\n",
    "        grad_norm_clipping=10\n",
    "    )\n",
    "    \n",
    "    ep_rew = env.get_episode_rewards()\n",
    "    ep_len = env.get_episode_lengths()\n",
    "    env.close()\n",
    "    return ep_rew, ep_len\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def set_global_seeds(i):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        pass\n",
    "    else:\n",
    "        tf.set_random_seed(i) \n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "def get_session():\n",
    "    tf.reset_default_graph()\n",
    "#     tf_config = tf.ConfigProto(\n",
    "#         inter_op_parallelism_threads=1,\n",
    "#         intra_op_parallelism_threads=1)\n",
    "    session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    print(\"AVAILABLE GPUS: \", get_available_gpus())\n",
    "    session = tf.Session()\n",
    "    return session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE GPUS:  []\n",
      "Timestep 5500\n",
      "mean reward (50 episodes) 88.740000\n",
      "mean length (50 episodes) 11.260000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 38.000000\n",
      "max_episode_length (50 episodes) 62.000000\n",
      "best mean reward 95.000000\n",
      "episodes 173\n",
      "exploration 0.518750\n",
      "learning_rate 0.000099\n",
      "\n",
      "\n",
      "Timestep 6000\n",
      "mean reward (50 episodes) 91.660000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 95.000000\n",
      "episodes 229\n",
      "exploration 0.475000\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 6500\n",
      "mean reward (50 episodes) 93.580000\n",
      "mean length (50 episodes) 6.420000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 84.000000\n",
      "max_episode_length (50 episodes) 16.000000\n",
      "best mean reward 95.000000\n",
      "episodes 302\n",
      "exploration 0.431250\n",
      "learning_rate 0.000096\n",
      "\n",
      "\n",
      "Timestep 7000\n",
      "mean reward (50 episodes) 93.940000\n",
      "mean length (50 episodes) 6.060000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 83.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 95.000000\n",
      "episodes 384\n",
      "exploration 0.387500\n",
      "learning_rate 0.000095\n",
      "\n",
      "\n",
      "Timestep 7500\n",
      "mean reward (50 episodes) 93.600000\n",
      "mean length (50 episodes) 6.400000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 84.000000\n",
      "max_episode_length (50 episodes) 16.000000\n",
      "best mean reward 95.000000\n",
      "episodes 459\n",
      "exploration 0.343750\n",
      "learning_rate 0.000094\n",
      "\n",
      "\n",
      "Timestep 8000\n",
      "mean reward (50 episodes) 94.100000\n",
      "mean length (50 episodes) 5.900000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.000000\n",
      "episodes 547\n",
      "exploration 0.300000\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 8500\n",
      "mean reward (50 episodes) 94.920000\n",
      "mean length (50 episodes) 5.080000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 95.360000\n",
      "episodes 645\n",
      "exploration 0.296548\n",
      "learning_rate 0.000091\n",
      "\n",
      "\n",
      "Timestep 9000\n",
      "mean reward (50 episodes) 89.500000\n",
      "mean length (50 episodes) 10.500000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 23.000000\n",
      "max_episode_length (50 episodes) 77.000000\n",
      "best mean reward 95.360000\n",
      "episodes 690\n",
      "exploration 0.293095\n",
      "learning_rate 0.000090\n",
      "\n",
      "\n",
      "Timestep 9500\n",
      "mean reward (50 episodes) 82.340000\n",
      "mean length (50 episodes) 17.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 23.000000\n",
      "max_episode_length (50 episodes) 77.000000\n",
      "best mean reward 95.360000\n",
      "episodes 708\n",
      "exploration 0.289643\n",
      "learning_rate 0.000089\n",
      "\n",
      "\n",
      "Timestep 10000\n",
      "mean reward (50 episodes) 80.100000\n",
      "mean length (50 episodes) 19.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 4.000000\n",
      "max_episode_length (50 episodes) 96.000000\n",
      "best mean reward 95.360000\n",
      "episodes 741\n",
      "exploration 0.286190\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 10500\n",
      "mean reward (50 episodes) 94.240000\n",
      "mean length (50 episodes) 5.760000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 88.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 95.360000\n",
      "episodes 825\n",
      "exploration 0.282738\n",
      "learning_rate 0.000086\n",
      "\n",
      "\n",
      "Timestep 11000\n",
      "mean reward (50 episodes) 94.440000\n",
      "mean length (50 episodes) 5.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 88.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 95.360000\n",
      "episodes 916\n",
      "exploration 0.279286\n",
      "learning_rate 0.000085\n",
      "\n",
      "\n",
      "Timestep 11500\n",
      "mean reward (50 episodes) 94.900000\n",
      "mean length (50 episodes) 5.100000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1012\n",
      "exploration 0.275833\n",
      "learning_rate 0.000084\n",
      "\n",
      "\n",
      "Timestep 12000\n",
      "mean reward (50 episodes) 94.600000\n",
      "mean length (50 episodes) 5.400000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1100\n",
      "exploration 0.272381\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 12500\n",
      "mean reward (50 episodes) 94.540000\n",
      "mean length (50 episodes) 5.460000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 87.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1196\n",
      "exploration 0.268929\n",
      "learning_rate 0.000081\n",
      "\n",
      "\n",
      "Timestep 13000\n",
      "mean reward (50 episodes) 94.540000\n",
      "mean length (50 episodes) 5.460000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 88.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1294\n",
      "exploration 0.265476\n",
      "learning_rate 0.000080\n",
      "\n",
      "\n",
      "Timestep 13500\n",
      "mean reward (50 episodes) 94.440000\n",
      "mean length (50 episodes) 5.560000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1386\n",
      "exploration 0.262024\n",
      "learning_rate 0.000079\n",
      "\n",
      "\n",
      "Timestep 14000\n",
      "mean reward (50 episodes) 94.740000\n",
      "mean length (50 episodes) 5.260000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1482\n",
      "exploration 0.258571\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 14500\n",
      "mean reward (50 episodes) 94.760000\n",
      "mean length (50 episodes) 5.240000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1580\n",
      "exploration 0.255119\n",
      "learning_rate 0.000076\n",
      "\n",
      "\n",
      "Timestep 15000\n",
      "mean reward (50 episodes) 95.300000\n",
      "mean length (50 episodes) 4.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1682\n",
      "exploration 0.251667\n",
      "learning_rate 0.000075\n",
      "\n",
      "\n",
      "Timestep 15500\n",
      "mean reward (50 episodes) 95.120000\n",
      "mean length (50 episodes) 4.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1783\n",
      "exploration 0.248214\n",
      "learning_rate 0.000074\n",
      "\n",
      "\n",
      "Timestep 16000\n",
      "mean reward (50 episodes) 94.540000\n",
      "mean length (50 episodes) 5.460000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1876\n",
      "exploration 0.244762\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 16500\n",
      "mean reward (50 episodes) 94.360000\n",
      "mean length (50 episodes) 5.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 95.360000\n",
      "episodes 1967\n",
      "exploration 0.241310\n",
      "learning_rate 0.000071\n",
      "\n",
      "\n",
      "Timestep 17000\n",
      "mean reward (50 episodes) 95.120000\n",
      "mean length (50 episodes) 4.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.360000\n",
      "episodes 2066\n",
      "exploration 0.237857\n",
      "learning_rate 0.000070\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 17500\n",
      "mean reward (50 episodes) 95.020000\n",
      "mean length (50 episodes) 4.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.360000\n",
      "episodes 2164\n",
      "exploration 0.234405\n",
      "learning_rate 0.000069\n",
      "\n",
      "\n",
      "Timestep 18000\n",
      "mean reward (50 episodes) 95.220000\n",
      "mean length (50 episodes) 4.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.360000\n",
      "episodes 2263\n",
      "exploration 0.230952\n",
      "learning_rate 0.000068\n",
      "\n",
      "\n",
      "Timestep 18500\n",
      "mean reward (50 episodes) 95.140000\n",
      "mean length (50 episodes) 4.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.360000\n",
      "episodes 2361\n",
      "exploration 0.227500\n",
      "learning_rate 0.000066\n",
      "\n",
      "\n",
      "Timestep 19000\n",
      "mean reward (50 episodes) 94.680000\n",
      "mean length (50 episodes) 5.320000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.420000\n",
      "episodes 2459\n",
      "exploration 0.224048\n",
      "learning_rate 0.000065\n",
      "\n",
      "\n",
      "Timestep 19500\n",
      "mean reward (50 episodes) 94.760000\n",
      "mean length (50 episodes) 5.240000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.420000\n",
      "episodes 2557\n",
      "exploration 0.220595\n",
      "learning_rate 0.000064\n",
      "\n",
      "\n",
      "Timestep 20000\n",
      "mean reward (50 episodes) 94.800000\n",
      "mean length (50 episodes) 5.200000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.440000\n",
      "episodes 2659\n",
      "exploration 0.217143\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 20500\n",
      "mean reward (50 episodes) 95.160000\n",
      "mean length (50 episodes) 4.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.440000\n",
      "episodes 2757\n",
      "exploration 0.213690\n",
      "learning_rate 0.000061\n",
      "\n",
      "\n",
      "Timestep 21000\n",
      "mean reward (50 episodes) 95.000000\n",
      "mean length (50 episodes) 5.000000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.440000\n",
      "episodes 2857\n",
      "exploration 0.210238\n",
      "learning_rate 0.000060\n",
      "\n",
      "\n",
      "Timestep 21500\n",
      "mean reward (50 episodes) 95.000000\n",
      "mean length (50 episodes) 5.000000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.440000\n",
      "episodes 2957\n",
      "exploration 0.206786\n",
      "learning_rate 0.000059\n",
      "\n",
      "\n",
      "Timestep 22000\n",
      "mean reward (50 episodes) 95.360000\n",
      "mean length (50 episodes) 4.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.440000\n",
      "episodes 3063\n",
      "exploration 0.203333\n",
      "learning_rate 0.000058\n",
      "\n",
      "\n",
      "Timestep 22500\n",
      "mean reward (50 episodes) 95.100000\n",
      "mean length (50 episodes) 4.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.440000\n",
      "episodes 3163\n",
      "exploration 0.199881\n",
      "learning_rate 0.000056\n",
      "\n",
      "\n",
      "Timestep 23000\n",
      "mean reward (50 episodes) 94.960000\n",
      "mean length (50 episodes) 5.040000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 88.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 95.440000\n",
      "episodes 3264\n",
      "exploration 0.196429\n",
      "learning_rate 0.000055\n",
      "\n",
      "\n",
      "Timestep 23500\n",
      "mean reward (50 episodes) 95.560000\n",
      "mean length (50 episodes) 4.440000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.660000\n",
      "episodes 3371\n",
      "exploration 0.192976\n",
      "learning_rate 0.000054\n",
      "\n",
      "\n",
      "Timestep 24000\n",
      "mean reward (50 episodes) 95.220000\n",
      "mean length (50 episodes) 4.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.660000\n",
      "episodes 3473\n",
      "exploration 0.189524\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 24500\n",
      "mean reward (50 episodes) 95.180000\n",
      "mean length (50 episodes) 4.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.660000\n",
      "episodes 3578\n",
      "exploration 0.186071\n",
      "learning_rate 0.000051\n",
      "\n",
      "\n",
      "Timestep 25000\n",
      "mean reward (50 episodes) 95.480000\n",
      "mean length (50 episodes) 4.520000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.660000\n",
      "episodes 3686\n",
      "exploration 0.182619\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 25500\n",
      "mean reward (50 episodes) 95.440000\n",
      "mean length (50 episodes) 4.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.680000\n",
      "episodes 3795\n",
      "exploration 0.179167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 26000\n",
      "mean reward (50 episodes) 95.420000\n",
      "mean length (50 episodes) 4.580000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.680000\n",
      "episodes 3901\n",
      "exploration 0.175714\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 26500\n",
      "mean reward (50 episodes) 95.020000\n",
      "mean length (50 episodes) 4.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.680000\n",
      "episodes 4004\n",
      "exploration 0.172262\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 27000\n",
      "mean reward (50 episodes) 95.160000\n",
      "mean length (50 episodes) 4.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.700000\n",
      "episodes 4111\n",
      "exploration 0.168810\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 27500\n",
      "mean reward (50 episodes) 95.280000\n",
      "mean length (50 episodes) 4.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 87.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 95.700000\n",
      "episodes 4219\n",
      "exploration 0.165357\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 28000\n",
      "mean reward (50 episodes) 95.340000\n",
      "mean length (50 episodes) 4.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.700000\n",
      "episodes 4325\n",
      "exploration 0.161905\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 28500\n",
      "mean reward (50 episodes) 95.040000\n",
      "mean length (50 episodes) 4.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.700000\n",
      "episodes 4429\n",
      "exploration 0.158452\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 29000\n",
      "mean reward (50 episodes) 95.520000\n",
      "mean length (50 episodes) 4.480000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 95.700000\n",
      "episodes 4536\n",
      "exploration 0.155000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 29500\n",
      "mean reward (50 episodes) 95.380000\n",
      "mean length (50 episodes) 4.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.700000\n",
      "episodes 4641\n",
      "exploration 0.151548\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 30000\n",
      "mean reward (50 episodes) 95.640000\n",
      "mean length (50 episodes) 4.360000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.820000\n",
      "episodes 4753\n",
      "exploration 0.148095\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 30500\n",
      "mean reward (50 episodes) 95.340000\n",
      "mean length (50 episodes) 4.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 95.820000\n",
      "episodes 4861\n",
      "exploration 0.144643\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 31000\n",
      "mean reward (50 episodes) 95.600000\n",
      "mean length (50 episodes) 4.400000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.820000\n",
      "episodes 4970\n",
      "exploration 0.141190\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 31500\n",
      "mean reward (50 episodes) 95.100000\n",
      "mean length (50 episodes) 4.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.820000\n",
      "episodes 5074\n",
      "exploration 0.137738\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 32000\n",
      "mean reward (50 episodes) 95.580000\n",
      "mean length (50 episodes) 4.420000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.820000\n",
      "episodes 5183\n",
      "exploration 0.134286\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 32500\n",
      "mean reward (50 episodes) 95.320000\n",
      "mean length (50 episodes) 4.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 95.820000\n",
      "episodes 5290\n",
      "exploration 0.130833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 33000\n",
      "mean reward (50 episodes) 94.880000\n",
      "mean length (50 episodes) 5.120000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 95.820000\n",
      "episodes 5391\n",
      "exploration 0.127381\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 33500\n",
      "mean reward (50 episodes) 95.440000\n",
      "mean length (50 episodes) 4.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.820000\n",
      "episodes 5502\n",
      "exploration 0.123929\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 34000\n",
      "mean reward (50 episodes) 95.480000\n",
      "mean length (50 episodes) 4.520000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.820000\n",
      "episodes 5610\n",
      "exploration 0.120476\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 34500\n",
      "mean reward (50 episodes) 95.780000\n",
      "mean length (50 episodes) 4.220000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 95.820000\n",
      "episodes 5723\n",
      "exploration 0.117024\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 35000\n",
      "mean reward (50 episodes) 95.200000\n",
      "mean length (50 episodes) 4.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.020000\n",
      "episodes 5837\n",
      "exploration 0.113571\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 35500\n",
      "mean reward (50 episodes) 95.400000\n",
      "mean length (50 episodes) 4.600000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.020000\n",
      "episodes 5948\n",
      "exploration 0.110119\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 36000\n",
      "mean reward (50 episodes) 95.520000\n",
      "mean length (50 episodes) 4.480000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6060\n",
      "exploration 0.106667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 36500\n",
      "mean reward (50 episodes) 95.800000\n",
      "mean length (50 episodes) 4.200000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6169\n",
      "exploration 0.103214\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 37000\n",
      "mean reward (50 episodes) 95.600000\n",
      "mean length (50 episodes) 4.400000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 91.000000\n",
      "max_episode_length (50 episodes) 9.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6280\n",
      "exploration 0.099762\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 37500\n",
      "mean reward (50 episodes) 95.440000\n",
      "mean length (50 episodes) 4.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6392\n",
      "exploration 0.096310\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 38000\n",
      "mean reward (50 episodes) 95.740000\n",
      "mean length (50 episodes) 4.260000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6508\n",
      "exploration 0.092857\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 38500\n",
      "mean reward (50 episodes) 95.600000\n",
      "mean length (50 episodes) 4.400000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6622\n",
      "exploration 0.089405\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 39000\n",
      "mean reward (50 episodes) 95.600000\n",
      "mean length (50 episodes) 4.400000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6736\n",
      "exploration 0.085952\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 39500\n",
      "mean reward (50 episodes) 95.840000\n",
      "mean length (50 episodes) 4.160000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6848\n",
      "exploration 0.082500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 40000\n",
      "mean reward (50 episodes) 95.560000\n",
      "mean length (50 episodes) 4.440000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.020000\n",
      "episodes 6962\n",
      "exploration 0.079048\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 40500\n",
      "mean reward (50 episodes) 95.640000\n",
      "mean length (50 episodes) 4.360000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.020000\n",
      "episodes 7080\n",
      "exploration 0.075595\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 41000\n",
      "mean reward (50 episodes) 95.560000\n",
      "mean length (50 episodes) 4.440000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.040000\n",
      "episodes 7198\n",
      "exploration 0.072143\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 41500\n",
      "mean reward (50 episodes) 95.500000\n",
      "mean length (50 episodes) 4.500000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 90.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 96.040000\n",
      "episodes 7313\n",
      "exploration 0.068690\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 42000\n",
      "mean reward (50 episodes) 95.600000\n",
      "mean length (50 episodes) 4.400000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.040000\n",
      "episodes 7429\n",
      "exploration 0.065238\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 42500\n",
      "mean reward (50 episodes) 95.760000\n",
      "mean length (50 episodes) 4.240000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.040000\n",
      "episodes 7546\n",
      "exploration 0.061786\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 43000\n",
      "mean reward (50 episodes) 95.700000\n",
      "mean length (50 episodes) 4.300000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.040000\n",
      "episodes 7664\n",
      "exploration 0.058333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 43500\n",
      "mean reward (50 episodes) 95.720000\n",
      "mean length (50 episodes) 4.280000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.040000\n",
      "episodes 7780\n",
      "exploration 0.054881\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 44000\n",
      "mean reward (50 episodes) 95.940000\n",
      "mean length (50 episodes) 4.060000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.040000\n",
      "episodes 7899\n",
      "exploration 0.051429\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 44500\n",
      "mean reward (50 episodes) 95.600000\n",
      "mean length (50 episodes) 4.400000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.040000\n",
      "episodes 8017\n",
      "exploration 0.047976\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 45000\n",
      "mean reward (50 episodes) 95.760000\n",
      "mean length (50 episodes) 4.240000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.040000\n",
      "episodes 8134\n",
      "exploration 0.044524\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 45500\n",
      "mean reward (50 episodes) 95.840000\n",
      "mean length (50 episodes) 4.160000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.100000\n",
      "episodes 8254\n",
      "exploration 0.041071\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 46000\n",
      "mean reward (50 episodes) 95.780000\n",
      "mean length (50 episodes) 4.220000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.100000\n",
      "episodes 8372\n",
      "exploration 0.037619\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 46500\n",
      "mean reward (50 episodes) 95.840000\n",
      "mean length (50 episodes) 4.160000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.100000\n",
      "episodes 8491\n",
      "exploration 0.034167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 47000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.160000\n",
      "episodes 8612\n",
      "exploration 0.030714\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 47500\n",
      "mean reward (50 episodes) 95.940000\n",
      "mean length (50 episodes) 4.060000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.160000\n",
      "episodes 8734\n",
      "exploration 0.027262\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 48000\n",
      "mean reward (50 episodes) 96.020000\n",
      "mean length (50 episodes) 3.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.160000\n",
      "episodes 8859\n",
      "exploration 0.023810\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 48500\n",
      "mean reward (50 episodes) 95.860000\n",
      "mean length (50 episodes) 4.140000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.160000\n",
      "episodes 8980\n",
      "exploration 0.020357\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 49000\n",
      "mean reward (50 episodes) 95.860000\n",
      "mean length (50 episodes) 4.140000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9104\n",
      "exploration 0.016905\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 49500\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9227\n",
      "exploration 0.013452\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 50000\n",
      "mean reward (50 episodes) 95.920000\n",
      "mean length (50 episodes) 4.080000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9348\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 50500\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9474\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 51000\n",
      "mean reward (50 episodes) 96.000000\n",
      "mean length (50 episodes) 4.000000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9598\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 51500\n",
      "mean reward (50 episodes) 95.880000\n",
      "mean length (50 episodes) 4.120000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9720\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 52000\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9844\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 52500\n",
      "mean reward (50 episodes) 95.980000\n",
      "mean length (50 episodes) 4.020000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 9968\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 53000\n",
      "mean reward (50 episodes) 96.020000\n",
      "mean length (50 episodes) 3.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 10096\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 53500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 10222\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 54000\n",
      "mean reward (50 episodes) 95.940000\n",
      "mean length (50 episodes) 4.060000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 10345\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 54500\n",
      "mean reward (50 episodes) 95.780000\n",
      "mean length (50 episodes) 4.220000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.260000\n",
      "episodes 10468\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 55000\n",
      "mean reward (50 episodes) 95.860000\n",
      "mean length (50 episodes) 4.140000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.260000\n",
      "episodes 10589\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 55500\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 10719\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 56000\n",
      "mean reward (50 episodes) 96.060000\n",
      "mean length (50 episodes) 3.940000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 10847\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 56500\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 10975\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 57000\n",
      "mean reward (50 episodes) 95.940000\n",
      "mean length (50 episodes) 4.060000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11099\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 57500\n",
      "mean reward (50 episodes) 95.900000\n",
      "mean length (50 episodes) 4.100000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11221\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 58000\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11347\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 58500\n",
      "mean reward (50 episodes) 96.020000\n",
      "mean length (50 episodes) 3.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11468\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 59000\n",
      "mean reward (50 episodes) 95.880000\n",
      "mean length (50 episodes) 4.120000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11594\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 59500\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11720\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 60000\n",
      "mean reward (50 episodes) 95.960000\n",
      "mean length (50 episodes) 4.040000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11844\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 60500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 11969\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 61000\n",
      "mean reward (50 episodes) 95.900000\n",
      "mean length (50 episodes) 4.100000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12091\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 61500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12217\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 62000\n",
      "mean reward (50 episodes) 95.900000\n",
      "mean length (50 episodes) 4.100000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12338\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 62500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12465\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63000\n",
      "mean reward (50 episodes) 95.920000\n",
      "mean length (50 episodes) 4.080000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12591\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12718\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64000\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12846\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64500\n",
      "mean reward (50 episodes) 95.920000\n",
      "mean length (50 episodes) 4.080000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 12960\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 65000\n",
      "mean reward (50 episodes) 96.020000\n",
      "mean length (50 episodes) 3.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13086\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 65500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13218\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66000\n",
      "mean reward (50 episodes) 96.060000\n",
      "mean length (50 episodes) 3.940000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13344\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66500\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13473\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67000\n",
      "mean reward (50 episodes) 95.960000\n",
      "mean length (50 episodes) 4.040000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13600\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67500\n",
      "mean reward (50 episodes) 96.060000\n",
      "mean length (50 episodes) 3.940000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13729\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68000\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13853\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68500\n",
      "mean reward (50 episodes) 95.960000\n",
      "mean length (50 episodes) 4.040000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.400000\n",
      "episodes 13977\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69000\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14105\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69500\n",
      "mean reward (50 episodes) 95.900000\n",
      "mean length (50 episodes) 4.100000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14227\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70000\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14353\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70500\n",
      "mean reward (50 episodes) 95.940000\n",
      "mean length (50 episodes) 4.060000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14477\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14607\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71500\n",
      "mean reward (50 episodes) 95.920000\n",
      "mean length (50 episodes) 4.080000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14732\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72000\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14864\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72500\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 14991\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73000\n",
      "mean reward (50 episodes) 96.060000\n",
      "mean length (50 episodes) 3.940000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 15118\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.400000\n",
      "episodes 15246\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74000\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.420000\n",
      "episodes 15376\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74500\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.420000\n",
      "episodes 15509\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75000\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.460000\n",
      "episodes 15640\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75500\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.460000\n",
      "episodes 15773\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76000\n",
      "mean reward (50 episodes) 95.920000\n",
      "mean length (50 episodes) 4.080000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.460000\n",
      "episodes 15898\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76500\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16027\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 77000\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16156\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 77500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16285\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16418\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78500\n",
      "mean reward (50 episodes) 96.060000\n",
      "mean length (50 episodes) 3.940000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16546\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16678\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79500\n",
      "mean reward (50 episodes) 96.000000\n",
      "mean length (50 episodes) 4.000000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 89.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16805\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.460000\n",
      "episodes 16936\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80500\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.460000\n",
      "episodes 17064\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 17199\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81500\n",
      "mean reward (50 episodes) 96.460000\n",
      "mean length (50 episodes) 3.540000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 17331\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 17461\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82500\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 17591\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 17728\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 17862\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84000\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 17994\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 18124\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85000\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 18254\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85500\n",
      "mean reward (50 episodes) 96.040000\n",
      "mean length (50 episodes) 3.960000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 18382\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 18516\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86500\n",
      "mean reward (50 episodes) 96.440000\n",
      "mean length (50 episodes) 3.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 18648\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 18781\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87500\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 88.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 96.520000\n",
      "episodes 18911\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19046\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19175\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 89000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19305\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 89500\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19436\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90000\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19565\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90500\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19697\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91000\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19828\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91500\n",
      "mean reward (50 episodes) 96.000000\n",
      "mean length (50 episodes) 4.000000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 19958\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20090\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92500\n",
      "mean reward (50 episodes) 94.780000\n",
      "mean length (50 episodes) 5.220000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 24.000000\n",
      "max_episode_length (50 episodes) 76.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20204\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93000\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20335\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93500\n",
      "mean reward (50 episodes) 96.440000\n",
      "mean length (50 episodes) 3.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20472\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20603\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94500\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20736\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20867\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 20998\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.520000\n",
      "episodes 21132\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96500\n",
      "mean reward (50 episodes) 96.540000\n",
      "mean length (50 episodes) 3.460000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 21266\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97000\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 21397\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 21530\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98000\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.540000\n",
      "episodes 21657\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 21785\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 21917\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22050\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100000\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22178\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100500\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22310\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 101000\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22441\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 101500\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22570\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102000\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22706\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102500\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22837\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.540000\n",
      "episodes 22973\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103500\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.540000\n",
      "episodes 23106\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.540000\n",
      "episodes 23242\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.540000\n",
      "episodes 23376\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105000\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.540000\n",
      "episodes 23504\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.560000\n",
      "episodes 23640\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106000\n",
      "mean reward (50 episodes) 95.940000\n",
      "mean length (50 episodes) 4.060000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.560000\n",
      "episodes 23767\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.560000\n",
      "episodes 23900\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.560000\n",
      "episodes 24032\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107500\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 24172\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 24305\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108500\n",
      "mean reward (50 episodes) 96.020000\n",
      "mean length (50 episodes) 3.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 24434\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109000\n",
      "mean reward (50 episodes) 96.400000\n",
      "mean length (50 episodes) 3.600000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 24569\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 24701\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 24834\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110500\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 24966\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 25100\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 25235\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 25368\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112500\n",
      "mean reward (50 episodes) 96.420000\n",
      "mean length (50 episodes) 3.580000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 25505\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 113000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 25641\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 113500\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 25774\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114000\n",
      "mean reward (50 episodes) 96.400000\n",
      "mean length (50 episodes) 3.600000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 25910\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114500\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26046\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115000\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26177\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115500\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26310\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116000\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26447\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116500\n",
      "mean reward (50 episodes) 96.460000\n",
      "mean length (50 episodes) 3.540000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26584\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26716\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117500\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26852\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 26986\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118500\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 27120\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119000\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 27251\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119500\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 27384\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 27521\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 27654\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121000\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 27784\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 27917\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122000\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28049\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28180\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123000\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28309\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123500\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28442\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28573\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28710\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 125000\n",
      "mean reward (50 episodes) 95.980000\n",
      "mean length (50 episodes) 4.020000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 92.000000\n",
      "max_episode_length (50 episodes) 8.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28838\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 125500\n",
      "mean reward (50 episodes) 96.060000\n",
      "mean length (50 episodes) 3.940000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 28971\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 29105\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126500\n",
      "mean reward (50 episodes) 96.440000\n",
      "mean length (50 episodes) 3.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 29240\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127000\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 29373\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 29510\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 29642\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 29777\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 29912\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30043\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130000\n",
      "mean reward (50 episodes) 96.400000\n",
      "mean length (50 episodes) 3.600000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30179\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30315\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30454\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30588\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30722\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132500\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30855\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133000\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 30990\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133500\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 31124\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 31256\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 31391\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135000\n",
      "mean reward (50 episodes) 96.440000\n",
      "mean length (50 episodes) 3.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 31527\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 31660\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 31795\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 31926\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 137000\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32058\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 137500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32191\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138000\n",
      "mean reward (50 episodes) 96.500000\n",
      "mean length (50 episodes) 3.500000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32326\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32462\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139000\n",
      "mean reward (50 episodes) 96.420000\n",
      "mean length (50 episodes) 3.580000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32599\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32733\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140000\n",
      "mean reward (50 episodes) 96.080000\n",
      "mean length (50 episodes) 3.920000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32860\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140500\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 32996\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141000\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 33132\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 33262\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 33394\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 33527\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143000\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 33659\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 33791\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144000\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 33922\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144500\n",
      "mean reward (50 episodes) 96.060000\n",
      "mean length (50 episodes) 3.940000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34055\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145000\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34190\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145500\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34323\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146000\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34455\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34585\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147000\n",
      "mean reward (50 episodes) 96.460000\n",
      "mean length (50 episodes) 3.540000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34722\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147500\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34859\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 34989\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 35119\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 149000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 35253\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 149500\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 35388\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150000\n",
      "mean reward (50 episodes) 96.500000\n",
      "mean length (50 episodes) 3.500000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 35522\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 35653\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 35784\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 35914\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36046\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152500\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36179\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153000\n",
      "mean reward (50 episodes) 96.440000\n",
      "mean length (50 episodes) 3.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36320\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153500\n",
      "mean reward (50 episodes) 96.440000\n",
      "mean length (50 episodes) 3.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36458\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154000\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36591\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154500\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36726\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155000\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36858\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 36993\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156000\n",
      "mean reward (50 episodes) 96.420000\n",
      "mean length (50 episodes) 3.580000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 37130\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 37267\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 37402\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157500\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 37533\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 37669\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158500\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 37805\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 37936\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 38072\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 38204\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 38336\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 161000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 38469\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 161500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 38605\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162000\n",
      "mean reward (50 episodes) 96.440000\n",
      "mean length (50 episodes) 3.560000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 38740\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 38874\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163000\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39007\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39138\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164000\n",
      "mean reward (50 episodes) 96.500000\n",
      "mean length (50 episodes) 3.500000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39273\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39406\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39536\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39667\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39799\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166500\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 39934\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40069\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40204\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168000\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40339\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168500\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40470\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169000\n",
      "mean reward (50 episodes) 95.980000\n",
      "mean length (50 episodes) 4.020000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40599\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169500\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40733\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170000\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40863\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 40995\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171000\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 41126\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 41259\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172000\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 41389\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 41522\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 173000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 41654\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 173500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 41789\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 41922\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174500\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42063\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175000\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42194\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42324\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42459\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42591\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177000\n",
      "mean reward (50 episodes) 95.980000\n",
      "mean length (50 episodes) 4.020000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42721\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177500\n",
      "mean reward (50 episodes) 96.220000\n",
      "mean length (50 episodes) 3.780000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42855\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 42987\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178500\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 43117\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179000\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 43250\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 43383\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180000\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 43515\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180500\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 43650\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181000\n",
      "mean reward (50 episodes) 96.020000\n",
      "mean length (50 episodes) 3.980000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 43780\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 43917\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44051\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44182\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183000\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44314\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183500\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44443\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184000\n",
      "mean reward (50 episodes) 96.100000\n",
      "mean length (50 episodes) 3.900000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44574\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184500\n",
      "mean reward (50 episodes) 96.140000\n",
      "mean length (50 episodes) 3.860000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44707\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 185000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44840\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 185500\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 44977\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186000\n",
      "mean reward (50 episodes) 96.380000\n",
      "mean length (50 episodes) 3.620000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 45112\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186500\n",
      "mean reward (50 episodes) 96.460000\n",
      "mean length (50 episodes) 3.540000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 96.000000\n",
      "max_episode_length (50 episodes) 4.000000\n",
      "best mean reward 96.640000\n",
      "episodes 45246\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 45383\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187500\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 45516\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 45650\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188500\n",
      "mean reward (50 episodes) 96.520000\n",
      "mean length (50 episodes) 3.480000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 45787\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189000\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 45919\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46054\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46184\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46320\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191000\n",
      "mean reward (50 episodes) 96.160000\n",
      "mean length (50 episodes) 3.840000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46456\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191500\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46591\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192000\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46726\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192500\n",
      "mean reward (50 episodes) 96.120000\n",
      "mean length (50 episodes) 3.880000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46863\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193000\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 46999\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193500\n",
      "mean reward (50 episodes) 96.200000\n",
      "mean length (50 episodes) 3.800000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 47129\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194000\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 97.000000\n",
      "min_episode_length (50 episodes) 3.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 47262\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194500\n",
      "mean reward (50 episodes) 96.340000\n",
      "mean length (50 episodes) 3.660000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 47395\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195000\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 47527\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195500\n",
      "mean reward (50 episodes) 96.260000\n",
      "mean length (50 episodes) 3.740000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 47659\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196000\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 47794\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196500\n",
      "mean reward (50 episodes) 96.280000\n",
      "mean length (50 episodes) 3.720000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 47926\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 197000\n",
      "mean reward (50 episodes) 96.360000\n",
      "mean length (50 episodes) 3.640000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 48062\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 197500\n",
      "mean reward (50 episodes) 96.240000\n",
      "mean length (50 episodes) 3.760000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 93.000000\n",
      "max_episode_length (50 episodes) 7.000000\n",
      "best mean reward 96.640000\n",
      "episodes 48195\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198000\n",
      "mean reward (50 episodes) 96.420000\n",
      "mean length (50 episodes) 3.580000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 94.000000\n",
      "max_episode_length (50 episodes) 6.000000\n",
      "best mean reward 96.640000\n",
      "episodes 48330\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198500\n",
      "mean reward (50 episodes) 96.180000\n",
      "mean length (50 episodes) 3.820000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 48464\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199000\n",
      "mean reward (50 episodes) 96.320000\n",
      "mean length (50 episodes) 3.680000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 48598\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199500\n",
      "mean reward (50 episodes) 96.300000\n",
      "mean length (50 episodes) 3.700000\n",
      "max_episode_reward (50 episodes) 98.000000\n",
      "min_episode_length (50 episodes) 2.000000\n",
      "min_episode_reward (50 episodes) 95.000000\n",
      "max_episode_length (50 episodes) 5.000000\n",
      "best mean reward 96.640000\n",
      "episodes 48732\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Model saved in path: option1_6_4_4/dqn_graph.ckpt\n",
      "32.31570763190587\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2547fe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4c78cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # Get Atari games.\n",
    "    # benchmark = gym.benchmark_spec('Atari40M')\n",
    "    #\n",
    "    # # Change the index to select a different game.\n",
    "    # task = benchmark.tasks[3]\n",
    "    #\n",
    "    # # Run training\n",
    "#     seed = 0  # Use a seed of zero (you may want to randomize the seed!)\n",
    "#     set_global_seeds(seed)\n",
    "    # env = get_env(task, seed)\n",
    "    env = ArmEnvDQN_1(episode_max_length=100,\n",
    "                 size_x=6,\n",
    "                 size_y=4,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=100,\n",
    "                 tower_target_size=4)\n",
    "\n",
    "    session = get_session()\n",
    "                \n",
    "    start = time.time()\n",
    "    ep_rew, ep_len = arm_learn(env, session, num_timesteps=200000)\n",
    "    end = time.time()\n",
    "    print((end - start)/60)\n",
    "    \n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=ep_len,\n",
    "        episode_rewards=ep_rew)\n",
    "    plotting.plot_episode_stats(stats)\n",
    "#     tf.summary.FileWriter(\"option1_logs\", tf.get_default_graph()).close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_observation(frame):\n",
    "    img_h, img_w = frame.shape[1], frame.shape[2]\n",
    "    return frame.transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n",
    "\n",
    "def main():\n",
    "    env = ArmEnvDQN(episode_max_length=200,\n",
    "                 size_x=5,\n",
    "                 size_y=5,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=400,\n",
    "                 tower_target_size=4)\n",
    "    # print(env.reset())\n",
    "    session = tf.Session()\n",
    "    # First let's load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph('option_go_down.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    frame_history_len = 1\n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_shape = (img_h, img_w, frame_history_len * img_c)  # size_x, size_y,\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "#     # placeholder for current observation (or state)\n",
    "#     obs_t_ph = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "#     # casting to float on GPU ensures lower data transfer times.\n",
    "#     obs_t_float = tf.cast(obs_t_ph, tf.float32) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "#     pred_q = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "#     pred_ac = tf.argmax(pred_q, axis=1)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    obs_t_float = graph.get_tensor_by_name(\"obs_t_ph:0\")\n",
    " \n",
    "    ## How to access saved operation\n",
    "    pred_ac = graph.get_tensor_by_name(\"pred_ac:0\")\n",
    "    \n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    last_obs = env.reset()\n",
    "\n",
    "    for t in itertools.count():\n",
    "\n",
    "        obs = encode_observation(np.array([last_obs]))\n",
    "        action = session.run(pred_ac, {obs_t_float: [obs]})[0]\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done or episode_length == 500:\n",
    "            break\n",
    "\n",
    "        last_obs = next_obs\n",
    "    print(episode_reward, episode_length)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
