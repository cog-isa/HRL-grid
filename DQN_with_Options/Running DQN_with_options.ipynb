{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "#from utils import plotting\n",
    "\n",
    "import dqn_with_options as dqn\n",
    "from dqn_utils_options import *\n",
    "#from atari_wrappers import *\n",
    "#from environments.arm_env.arm_env import ArmEnv\n",
    "from arm_env_dqn_with_options import ArmEnvDQN\n",
    "from option_class import option\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arm_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=256,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def arm_learn(env, options, session, num_timesteps):\n",
    "    # This is just a rough estimate\n",
    "    num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "    lr_multiplier = 1.0\n",
    "    lr_schedule = PiecewiseSchedule([\n",
    "                                         (0,                   1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                    ],\n",
    "                                    outside_value=5e-5 * lr_multiplier)\n",
    "    optimizer = dqn.OptimizerSpec(\n",
    "        constructor=tf.train.AdamOptimizer,\n",
    "        kwargs=dict(epsilon=1e-4),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "\n",
    "    def stopping_criterion(env, t):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return t >= num_timesteps\n",
    "\n",
    "    exploration_schedule = PiecewiseSchedule(\n",
    "        [\n",
    "            (0, 1.0),\n",
    "            (80000, 0.3),\n",
    "            (200000, 0.01),\n",
    "        ], outside_value=0.01\n",
    "    )\n",
    "\n",
    "    dqn.learn(\n",
    "        env,\n",
    "        options = options,\n",
    "        q_func=arm_model,\n",
    "        optimizer_spec=optimizer,\n",
    "        session=session,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=1000000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        learning_starts=5000,\n",
    "        learning_freq=1,\n",
    "        frame_history_len=1,\n",
    "        target_update_freq=200,\n",
    "        grad_norm_clipping=10\n",
    "    )\n",
    "    \n",
    "    ep_rew = env.get_episode_rewards()\n",
    "    ep_len = env.get_episode_lengths()\n",
    "    env.close()\n",
    "    return ep_rew, ep_len\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def set_global_seeds(i):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        pass\n",
    "    else:\n",
    "        tf.set_random_seed(i) \n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "def get_session():\n",
    "    tf.reset_default_graph()\n",
    "#     tf_config = tf.ConfigProto(\n",
    "#         inter_op_parallelism_threads=1,\n",
    "#         intra_op_parallelism_threads=1)\n",
    "    session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    print(\"AVAILABLE GPUS: \", get_available_gpus())\n",
    "    session = tf.Session()\n",
    "    return session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE GPUS:  []\n",
      "INFO:tensorflow:Restoring parameters from option1_6_4_4/dqn_graph.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 21:17:33,348] Restoring parameters from option1_6_4_4/dqn_graph.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from option2/dqn_graph.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-14 21:17:33,683] Restoring parameters from option2/dqn_graph.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 5500\n",
      "mean reward (50 episodes) -428.481481\n",
      "mean length (50 episodes) 197.000000\n",
      "max_episode_reward (50 episodes) -213.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 27\n",
      "exploration 0.951875\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 6000\n",
      "mean reward (50 episodes) -424.200000\n",
      "mean length (50 episodes) 197.300000\n",
      "max_episode_reward (50 episodes) -213.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 30\n",
      "exploration 0.947500\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 6500\n",
      "mean reward (50 episodes) -420.062500\n",
      "mean length (50 episodes) 197.468750\n",
      "max_episode_reward (50 episodes) -213.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 32\n",
      "exploration 0.943125\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 7000\n",
      "mean reward (50 episodes) -416.142857\n",
      "mean length (50 episodes) 197.685714\n",
      "max_episode_reward (50 episodes) -213.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 35\n",
      "exploration 0.938750\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 7500\n",
      "mean reward (50 episodes) -412.297297\n",
      "mean length (50 episodes) 197.810811\n",
      "max_episode_reward (50 episodes) -213.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 37\n",
      "exploration 0.934375\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 8000\n",
      "mean reward (50 episodes) -416.300000\n",
      "mean length (50 episodes) 197.975000\n",
      "max_episode_reward (50 episodes) -213.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 40\n",
      "exploration 0.930000\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 8500\n",
      "mean reward (50 episodes) -408.785714\n",
      "mean length (50 episodes) 196.785714\n",
      "max_episode_reward (50 episodes) -203.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 42\n",
      "exploration 0.925625\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 9000\n",
      "mean reward (50 episodes) -407.866667\n",
      "mean length (50 episodes) 197.000000\n",
      "max_episode_reward (50 episodes) -203.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 45\n",
      "exploration 0.921250\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 9500\n",
      "mean reward (50 episodes) -411.319149\n",
      "mean length (50 episodes) 197.127660\n",
      "max_episode_reward (50 episodes) -203.000000\n",
      "min_episode_length (50 episodes) 119.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 47\n",
      "exploration 0.916875\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 10000\n",
      "mean reward (50 episodes) -404.440000\n",
      "mean length (50 episodes) 195.080000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 50\n",
      "exploration 0.912500\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 10500\n",
      "mean reward (50 episodes) -403.740000\n",
      "mean length (50 episodes) 195.080000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 53\n",
      "exploration 0.908125\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 11000\n",
      "mean reward (50 episodes) -406.960000\n",
      "mean length (50 episodes) 195.080000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -586.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 55\n",
      "exploration 0.903750\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 11500\n",
      "mean reward (50 episodes) -397.940000\n",
      "mean length (50 episodes) 195.080000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 58\n",
      "exploration 0.899375\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 12000\n",
      "mean reward (50 episodes) -400.540000\n",
      "mean length (50 episodes) 195.080000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 60\n",
      "exploration 0.895000\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 12500\n",
      "mean reward (50 episodes) -392.880000\n",
      "mean length (50 episodes) 195.080000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 63\n",
      "exploration 0.890625\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 13000\n",
      "mean reward (50 episodes) -391.140000\n",
      "mean length (50 episodes) 195.080000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 65\n",
      "exploration 0.886250\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 13500\n",
      "mean reward (50 episodes) -390.760000\n",
      "mean length (50 episodes) 196.700000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 68\n",
      "exploration 0.881875\n",
      "learning_rate 0.000099\n",
      "\n",
      "\n",
      "Timestep 14000\n",
      "mean reward (50 episodes) -392.400000\n",
      "mean length (50 episodes) 196.700000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 70\n",
      "exploration 0.877500\n",
      "learning_rate 0.000099\n",
      "\n",
      "\n",
      "Timestep 14500\n",
      "mean reward (50 episodes) -386.540000\n",
      "mean length (50 episodes) 196.700000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 73\n",
      "exploration 0.873125\n",
      "learning_rate 0.000098\n",
      "\n",
      "\n",
      "Timestep 15000\n",
      "mean reward (50 episodes) -379.820000\n",
      "mean length (50 episodes) 195.780000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 76\n",
      "exploration 0.868750\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 15500\n",
      "mean reward (50 episodes) -381.440000\n",
      "mean length (50 episodes) 195.780000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 78\n",
      "exploration 0.864375\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 16000\n",
      "mean reward (50 episodes) -383.060000\n",
      "mean length (50 episodes) 195.780000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 81\n",
      "exploration 0.860000\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 16500\n",
      "mean reward (50 episodes) -380.160000\n",
      "mean length (50 episodes) 195.780000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 83\n",
      "exploration 0.855625\n",
      "learning_rate 0.000096\n",
      "\n",
      "\n",
      "Timestep 17000\n",
      "mean reward (50 episodes) -374.220000\n",
      "mean length (50 episodes) 195.300000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 86\n",
      "exploration 0.851250\n",
      "learning_rate 0.000096\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 17500\n",
      "mean reward (50 episodes) -371.480000\n",
      "mean length (50 episodes) 193.760000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -581.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 89\n",
      "exploration 0.846875\n",
      "learning_rate 0.000095\n",
      "\n",
      "\n",
      "Timestep 18000\n",
      "mean reward (50 episodes) -370.240000\n",
      "mean length (50 episodes) 194.840000\n",
      "max_episode_reward (50 episodes) -99.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 91\n",
      "exploration 0.842500\n",
      "learning_rate 0.000095\n",
      "\n",
      "\n",
      "Timestep 18500\n",
      "mean reward (50 episodes) -369.400000\n",
      "mean length (50 episodes) 193.540000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 94\n",
      "exploration 0.838125\n",
      "learning_rate 0.000094\n",
      "\n",
      "\n",
      "Timestep 19000\n",
      "mean reward (50 episodes) -370.160000\n",
      "mean length (50 episodes) 193.540000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 96\n",
      "exploration 0.833750\n",
      "learning_rate 0.000094\n",
      "\n",
      "\n",
      "Timestep 19500\n",
      "mean reward (50 episodes) -366.060000\n",
      "mean length (50 episodes) 193.540000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 89.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 99\n",
      "exploration 0.829375\n",
      "learning_rate 0.000093\n",
      "\n",
      "\n",
      "Timestep 20000\n",
      "mean reward (50 episodes) -371.780000\n",
      "mean length (50 episodes) 194.500000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 102\n",
      "exploration 0.825000\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 20500\n",
      "mean reward (50 episodes) -374.580000\n",
      "mean length (50 episodes) 194.500000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 104\n",
      "exploration 0.820625\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 21000\n",
      "mean reward (50 episodes) -373.820000\n",
      "mean length (50 episodes) 194.500000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 107\n",
      "exploration 0.816250\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 21500\n",
      "mean reward (50 episodes) -372.720000\n",
      "mean length (50 episodes) 194.500000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 109\n",
      "exploration 0.811875\n",
      "learning_rate 0.000091\n",
      "\n",
      "\n",
      "Timestep 22000\n",
      "mean reward (50 episodes) -368.660000\n",
      "mean length (50 episodes) 194.140000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -574.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 112\n",
      "exploration 0.807500\n",
      "learning_rate 0.000091\n",
      "\n",
      "\n",
      "Timestep 22500\n",
      "mean reward (50 episodes) -363.760000\n",
      "mean length (50 episodes) 194.140000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -548.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 114\n",
      "exploration 0.803125\n",
      "learning_rate 0.000090\n",
      "\n",
      "\n",
      "Timestep 23000\n",
      "mean reward (50 episodes) -362.380000\n",
      "mean length (50 episodes) 194.140000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -548.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 117\n",
      "exploration 0.798750\n",
      "learning_rate 0.000090\n",
      "\n",
      "\n",
      "Timestep 23500\n",
      "mean reward (50 episodes) -357.960000\n",
      "mean length (50 episodes) 194.140000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -548.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 119\n",
      "exploration 0.794375\n",
      "learning_rate 0.000089\n",
      "\n",
      "\n",
      "Timestep 24000\n",
      "mean reward (50 episodes) -355.320000\n",
      "mean length (50 episodes) 194.140000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -548.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 122\n",
      "exploration 0.790000\n",
      "learning_rate 0.000089\n",
      "\n",
      "\n",
      "Timestep 24500\n",
      "mean reward (50 episodes) -353.460000\n",
      "mean length (50 episodes) 194.140000\n",
      "max_episode_reward (50 episodes) -81.000000\n",
      "min_episode_length (50 episodes) 123.000000\n",
      "min_episode_reward (50 episodes) -548.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 124\n",
      "exploration 0.785625\n",
      "learning_rate 0.000088\n",
      "\n",
      "\n",
      "Timestep 25000\n",
      "mean reward (50 episodes) -348.580000\n",
      "mean length (50 episodes) 193.100000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -548.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 127\n",
      "exploration 0.781250\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 25500\n",
      "mean reward (50 episodes) -350.340000\n",
      "mean length (50 episodes) 192.140000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -548.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 130\n",
      "exploration 0.776875\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 26000\n",
      "mean reward (50 episodes) -349.040000\n",
      "mean length (50 episodes) 192.140000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -545.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 132\n",
      "exploration 0.772500\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 26500\n",
      "mean reward (50 episodes) -352.000000\n",
      "mean length (50 episodes) 192.620000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -545.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 135\n",
      "exploration 0.768125\n",
      "learning_rate 0.000086\n",
      "\n",
      "\n",
      "Timestep 27000\n",
      "mean reward (50 episodes) -353.520000\n",
      "mean length (50 episodes) 194.160000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -545.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 137\n",
      "exploration 0.763750\n",
      "learning_rate 0.000086\n",
      "\n",
      "\n",
      "Timestep 27500\n",
      "mean reward (50 episodes) -355.360000\n",
      "mean length (50 episodes) 194.160000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 140\n",
      "exploration 0.759375\n",
      "learning_rate 0.000085\n",
      "\n",
      "\n",
      "Timestep 28000\n",
      "mean reward (50 episodes) -362.920000\n",
      "mean length (50 episodes) 195.460000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 142\n",
      "exploration 0.755000\n",
      "learning_rate 0.000085\n",
      "\n",
      "\n",
      "Timestep 28500\n",
      "mean reward (50 episodes) -350.200000\n",
      "mean length (50 episodes) 194.900000\n",
      "max_episode_reward (50 episodes) -48.000000\n",
      "min_episode_length (50 episodes) 102.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 145\n",
      "exploration 0.750625\n",
      "learning_rate 0.000084\n",
      "\n",
      "\n",
      "Timestep 29000\n",
      "mean reward (50 episodes) -339.680000\n",
      "mean length (50 episodes) 192.580000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 84.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 148\n",
      "exploration 0.746250\n",
      "learning_rate 0.000083\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 29500\n",
      "mean reward (50 episodes) -340.940000\n",
      "mean length (50 episodes) 193.840000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 84.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 150\n",
      "exploration 0.741875\n",
      "learning_rate 0.000083\n",
      "\n",
      "\n",
      "Timestep 30000\n",
      "mean reward (50 episodes) -333.180000\n",
      "mean length (50 episodes) 193.840000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 84.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 153\n",
      "exploration 0.737500\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 30500\n",
      "mean reward (50 episodes) -329.440000\n",
      "mean length (50 episodes) 193.840000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 84.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 155\n",
      "exploration 0.733125\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 31000\n",
      "mean reward (50 episodes) -325.040000\n",
      "mean length (50 episodes) 193.840000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 84.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -319.000000\n",
      "episodes 158\n",
      "exploration 0.728750\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 31500\n",
      "mean reward (50 episodes) -316.540000\n",
      "mean length (50 episodes) 191.120000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -316.540000\n",
      "episodes 161\n",
      "exploration 0.724375\n",
      "learning_rate 0.000081\n",
      "\n",
      "\n",
      "Timestep 32000\n",
      "mean reward (50 episodes) -315.660000\n",
      "mean length (50 episodes) 189.440000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -315.660000\n",
      "episodes 164\n",
      "exploration 0.720000\n",
      "learning_rate 0.000081\n",
      "\n",
      "\n",
      "Timestep 32500\n",
      "mean reward (50 episodes) -308.160000\n",
      "mean length (50 episodes) 186.020000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -308.160000\n",
      "episodes 167\n",
      "exploration 0.715625\n",
      "learning_rate 0.000080\n",
      "\n",
      "\n",
      "Timestep 33000\n",
      "mean reward (50 episodes) -307.020000\n",
      "mean length (50 episodes) 186.020000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -307.020000\n",
      "episodes 170\n",
      "exploration 0.711250\n",
      "learning_rate 0.000080\n",
      "\n",
      "\n",
      "Timestep 33500\n",
      "mean reward (50 episodes) -307.280000\n",
      "mean length (50 episodes) 186.020000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -306.920000\n",
      "episodes 172\n",
      "exploration 0.706875\n",
      "learning_rate 0.000079\n",
      "\n",
      "\n",
      "Timestep 34000\n",
      "mean reward (50 episodes) -304.220000\n",
      "mean length (50 episodes) 186.020000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -304.220000\n",
      "episodes 175\n",
      "exploration 0.702500\n",
      "learning_rate 0.000079\n",
      "\n",
      "\n",
      "Timestep 34500\n",
      "mean reward (50 episodes) -307.080000\n",
      "mean length (50 episodes) 187.980000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -302.900000\n",
      "episodes 177\n",
      "exploration 0.698125\n",
      "learning_rate 0.000078\n",
      "\n",
      "\n",
      "Timestep 35000\n",
      "mean reward (50 episodes) -307.320000\n",
      "mean length (50 episodes) 188.940000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -302.900000\n",
      "episodes 180\n",
      "exploration 0.693750\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 35500\n",
      "mean reward (50 episodes) -300.260000\n",
      "mean length (50 episodes) 187.540000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -300.260000\n",
      "episodes 183\n",
      "exploration 0.689375\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 36000\n",
      "mean reward (50 episodes) -300.780000\n",
      "mean length (50 episodes) 187.140000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -299.180000\n",
      "episodes 185\n",
      "exploration 0.685000\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 36500\n",
      "mean reward (50 episodes) -301.400000\n",
      "mean length (50 episodes) 187.120000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -565.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -299.180000\n",
      "episodes 188\n",
      "exploration 0.680625\n",
      "learning_rate 0.000076\n",
      "\n",
      "\n",
      "Timestep 37000\n",
      "mean reward (50 episodes) -293.340000\n",
      "mean length (50 episodes) 187.120000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -293.340000\n",
      "episodes 190\n",
      "exploration 0.676250\n",
      "learning_rate 0.000076\n",
      "\n",
      "\n",
      "Timestep 37500\n",
      "mean reward (50 episodes) -290.680000\n",
      "mean length (50 episodes) 187.120000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -290.680000\n",
      "episodes 193\n",
      "exploration 0.671875\n",
      "learning_rate 0.000075\n",
      "\n",
      "\n",
      "Timestep 38000\n",
      "mean reward (50 episodes) -290.280000\n",
      "mean length (50 episodes) 185.600000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 64.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -286.620000\n",
      "episodes 196\n",
      "exploration 0.667500\n",
      "learning_rate 0.000074\n",
      "\n",
      "\n",
      "Timestep 38500\n",
      "mean reward (50 episodes) -281.940000\n",
      "mean length (50 episodes) 182.420000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -281.940000\n",
      "episodes 200\n",
      "exploration 0.663125\n",
      "learning_rate 0.000074\n",
      "\n",
      "\n",
      "Timestep 39000\n",
      "mean reward (50 episodes) -271.980000\n",
      "mean length (50 episodes) 179.740000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -271.980000\n",
      "episodes 203\n",
      "exploration 0.658750\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 39500\n",
      "mean reward (50 episodes) -265.440000\n",
      "mean length (50 episodes) 179.660000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -265.440000\n",
      "episodes 205\n",
      "exploration 0.654375\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 40000\n",
      "mean reward (50 episodes) -263.840000\n",
      "mean length (50 episodes) 177.820000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -263.840000\n",
      "episodes 208\n",
      "exploration 0.650000\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 40500\n",
      "mean reward (50 episodes) -270.300000\n",
      "mean length (50 episodes) 180.540000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -263.500000\n",
      "episodes 211\n",
      "exploration 0.645625\n",
      "learning_rate 0.000072\n",
      "\n",
      "\n",
      "Timestep 41000\n",
      "mean reward (50 episodes) -262.340000\n",
      "mean length (50 episodes) 177.820000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -479.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -262.340000\n",
      "episodes 214\n",
      "exploration 0.641250\n",
      "learning_rate 0.000072\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 41500\n",
      "mean reward (50 episodes) -259.820000\n",
      "mean length (50 episodes) 177.360000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -258.880000\n",
      "episodes 218\n",
      "exploration 0.636875\n",
      "learning_rate 0.000071\n",
      "\n",
      "\n",
      "Timestep 42000\n",
      "mean reward (50 episodes) -254.220000\n",
      "mean length (50 episodes) 175.240000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -254.020000\n",
      "episodes 221\n",
      "exploration 0.632500\n",
      "learning_rate 0.000071\n",
      "\n",
      "\n",
      "Timestep 42500\n",
      "mean reward (50 episodes) -245.740000\n",
      "mean length (50 episodes) 170.380000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -243.740000\n",
      "episodes 224\n",
      "exploration 0.628125\n",
      "learning_rate 0.000070\n",
      "\n",
      "\n",
      "Timestep 43000\n",
      "mean reward (50 episodes) -236.100000\n",
      "mean length (50 episodes) 166.860000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -236.100000\n",
      "episodes 228\n",
      "exploration 0.623750\n",
      "learning_rate 0.000070\n",
      "\n",
      "\n",
      "Timestep 43500\n",
      "mean reward (50 episodes) -237.080000\n",
      "mean length (50 episodes) 166.860000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -236.100000\n",
      "episodes 230\n",
      "exploration 0.619375\n",
      "learning_rate 0.000069\n",
      "\n",
      "\n",
      "Timestep 44000\n",
      "mean reward (50 episodes) -233.040000\n",
      "mean length (50 episodes) 166.460000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -233.040000\n",
      "episodes 233\n",
      "exploration 0.615000\n",
      "learning_rate 0.000069\n",
      "\n",
      "\n",
      "Timestep 44500\n",
      "mean reward (50 episodes) -233.820000\n",
      "mean length (50 episodes) 166.880000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -232.760000\n",
      "episodes 236\n",
      "exploration 0.610625\n",
      "learning_rate 0.000068\n",
      "\n",
      "\n",
      "Timestep 45000\n",
      "mean reward (50 episodes) -231.940000\n",
      "mean length (50 episodes) 166.820000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -231.140000\n",
      "episodes 238\n",
      "exploration 0.606250\n",
      "learning_rate 0.000068\n",
      "\n",
      "\n",
      "Timestep 45500\n",
      "mean reward (50 episodes) -226.940000\n",
      "mean length (50 episodes) 164.380000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -226.940000\n",
      "episodes 241\n",
      "exploration 0.601875\n",
      "learning_rate 0.000067\n",
      "\n",
      "\n",
      "Timestep 46000\n",
      "mean reward (50 episodes) -228.660000\n",
      "mean length (50 episodes) 166.680000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -225.840000\n",
      "episodes 244\n",
      "exploration 0.597500\n",
      "learning_rate 0.000067\n",
      "\n",
      "\n",
      "Timestep 46500\n",
      "mean reward (50 episodes) -233.280000\n",
      "mean length (50 episodes) 168.480000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -431.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -225.840000\n",
      "episodes 246\n",
      "exploration 0.593125\n",
      "learning_rate 0.000066\n",
      "\n",
      "\n",
      "Timestep 47000\n",
      "mean reward (50 episodes) -232.780000\n",
      "mean length (50 episodes) 168.860000\n",
      "max_episode_reward (50 episodes) 26.000000\n",
      "min_episode_length (50 episodes) 46.000000\n",
      "min_episode_reward (50 episodes) -420.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -225.840000\n",
      "episodes 250\n",
      "exploration 0.588750\n",
      "learning_rate 0.000066\n",
      "\n",
      "\n",
      "Timestep 47500\n",
      "mean reward (50 episodes) -225.340000\n",
      "mean length (50 episodes) 165.860000\n",
      "max_episode_reward (50 episodes) 26.000000\n",
      "min_episode_length (50 episodes) 46.000000\n",
      "min_episode_reward (50 episodes) -420.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -225.340000\n",
      "episodes 253\n",
      "exploration 0.584375\n",
      "learning_rate 0.000065\n",
      "\n",
      "\n",
      "Timestep 48000\n",
      "mean reward (50 episodes) -221.680000\n",
      "mean length (50 episodes) 163.760000\n",
      "max_episode_reward (50 episodes) 26.000000\n",
      "min_episode_length (50 episodes) 46.000000\n",
      "min_episode_reward (50 episodes) -415.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -220.300000\n",
      "episodes 257\n",
      "exploration 0.580000\n",
      "learning_rate 0.000064\n",
      "\n",
      "\n",
      "Timestep 48500\n",
      "mean reward (50 episodes) -206.340000\n",
      "mean length (50 episodes) 158.160000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -415.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -206.340000\n",
      "episodes 261\n",
      "exploration 0.575625\n",
      "learning_rate 0.000064\n",
      "\n",
      "\n",
      "Timestep 49000\n",
      "mean reward (50 episodes) -210.780000\n",
      "mean length (50 episodes) 161.040000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -396.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -204.540000\n",
      "episodes 263\n",
      "exploration 0.571250\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 49500\n",
      "mean reward (50 episodes) -203.840000\n",
      "mean length (50 episodes) 160.340000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -203.840000\n",
      "episodes 267\n",
      "exploration 0.566875\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 50000\n",
      "mean reward (50 episodes) -209.120000\n",
      "mean length (50 episodes) 161.980000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -203.840000\n",
      "episodes 270\n",
      "exploration 0.562500\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 50500\n",
      "mean reward (50 episodes) -210.880000\n",
      "mean length (50 episodes) 163.460000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -203.840000\n",
      "episodes 273\n",
      "exploration 0.558125\n",
      "learning_rate 0.000062\n",
      "\n",
      "\n",
      "Timestep 51000\n",
      "mean reward (50 episodes) -207.520000\n",
      "mean length (50 episodes) 163.580000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -203.500000\n",
      "episodes 276\n",
      "exploration 0.553750\n",
      "learning_rate 0.000062\n",
      "\n",
      "\n",
      "Timestep 51500\n",
      "mean reward (50 episodes) -196.880000\n",
      "mean length (50 episodes) 158.360000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 39.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -196.880000\n",
      "episodes 281\n",
      "exploration 0.549375\n",
      "learning_rate 0.000061\n",
      "\n",
      "\n",
      "Timestep 52000\n",
      "mean reward (50 episodes) -198.540000\n",
      "mean length (50 episodes) 159.820000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 39.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -195.240000\n",
      "episodes 283\n",
      "exploration 0.545000\n",
      "learning_rate 0.000061\n",
      "\n",
      "\n",
      "Timestep 52500\n",
      "mean reward (50 episodes) -181.640000\n",
      "mean length (50 episodes) 151.420000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.640000\n",
      "episodes 288\n",
      "exploration 0.540625\n",
      "learning_rate 0.000060\n",
      "\n",
      "\n",
      "Timestep 53000\n",
      "mean reward (50 episodes) -181.260000\n",
      "mean length (50 episodes) 151.420000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.260000\n",
      "episodes 290\n",
      "exploration 0.536250\n",
      "learning_rate 0.000060\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 53500\n",
      "mean reward (50 episodes) -189.260000\n",
      "mean length (50 episodes) 153.860000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.260000\n",
      "episodes 293\n",
      "exploration 0.531875\n",
      "learning_rate 0.000059\n",
      "\n",
      "\n",
      "Timestep 54000\n",
      "mean reward (50 episodes) -179.720000\n",
      "mean length (50 episodes) 147.500000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -179.720000\n",
      "episodes 297\n",
      "exploration 0.527500\n",
      "learning_rate 0.000059\n",
      "\n",
      "\n",
      "Timestep 54500\n",
      "mean reward (50 episodes) -171.880000\n",
      "mean length (50 episodes) 143.000000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -167.600000\n",
      "episodes 301\n",
      "exploration 0.523125\n",
      "learning_rate 0.000058\n",
      "\n",
      "\n",
      "Timestep 55000\n",
      "mean reward (50 episodes) -169.160000\n",
      "mean length (50 episodes) 142.320000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -390.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -167.600000\n",
      "episodes 305\n",
      "exploration 0.518750\n",
      "learning_rate 0.000058\n",
      "\n",
      "\n",
      "Timestep 55500\n",
      "mean reward (50 episodes) -167.700000\n",
      "mean length (50 episodes) 143.980000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -359.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -166.240000\n",
      "episodes 309\n",
      "exploration 0.514375\n",
      "learning_rate 0.000057\n",
      "\n",
      "\n",
      "Timestep 56000\n",
      "mean reward (50 episodes) -168.300000\n",
      "mean length (50 episodes) 144.220000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -359.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -166.240000\n",
      "episodes 312\n",
      "exploration 0.510000\n",
      "learning_rate 0.000057\n",
      "\n",
      "\n",
      "Timestep 56500\n",
      "mean reward (50 episodes) -162.000000\n",
      "mean length (50 episodes) 139.940000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -359.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -158.580000\n",
      "episodes 317\n",
      "exploration 0.505625\n",
      "learning_rate 0.000056\n",
      "\n",
      "\n",
      "Timestep 57000\n",
      "mean reward (50 episodes) -157.000000\n",
      "mean length (50 episodes) 137.660000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -359.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -157.000000\n",
      "episodes 320\n",
      "exploration 0.501250\n",
      "learning_rate 0.000056\n",
      "\n",
      "\n",
      "Timestep 57500\n",
      "mean reward (50 episodes) -164.440000\n",
      "mean length (50 episodes) 140.560000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -156.480000\n",
      "episodes 323\n",
      "exploration 0.496875\n",
      "learning_rate 0.000055\n",
      "\n",
      "\n",
      "Timestep 58000\n",
      "mean reward (50 episodes) -167.720000\n",
      "mean length (50 episodes) 139.900000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -156.480000\n",
      "episodes 326\n",
      "exploration 0.492500\n",
      "learning_rate 0.000055\n",
      "\n",
      "\n",
      "Timestep 58500\n",
      "mean reward (50 episodes) -167.200000\n",
      "mean length (50 episodes) 139.160000\n",
      "max_episode_reward (50 episodes) 46.000000\n",
      "min_episode_length (50 episodes) 30.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -156.480000\n",
      "episodes 329\n",
      "exploration 0.488125\n",
      "learning_rate 0.000054\n",
      "\n",
      "\n",
      "Timestep 59000\n",
      "mean reward (50 episodes) -158.220000\n",
      "mean length (50 episodes) 135.140000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 31.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -156.480000\n",
      "episodes 335\n",
      "exploration 0.483750\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 59500\n",
      "mean reward (50 episodes) -164.660000\n",
      "mean length (50 episodes) 138.540000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 31.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -156.480000\n",
      "episodes 338\n",
      "exploration 0.479375\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 60000\n",
      "mean reward (50 episodes) -144.820000\n",
      "mean length (50 episodes) 131.240000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -144.820000\n",
      "episodes 342\n",
      "exploration 0.475000\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 60500\n",
      "mean reward (50 episodes) -143.080000\n",
      "mean length (50 episodes) 132.180000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -142.840000\n",
      "episodes 346\n",
      "exploration 0.470625\n",
      "learning_rate 0.000052\n",
      "\n",
      "\n",
      "Timestep 61000\n",
      "mean reward (50 episodes) -148.060000\n",
      "mean length (50 episodes) 134.860000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -140.860000\n",
      "episodes 350\n",
      "exploration 0.466250\n",
      "learning_rate 0.000052\n",
      "\n",
      "\n",
      "Timestep 61500\n",
      "mean reward (50 episodes) -142.820000\n",
      "mean length (50 episodes) 132.380000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -140.860000\n",
      "episodes 354\n",
      "exploration 0.461875\n",
      "learning_rate 0.000051\n",
      "\n",
      "\n",
      "Timestep 62000\n",
      "mean reward (50 episodes) -149.340000\n",
      "mean length (50 episodes) 135.200000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -140.860000\n",
      "episodes 356\n",
      "exploration 0.457500\n",
      "learning_rate 0.000051\n",
      "\n",
      "\n",
      "Timestep 62500\n",
      "mean reward (50 episodes) -151.760000\n",
      "mean length (50 episodes) 134.660000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -140.860000\n",
      "episodes 360\n",
      "exploration 0.453125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63000\n",
      "mean reward (50 episodes) -160.620000\n",
      "mean length (50 episodes) 137.260000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -140.860000\n",
      "episodes 363\n",
      "exploration 0.448750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63500\n",
      "mean reward (50 episodes) -159.560000\n",
      "mean length (50 episodes) 136.520000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -140.860000\n",
      "episodes 368\n",
      "exploration 0.444375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64000\n",
      "mean reward (50 episodes) -160.960000\n",
      "mean length (50 episodes) 137.780000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -374.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -140.860000\n",
      "episodes 371\n",
      "exploration 0.440000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64500\n",
      "mean reward (50 episodes) -139.300000\n",
      "mean length (50 episodes) 130.220000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -356.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -139.300000\n",
      "episodes 376\n",
      "exploration 0.435625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 65000\n",
      "mean reward (50 episodes) -133.320000\n",
      "mean length (50 episodes) 128.360000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -356.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -133.320000\n",
      "episodes 380\n",
      "exploration 0.431250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 65500\n",
      "mean reward (50 episodes) -136.220000\n",
      "mean length (50 episodes) 129.160000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -356.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -132.820000\n",
      "episodes 384\n",
      "exploration 0.426875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66000\n",
      "mean reward (50 episodes) -142.800000\n",
      "mean length (50 episodes) 132.640000\n",
      "max_episode_reward (50 episodes) 51.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -356.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -132.820000\n",
      "episodes 387\n",
      "exploration 0.422500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66500\n",
      "mean reward (50 episodes) -129.440000\n",
      "mean length (50 episodes) 125.380000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -356.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -129.440000\n",
      "episodes 393\n",
      "exploration 0.418125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67000\n",
      "mean reward (50 episodes) -132.500000\n",
      "mean length (50 episodes) 128.140000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -356.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -127.620000\n",
      "episodes 396\n",
      "exploration 0.413750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67500\n",
      "mean reward (50 episodes) -136.340000\n",
      "mean length (50 episodes) 129.100000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -342.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -127.620000\n",
      "episodes 400\n",
      "exploration 0.409375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68000\n",
      "mean reward (50 episodes) -110.100000\n",
      "mean length (50 episodes) 115.560000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -342.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -110.100000\n",
      "episodes 407\n",
      "exploration 0.405000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68500\n",
      "mean reward (50 episodes) -96.440000\n",
      "mean length (50 episodes) 109.440000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -342.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -96.440000\n",
      "episodes 413\n",
      "exploration 0.400625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69000\n",
      "mean reward (50 episodes) -72.880000\n",
      "mean length (50 episodes) 98.160000\n",
      "max_episode_reward (50 episodes) 49.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -329.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -72.880000\n",
      "episodes 422\n",
      "exploration 0.396250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69500\n",
      "mean reward (50 episodes) -46.640000\n",
      "mean length (50 episodes) 82.500000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) -329.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -46.640000\n",
      "episodes 432\n",
      "exploration 0.391875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70000\n",
      "mean reward (50 episodes) -32.340000\n",
      "mean length (50 episodes) 72.440000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) -329.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -31.600000\n",
      "episodes 442\n",
      "exploration 0.387500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70500\n",
      "mean reward (50 episodes) -12.340000\n",
      "mean length (50 episodes) 61.820000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) -286.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -12.340000\n",
      "episodes 448\n",
      "exploration 0.383125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71000\n",
      "mean reward (50 episodes) -4.600000\n",
      "mean length (50 episodes) 56.660000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) -277.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -4.600000\n",
      "episodes 458\n",
      "exploration 0.378750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71500\n",
      "mean reward (50 episodes) 0.100000\n",
      "mean length (50 episodes) 53.360000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) -277.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 0.100000\n",
      "episodes 468\n",
      "exploration 0.374375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72000\n",
      "mean reward (50 episodes) -0.520000\n",
      "mean length (50 episodes) 52.260000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) -277.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 1.400000\n",
      "episodes 480\n",
      "exploration 0.370000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72500\n",
      "mean reward (50 episodes) 7.440000\n",
      "mean length (50 episodes) 48.060000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) -277.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 9.440000\n",
      "episodes 493\n",
      "exploration 0.365625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73000\n",
      "mean reward (50 episodes) 17.820000\n",
      "mean length (50 episodes) 42.580000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) -241.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 17.820000\n",
      "episodes 504\n",
      "exploration 0.361250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73500\n",
      "mean reward (50 episodes) 27.280000\n",
      "mean length (50 episodes) 36.880000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) -81.000000\n",
      "max_episode_length (50 episodes) 124.000000\n",
      "best mean reward 28.120000\n",
      "episodes 517\n",
      "exploration 0.356875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74000\n",
      "mean reward (50 episodes) 23.600000\n",
      "mean length (50 episodes) 41.060000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) -179.000000\n",
      "max_episode_length (50 episodes) 187.000000\n",
      "best mean reward 28.120000\n",
      "episodes 527\n",
      "exploration 0.352500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74500\n",
      "mean reward (50 episodes) 20.320000\n",
      "mean length (50 episodes) 41.920000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -275.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 28.120000\n",
      "episodes 541\n",
      "exploration 0.348125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75000\n",
      "mean reward (50 episodes) 17.220000\n",
      "mean length (50 episodes) 41.040000\n",
      "max_episode_reward (50 episodes) 58.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -275.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 28.120000\n",
      "episodes 552\n",
      "exploration 0.343750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75500\n",
      "mean reward (50 episodes) 11.500000\n",
      "mean length (50 episodes) 43.700000\n",
      "max_episode_reward (50 episodes) 58.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -275.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 28.120000\n",
      "episodes 560\n",
      "exploration 0.339375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76000\n",
      "mean reward (50 episodes) 12.980000\n",
      "mean length (50 episodes) 40.720000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -275.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 28.120000\n",
      "episodes 576\n",
      "exploration 0.335000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76500\n",
      "mean reward (50 episodes) 15.120000\n",
      "mean length (50 episodes) 37.460000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -260.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 28.120000\n",
      "episodes 593\n",
      "exploration 0.330625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 77000\n",
      "mean reward (50 episodes) 35.180000\n",
      "mean length (50 episodes) 27.200000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -42.000000\n",
      "max_episode_length (50 episodes) 68.000000\n",
      "best mean reward 35.180000\n",
      "episodes 611\n",
      "exploration 0.326250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 77500\n",
      "mean reward (50 episodes) 40.080000\n",
      "mean length (50 episodes) 23.600000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -42.000000\n",
      "max_episode_length (50 episodes) 68.000000\n",
      "best mean reward 40.080000\n",
      "episodes 633\n",
      "exploration 0.321875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78000\n",
      "mean reward (50 episodes) 42.280000\n",
      "mean length (50 episodes) 23.620000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 11.000000\n",
      "min_episode_reward (50 episodes) -22.000000\n",
      "max_episode_length (50 episodes) 70.000000\n",
      "best mean reward 42.580000\n",
      "episodes 653\n",
      "exploration 0.317500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78500\n",
      "mean reward (50 episodes) 37.280000\n",
      "mean length (50 episodes) 26.360000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 11.000000\n",
      "min_episode_reward (50 episodes) -51.000000\n",
      "max_episode_length (50 episodes) 76.000000\n",
      "best mean reward 42.580000\n",
      "episodes 665\n",
      "exploration 0.313125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79000\n",
      "mean reward (50 episodes) 26.300000\n",
      "mean length (50 episodes) 33.240000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 11.000000\n",
      "min_episode_reward (50 episodes) -270.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 42.580000\n",
      "episodes 674\n",
      "exploration 0.308750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79500\n",
      "mean reward (50 episodes) 30.060000\n",
      "mean length (50 episodes) 31.360000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 11.000000\n",
      "min_episode_reward (50 episodes) -270.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 42.580000\n",
      "episodes 699\n",
      "exploration 0.304375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80000\n",
      "mean reward (50 episodes) 47.220000\n",
      "mean length (50 episodes) 20.720000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -1.000000\n",
      "max_episode_length (50 episodes) 39.000000\n",
      "best mean reward 47.440000\n",
      "episodes 721\n",
      "exploration 0.300000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80500\n",
      "mean reward (50 episodes) 49.380000\n",
      "mean length (50 episodes) 20.060000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -1.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 49.680000\n",
      "episodes 746\n",
      "exploration 0.298792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81000\n",
      "mean reward (50 episodes) 50.180000\n",
      "mean length (50 episodes) 20.140000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 10.000000\n",
      "max_episode_length (50 episodes) 41.000000\n",
      "best mean reward 51.120000\n",
      "episodes 767\n",
      "exploration 0.297583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81500\n",
      "mean reward (50 episodes) 40.780000\n",
      "mean length (50 episodes) 25.100000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -77.000000\n",
      "max_episode_length (50 episodes) 110.000000\n",
      "best mean reward 51.120000\n",
      "episodes 781\n",
      "exploration 0.296375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82000\n",
      "mean reward (50 episodes) 38.340000\n",
      "mean length (50 episodes) 27.460000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -77.000000\n",
      "max_episode_length (50 episodes) 110.000000\n",
      "best mean reward 51.120000\n",
      "episodes 799\n",
      "exploration 0.295167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82500\n",
      "mean reward (50 episodes) 34.560000\n",
      "mean length (50 episodes) 27.920000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -105.000000\n",
      "max_episode_length (50 episodes) 110.000000\n",
      "best mean reward 51.120000\n",
      "episodes 818\n",
      "exploration 0.293958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83000\n",
      "mean reward (50 episodes) 40.080000\n",
      "mean length (50 episodes) 24.260000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -105.000000\n",
      "max_episode_length (50 episodes) 48.000000\n",
      "best mean reward 51.120000\n",
      "episodes 841\n",
      "exploration 0.292750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83500\n",
      "mean reward (50 episodes) 42.180000\n",
      "mean length (50 episodes) 21.920000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -105.000000\n",
      "max_episode_length (50 episodes) 51.000000\n",
      "best mean reward 51.120000\n",
      "episodes 862\n",
      "exploration 0.291542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84000\n",
      "mean reward (50 episodes) 43.460000\n",
      "mean length (50 episodes) 21.760000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -22.000000\n",
      "max_episode_length (50 episodes) 51.000000\n",
      "best mean reward 51.120000\n",
      "episodes 885\n",
      "exploration 0.290333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84500\n",
      "mean reward (50 episodes) 43.120000\n",
      "mean length (50 episodes) 23.160000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -38.000000\n",
      "max_episode_length (50 episodes) 105.000000\n",
      "best mean reward 51.120000\n",
      "episodes 904\n",
      "exploration 0.289125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85000\n",
      "mean reward (50 episodes) 47.880000\n",
      "mean length (50 episodes) 22.420000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -38.000000\n",
      "max_episode_length (50 episodes) 105.000000\n",
      "best mean reward 51.120000\n",
      "episodes 928\n",
      "exploration 0.287917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85500\n",
      "mean reward (50 episodes) 52.200000\n",
      "mean length (50 episodes) 19.420000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 22.000000\n",
      "max_episode_length (50 episodes) 47.000000\n",
      "best mean reward 52.620000\n",
      "episodes 953\n",
      "exploration 0.286708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86000\n",
      "mean reward (50 episodes) 50.960000\n",
      "mean length (50 episodes) 19.460000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 26.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 52.800000\n",
      "episodes 977\n",
      "exploration 0.285500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86500\n",
      "mean reward (50 episodes) 45.440000\n",
      "mean length (50 episodes) 21.240000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -67.000000\n",
      "max_episode_length (50 episodes) 48.000000\n",
      "best mean reward 52.800000\n",
      "episodes 997\n",
      "exploration 0.284292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87000\n",
      "mean reward (50 episodes) 42.360000\n",
      "mean length (50 episodes) 21.680000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -93.000000\n",
      "max_episode_length (50 episodes) 62.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1020\n",
      "exploration 0.283083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87500\n",
      "mean reward (50 episodes) 43.080000\n",
      "mean length (50 episodes) 21.300000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -93.000000\n",
      "max_episode_length (50 episodes) 62.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1041\n",
      "exploration 0.281875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88000\n",
      "mean reward (50 episodes) 42.000000\n",
      "mean length (50 episodes) 22.160000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -93.000000\n",
      "max_episode_length (50 episodes) 62.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1064\n",
      "exploration 0.280667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88500\n",
      "mean reward (50 episodes) 42.680000\n",
      "mean length (50 episodes) 23.380000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -72.000000\n",
      "max_episode_length (50 episodes) 55.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1083\n",
      "exploration 0.279458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 89000\n",
      "mean reward (50 episodes) 39.600000\n",
      "mean length (50 episodes) 27.600000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -72.000000\n",
      "max_episode_length (50 episodes) 77.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1094\n",
      "exploration 0.278250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 89500\n",
      "mean reward (50 episodes) 28.560000\n",
      "mean length (50 episodes) 35.960000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 164.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1098\n",
      "exploration 0.277042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90000\n",
      "mean reward (50 episodes) 24.740000\n",
      "mean length (50 episodes) 40.400000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 164.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1110\n",
      "exploration 0.275833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90500\n",
      "mean reward (50 episodes) 23.760000\n",
      "mean length (50 episodes) 43.180000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 164.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1126\n",
      "exploration 0.274625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91000\n",
      "mean reward (50 episodes) 21.460000\n",
      "mean length (50 episodes) 46.920000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 164.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1133\n",
      "exploration 0.273417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91500\n",
      "mean reward (50 episodes) 20.120000\n",
      "mean length (50 episodes) 47.120000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 164.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1145\n",
      "exploration 0.272208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92000\n",
      "mean reward (50 episodes) 36.200000\n",
      "mean length (50 episodes) 34.360000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -79.000000\n",
      "max_episode_length (50 episodes) 154.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1166\n",
      "exploration 0.271000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92500\n",
      "mean reward (50 episodes) 34.160000\n",
      "mean length (50 episodes) 36.320000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -79.000000\n",
      "max_episode_length (50 episodes) 154.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1179\n",
      "exploration 0.269792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93000\n",
      "mean reward (50 episodes) 45.860000\n",
      "mean length (50 episodes) 25.860000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -62.000000\n",
      "max_episode_length (50 episodes) 111.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1201\n",
      "exploration 0.268583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93500\n",
      "mean reward (50 episodes) 49.460000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -6.000000\n",
      "max_episode_length (50 episodes) 72.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1222\n",
      "exploration 0.267375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94000\n",
      "mean reward (50 episodes) 50.060000\n",
      "mean length (50 episodes) 21.080000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 0.000000\n",
      "max_episode_length (50 episodes) 69.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1246\n",
      "exploration 0.266167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94500\n",
      "mean reward (50 episodes) 49.300000\n",
      "mean length (50 episodes) 20.380000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 0.000000\n",
      "max_episode_length (50 episodes) 48.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1270\n",
      "exploration 0.264958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95000\n",
      "mean reward (50 episodes) 51.240000\n",
      "mean length (50 episodes) 19.040000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 14.000000\n",
      "max_episode_length (50 episodes) 48.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1296\n",
      "exploration 0.263750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95500\n",
      "mean reward (50 episodes) 49.600000\n",
      "mean length (50 episodes) 20.060000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 16.000000\n",
      "max_episode_length (50 episodes) 47.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1318\n",
      "exploration 0.262542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96000\n",
      "mean reward (50 episodes) 47.340000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 10.000000\n",
      "max_episode_length (50 episodes) 47.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1343\n",
      "exploration 0.261333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96500\n",
      "mean reward (50 episodes) 49.880000\n",
      "mean length (50 episodes) 19.100000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 10.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1368\n",
      "exploration 0.260125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97000\n",
      "mean reward (50 episodes) 42.300000\n",
      "mean length (50 episodes) 23.820000\n",
      "max_episode_reward (50 episodes) 61.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -295.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1381\n",
      "exploration 0.258917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97500\n",
      "mean reward (50 episodes) 43.080000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -295.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1407\n",
      "exploration 0.257708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98000\n",
      "mean reward (50 episodes) 49.720000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -4.000000\n",
      "max_episode_length (50 episodes) 35.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1430\n",
      "exploration 0.256500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98500\n",
      "mean reward (50 episodes) 48.540000\n",
      "mean length (50 episodes) 19.720000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -4.000000\n",
      "max_episode_length (50 episodes) 39.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1447\n",
      "exploration 0.255292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99000\n",
      "mean reward (50 episodes) 40.300000\n",
      "mean length (50 episodes) 25.200000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -282.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1465\n",
      "exploration 0.254083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99500\n",
      "mean reward (50 episodes) 42.000000\n",
      "mean length (50 episodes) 25.900000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -282.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1486\n",
      "exploration 0.252875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100000\n",
      "mean reward (50 episodes) 48.640000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 14.000000\n",
      "max_episode_length (50 episodes) 51.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1508\n",
      "exploration 0.251667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100500\n",
      "mean reward (50 episodes) 45.560000\n",
      "mean length (50 episodes) 25.800000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -5.000000\n",
      "max_episode_length (50 episodes) 78.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1521\n",
      "exploration 0.250458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 101000\n",
      "mean reward (50 episodes) 43.020000\n",
      "mean length (50 episodes) 28.500000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -5.000000\n",
      "max_episode_length (50 episodes) 78.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1536\n",
      "exploration 0.249250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 101500\n",
      "mean reward (50 episodes) 42.420000\n",
      "mean length (50 episodes) 29.620000\n",
      "max_episode_reward (50 episodes) 62.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -5.000000\n",
      "max_episode_length (50 episodes) 78.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1556\n",
      "exploration 0.248042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102000\n",
      "mean reward (50 episodes) 45.960000\n",
      "mean length (50 episodes) 24.060000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -6.000000\n",
      "max_episode_length (50 episodes) 60.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1579\n",
      "exploration 0.246833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102500\n",
      "mean reward (50 episodes) 49.240000\n",
      "mean length (50 episodes) 19.520000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) -6.000000\n",
      "max_episode_length (50 episodes) 51.000000\n",
      "best mean reward 52.800000\n",
      "episodes 1605\n",
      "exploration 0.245625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103000\n",
      "mean reward (50 episodes) 52.140000\n",
      "mean length (50 episodes) 18.360000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 19.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 53.900000\n",
      "episodes 1630\n",
      "exploration 0.244417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103500\n",
      "mean reward (50 episodes) 54.520000\n",
      "mean length (50 episodes) 18.420000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 19.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 54.520000\n",
      "episodes 1656\n",
      "exploration 0.243208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104000\n",
      "mean reward (50 episodes) 55.360000\n",
      "mean length (50 episodes) 19.040000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 36.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 55.360000\n",
      "episodes 1680\n",
      "exploration 0.242000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104500\n",
      "mean reward (50 episodes) 53.200000\n",
      "mean length (50 episodes) 20.200000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 15.000000\n",
      "max_episode_length (50 episodes) 54.000000\n",
      "best mean reward 55.440000\n",
      "episodes 1703\n",
      "exploration 0.240792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105000\n",
      "mean reward (50 episodes) 51.960000\n",
      "mean length (50 episodes) 21.380000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 15.000000\n",
      "max_episode_length (50 episodes) 54.000000\n",
      "best mean reward 55.440000\n",
      "episodes 1725\n",
      "exploration 0.239583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105500\n",
      "mean reward (50 episodes) 53.800000\n",
      "mean length (50 episodes) 20.900000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 33.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 55.440000\n",
      "episodes 1748\n",
      "exploration 0.238375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106000\n",
      "mean reward (50 episodes) 55.520000\n",
      "mean length (50 episodes) 20.200000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 30.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 55.740000\n",
      "episodes 1772\n",
      "exploration 0.237167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106500\n",
      "mean reward (50 episodes) 55.580000\n",
      "mean length (50 episodes) 20.020000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 22.000000\n",
      "max_episode_length (50 episodes) 41.000000\n",
      "best mean reward 56.540000\n",
      "episodes 1795\n",
      "exploration 0.235958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107000\n",
      "mean reward (50 episodes) 54.260000\n",
      "mean length (50 episodes) 20.180000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 22.000000\n",
      "max_episode_length (50 episodes) 41.000000\n",
      "best mean reward 56.540000\n",
      "episodes 1820\n",
      "exploration 0.234750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107500\n",
      "mean reward (50 episodes) 55.740000\n",
      "mean length (50 episodes) 20.140000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 23.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 56.540000\n",
      "episodes 1842\n",
      "exploration 0.233542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108000\n",
      "mean reward (50 episodes) 56.180000\n",
      "mean length (50 episodes) 20.520000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 42.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 56.540000\n",
      "episodes 1866\n",
      "exploration 0.232333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108500\n",
      "mean reward (50 episodes) 53.660000\n",
      "mean length (50 episodes) 19.780000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 14.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 56.540000\n",
      "episodes 1891\n",
      "exploration 0.231125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109000\n",
      "mean reward (50 episodes) 54.580000\n",
      "mean length (50 episodes) 18.760000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 14.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 56.540000\n",
      "episodes 1917\n",
      "exploration 0.229917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109500\n",
      "mean reward (50 episodes) 56.640000\n",
      "mean length (50 episodes) 19.240000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 29.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 57.020000\n",
      "episodes 1941\n",
      "exploration 0.228708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110000\n",
      "mean reward (50 episodes) 56.440000\n",
      "mean length (50 episodes) 19.260000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 23.000000\n",
      "max_episode_length (50 episodes) 37.000000\n",
      "best mean reward 57.020000\n",
      "episodes 1966\n",
      "exploration 0.227500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110500\n",
      "mean reward (50 episodes) 54.500000\n",
      "mean length (50 episodes) 19.960000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) 20.000000\n",
      "max_episode_length (50 episodes) 37.000000\n",
      "best mean reward 57.020000\n",
      "episodes 1989\n",
      "exploration 0.226292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111000\n",
      "mean reward (50 episodes) 54.800000\n",
      "mean length (50 episodes) 20.220000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 20.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2012\n",
      "exploration 0.225083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111500\n",
      "mean reward (50 episodes) 54.220000\n",
      "mean length (50 episodes) 19.980000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 21.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2037\n",
      "exploration 0.223875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112000\n",
      "mean reward (50 episodes) 53.980000\n",
      "mean length (50 episodes) 19.620000\n",
      "max_episode_reward (50 episodes) 63.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 21.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2061\n",
      "exploration 0.222667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112500\n",
      "mean reward (50 episodes) 55.920000\n",
      "mean length (50 episodes) 20.520000\n",
      "max_episode_reward (50 episodes) 68.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 29.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2083\n",
      "exploration 0.221458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 113000\n",
      "mean reward (50 episodes) 54.560000\n",
      "mean length (50 episodes) 20.660000\n",
      "max_episode_reward (50 episodes) 68.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 8.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2107\n",
      "exploration 0.220250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 113500\n",
      "mean reward (50 episodes) 55.440000\n",
      "mean length (50 episodes) 19.840000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 8.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2131\n",
      "exploration 0.219042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114000\n",
      "mean reward (50 episodes) 56.400000\n",
      "mean length (50 episodes) 20.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 31.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2152\n",
      "exploration 0.217833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114500\n",
      "mean reward (50 episodes) 54.820000\n",
      "mean length (50 episodes) 21.180000\n",
      "max_episode_reward (50 episodes) 67.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 31.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2176\n",
      "exploration 0.216625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115000\n",
      "mean reward (50 episodes) 55.640000\n",
      "mean length (50 episodes) 19.440000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 29.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2201\n",
      "exploration 0.215417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115500\n",
      "mean reward (50 episodes) 56.080000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 13.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 57.020000\n",
      "episodes 2225\n",
      "exploration 0.214208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116000\n",
      "mean reward (50 episodes) 56.420000\n",
      "mean length (50 episodes) 19.660000\n",
      "max_episode_reward (50 episodes) 67.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 58.020000\n",
      "episodes 2249\n",
      "exploration 0.213000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116500\n",
      "mean reward (50 episodes) 57.220000\n",
      "mean length (50 episodes) 19.360000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 38.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 58.020000\n",
      "episodes 2273\n",
      "exploration 0.211792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117000\n",
      "mean reward (50 episodes) 58.900000\n",
      "mean length (50 episodes) 19.580000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 39.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 58.900000\n",
      "episodes 2298\n",
      "exploration 0.210583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117500\n",
      "mean reward (50 episodes) 58.100000\n",
      "mean length (50 episodes) 21.380000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 38.000000\n",
      "max_episode_length (50 episodes) 45.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2317\n",
      "exploration 0.209375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118000\n",
      "mean reward (50 episodes) 55.540000\n",
      "mean length (50 episodes) 24.140000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) -2.000000\n",
      "max_episode_length (50 episodes) 69.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2334\n",
      "exploration 0.208167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118500\n",
      "mean reward (50 episodes) 54.660000\n",
      "mean length (50 episodes) 25.080000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -2.000000\n",
      "max_episode_length (50 episodes) 69.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2355\n",
      "exploration 0.206958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119000\n",
      "mean reward (50 episodes) 57.660000\n",
      "mean length (50 episodes) 23.040000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 9.000000\n",
      "max_episode_length (50 episodes) 62.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2378\n",
      "exploration 0.205750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119500\n",
      "mean reward (50 episodes) 58.520000\n",
      "mean length (50 episodes) 21.940000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 31.000000\n",
      "max_episode_length (50 episodes) 50.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2400\n",
      "exploration 0.204542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120000\n",
      "mean reward (50 episodes) 56.620000\n",
      "mean length (50 episodes) 23.020000\n",
      "max_episode_reward (50 episodes) 68.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 31.000000\n",
      "max_episode_length (50 episodes) 50.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2420\n",
      "exploration 0.203333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120500\n",
      "mean reward (50 episodes) 56.380000\n",
      "mean length (50 episodes) 22.380000\n",
      "max_episode_reward (50 episodes) 68.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 31.000000\n",
      "max_episode_length (50 episodes) 41.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2442\n",
      "exploration 0.202125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121000\n",
      "mean reward (50 episodes) 55.740000\n",
      "mean length (50 episodes) 22.120000\n",
      "max_episode_reward (50 episodes) 68.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 16.000000\n",
      "max_episode_length (50 episodes) 38.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2464\n",
      "exploration 0.200917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121500\n",
      "mean reward (50 episodes) 58.680000\n",
      "mean length (50 episodes) 20.840000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 16.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 59.000000\n",
      "episodes 2488\n",
      "exploration 0.199708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122000\n",
      "mean reward (50 episodes) 61.980000\n",
      "mean length (50 episodes) 19.580000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 37.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 62.220000\n",
      "episodes 2513\n",
      "exploration 0.198500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122500\n",
      "mean reward (50 episodes) 62.120000\n",
      "mean length (50 episodes) 19.420000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 39.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 62.280000\n",
      "episodes 2537\n",
      "exploration 0.197292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123000\n",
      "mean reward (50 episodes) 59.040000\n",
      "mean length (50 episodes) 22.260000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 13.000000\n",
      "max_episode_length (50 episodes) 64.000000\n",
      "best mean reward 62.280000\n",
      "episodes 2555\n",
      "exploration 0.196083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123500\n",
      "mean reward (50 episodes) 60.200000\n",
      "mean length (50 episodes) 22.180000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 13.000000\n",
      "max_episode_length (50 episodes) 64.000000\n",
      "best mean reward 62.280000\n",
      "episodes 2580\n",
      "exploration 0.194875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124000\n",
      "mean reward (50 episodes) 62.900000\n",
      "mean length (50 episodes) 19.660000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 62.980000\n",
      "episodes 2604\n",
      "exploration 0.193667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124500\n",
      "mean reward (50 episodes) 62.540000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2628\n",
      "exploration 0.192458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 125000\n",
      "mean reward (50 episodes) 60.940000\n",
      "mean length (50 episodes) 20.840000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 18.000000\n",
      "max_episode_length (50 episodes) 64.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2649\n",
      "exploration 0.191250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 125500\n",
      "mean reward (50 episodes) 59.080000\n",
      "mean length (50 episodes) 21.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 11.000000\n",
      "max_episode_length (50 episodes) 64.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2671\n",
      "exploration 0.190042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126000\n",
      "mean reward (50 episodes) 59.460000\n",
      "mean length (50 episodes) 21.740000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 11.000000\n",
      "max_episode_length (50 episodes) 64.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2695\n",
      "exploration 0.188833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126500\n",
      "mean reward (50 episodes) 61.160000\n",
      "mean length (50 episodes) 20.560000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 27.000000\n",
      "max_episode_length (50 episodes) 49.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2717\n",
      "exploration 0.187625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127000\n",
      "mean reward (50 episodes) 60.820000\n",
      "mean length (50 episodes) 20.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 27.000000\n",
      "max_episode_length (50 episodes) 49.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2741\n",
      "exploration 0.186417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127500\n",
      "mean reward (50 episodes) 60.860000\n",
      "mean length (50 episodes) 19.460000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 30.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2766\n",
      "exploration 0.185208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128000\n",
      "mean reward (50 episodes) 61.580000\n",
      "mean length (50 episodes) 19.000000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 38.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2791\n",
      "exploration 0.184000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128500\n",
      "mean reward (50 episodes) 61.940000\n",
      "mean length (50 episodes) 19.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 42.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2814\n",
      "exploration 0.182792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129000\n",
      "mean reward (50 episodes) 61.180000\n",
      "mean length (50 episodes) 20.700000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 42.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2837\n",
      "exploration 0.181583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129500\n",
      "mean reward (50 episodes) 61.520000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 70.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 16.000000\n",
      "max_episode_length (50 episodes) 39.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2861\n",
      "exploration 0.180375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130000\n",
      "mean reward (50 episodes) 61.800000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 70.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 16.000000\n",
      "max_episode_length (50 episodes) 39.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2884\n",
      "exploration 0.179167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130500\n",
      "mean reward (50 episodes) 61.860000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 41.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2910\n",
      "exploration 0.177958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131000\n",
      "mean reward (50 episodes) 61.200000\n",
      "mean length (50 episodes) 19.620000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 30.000000\n",
      "max_episode_length (50 episodes) 37.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2933\n",
      "exploration 0.176750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131500\n",
      "mean reward (50 episodes) 58.700000\n",
      "mean length (50 episodes) 21.680000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 5.000000\n",
      "max_episode_length (50 episodes) 51.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2954\n",
      "exploration 0.175542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132000\n",
      "mean reward (50 episodes) 60.100000\n",
      "mean length (50 episodes) 21.120000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 5.000000\n",
      "max_episode_length (50 episodes) 51.000000\n",
      "best mean reward 63.020000\n",
      "episodes 2978\n",
      "exploration 0.174333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132500\n",
      "mean reward (50 episodes) 62.180000\n",
      "mean length (50 episodes) 20.320000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 30.000000\n",
      "max_episode_length (50 episodes) 51.000000\n",
      "best mean reward 63.020000\n",
      "episodes 3002\n",
      "exploration 0.173125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133000\n",
      "mean reward (50 episodes) 63.560000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 43.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 63.560000\n",
      "episodes 3026\n",
      "exploration 0.171917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133500\n",
      "mean reward (50 episodes) 64.000000\n",
      "mean length (50 episodes) 19.020000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 43.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 64.540000\n",
      "episodes 3052\n",
      "exploration 0.170708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134000\n",
      "mean reward (50 episodes) 64.560000\n",
      "mean length (50 episodes) 18.560000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 64.560000\n",
      "episodes 3077\n",
      "exploration 0.169500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134500\n",
      "mean reward (50 episodes) 62.200000\n",
      "mean length (50 episodes) 19.720000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 21.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 64.560000\n",
      "episodes 3100\n",
      "exploration 0.168292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135000\n",
      "mean reward (50 episodes) 62.160000\n",
      "mean length (50 episodes) 19.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 21.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 64.560000\n",
      "episodes 3125\n",
      "exploration 0.167083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135500\n",
      "mean reward (50 episodes) 63.480000\n",
      "mean length (50 episodes) 19.340000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 40.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 64.560000\n",
      "episodes 3149\n",
      "exploration 0.165875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136000\n",
      "mean reward (50 episodes) 63.240000\n",
      "mean length (50 episodes) 19.280000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 21.000000\n",
      "max_episode_length (50 episodes) 38.000000\n",
      "best mean reward 64.560000\n",
      "episodes 3175\n",
      "exploration 0.164667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136500\n",
      "mean reward (50 episodes) 63.280000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 21.000000\n",
      "max_episode_length (50 episodes) 38.000000\n",
      "best mean reward 64.560000\n",
      "episodes 3199\n",
      "exploration 0.163458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 137000\n",
      "mean reward (50 episodes) 63.620000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 36.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 64.560000\n",
      "episodes 3224\n",
      "exploration 0.162250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 137500\n",
      "mean reward (50 episodes) 64.580000\n",
      "mean length (50 episodes) 18.520000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 50.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 64.580000\n",
      "episodes 3250\n",
      "exploration 0.161042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138000\n",
      "mean reward (50 episodes) 64.300000\n",
      "mean length (50 episodes) 18.640000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3275\n",
      "exploration 0.159833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138500\n",
      "mean reward (50 episodes) 63.780000\n",
      "mean length (50 episodes) 18.900000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3300\n",
      "exploration 0.158625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139000\n",
      "mean reward (50 episodes) 63.240000\n",
      "mean length (50 episodes) 19.200000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 34.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3324\n",
      "exploration 0.157417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139500\n",
      "mean reward (50 episodes) 63.540000\n",
      "mean length (50 episodes) 19.580000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 34.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3349\n",
      "exploration 0.156208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140000\n",
      "mean reward (50 episodes) 64.040000\n",
      "mean length (50 episodes) 18.160000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 37.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3376\n",
      "exploration 0.155000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140500\n",
      "mean reward (50 episodes) 63.180000\n",
      "mean length (50 episodes) 18.080000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 32.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3402\n",
      "exploration 0.153792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141000\n",
      "mean reward (50 episodes) 62.640000\n",
      "mean length (50 episodes) 18.980000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 22.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3426\n",
      "exploration 0.152583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141500\n",
      "mean reward (50 episodes) 60.880000\n",
      "mean length (50 episodes) 20.760000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -18.000000\n",
      "max_episode_length (50 episodes) 97.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3448\n",
      "exploration 0.151375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142000\n",
      "mean reward (50 episodes) 61.020000\n",
      "mean length (50 episodes) 20.500000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) -18.000000\n",
      "max_episode_length (50 episodes) 97.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3473\n",
      "exploration 0.150167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142500\n",
      "mean reward (50 episodes) 63.460000\n",
      "mean length (50 episodes) 18.680000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 33.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3499\n",
      "exploration 0.148958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143000\n",
      "mean reward (50 episodes) 64.360000\n",
      "mean length (50 episodes) 18.240000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3525\n",
      "exploration 0.147750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143500\n",
      "mean reward (50 episodes) 63.420000\n",
      "mean length (50 episodes) 18.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3550\n",
      "exploration 0.146542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144000\n",
      "mean reward (50 episodes) 62.860000\n",
      "mean length (50 episodes) 19.000000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 64.800000\n",
      "episodes 3575\n",
      "exploration 0.145333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144500\n",
      "mean reward (50 episodes) 64.940000\n",
      "mean length (50 episodes) 18.420000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 65.020000\n",
      "episodes 3602\n",
      "exploration 0.144125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145000\n",
      "mean reward (50 episodes) 64.460000\n",
      "mean length (50 episodes) 18.100000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3627\n",
      "exploration 0.142917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145500\n",
      "mean reward (50 episodes) 62.880000\n",
      "mean length (50 episodes) 19.380000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3651\n",
      "exploration 0.141708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146000\n",
      "mean reward (50 episodes) 62.960000\n",
      "mean length (50 episodes) 20.180000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 38.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3674\n",
      "exploration 0.140500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146500\n",
      "mean reward (50 episodes) 63.580000\n",
      "mean length (50 episodes) 19.660000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 38.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3699\n",
      "exploration 0.139292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147000\n",
      "mean reward (50 episodes) 63.200000\n",
      "mean length (50 episodes) 19.020000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 41.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3724\n",
      "exploration 0.138083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147500\n",
      "mean reward (50 episodes) 63.460000\n",
      "mean length (50 episodes) 18.960000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 41.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3749\n",
      "exploration 0.136875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148000\n",
      "mean reward (50 episodes) 62.520000\n",
      "mean length (50 episodes) 19.960000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 9.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3772\n",
      "exploration 0.135667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148500\n",
      "mean reward (50 episodes) 62.700000\n",
      "mean length (50 episodes) 19.300000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 9.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3798\n",
      "exploration 0.134458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 149000\n",
      "mean reward (50 episodes) 64.280000\n",
      "mean length (50 episodes) 18.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 43.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3822\n",
      "exploration 0.133250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 149500\n",
      "mean reward (50 episodes) 63.920000\n",
      "mean length (50 episodes) 19.680000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 37.000000\n",
      "max_episode_length (50 episodes) 39.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3846\n",
      "exploration 0.132042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150000\n",
      "mean reward (50 episodes) 63.360000\n",
      "mean length (50 episodes) 19.840000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 34.000000\n",
      "max_episode_length (50 episodes) 39.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3870\n",
      "exploration 0.130833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150500\n",
      "mean reward (50 episodes) 63.600000\n",
      "mean length (50 episodes) 19.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 34.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 65.360000\n",
      "episodes 3895\n",
      "exploration 0.129625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151000\n",
      "mean reward (50 episodes) 65.500000\n",
      "mean length (50 episodes) 18.560000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 52.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 65.500000\n",
      "episodes 3921\n",
      "exploration 0.128417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151500\n",
      "mean reward (50 episodes) 64.520000\n",
      "mean length (50 episodes) 19.280000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 36.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 65.660000\n",
      "episodes 3944\n",
      "exploration 0.127208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152000\n",
      "mean reward (50 episodes) 63.620000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 27.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 65.660000\n",
      "episodes 3968\n",
      "exploration 0.126000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152500\n",
      "mean reward (50 episodes) 64.300000\n",
      "mean length (50 episodes) 19.460000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 27.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 65.660000\n",
      "episodes 3993\n",
      "exploration 0.124792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153000\n",
      "mean reward (50 episodes) 64.720000\n",
      "mean length (50 episodes) 19.300000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 50.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 65.660000\n",
      "episodes 4018\n",
      "exploration 0.123583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153500\n",
      "mean reward (50 episodes) 65.240000\n",
      "mean length (50 episodes) 19.080000\n",
      "max_episode_reward (50 episodes) 70.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4043\n",
      "exploration 0.122375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154000\n",
      "mean reward (50 episodes) 65.080000\n",
      "mean length (50 episodes) 19.300000\n",
      "max_episode_reward (50 episodes) 70.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4067\n",
      "exploration 0.121167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154500\n",
      "mean reward (50 episodes) 64.720000\n",
      "mean length (50 episodes) 19.520000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4091\n",
      "exploration 0.119958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155000\n",
      "mean reward (50 episodes) 63.380000\n",
      "mean length (50 episodes) 20.620000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 35.000000\n",
      "max_episode_length (50 episodes) 45.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4113\n",
      "exploration 0.118750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155500\n",
      "mean reward (50 episodes) 63.120000\n",
      "mean length (50 episodes) 21.360000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 35.000000\n",
      "max_episode_length (50 episodes) 45.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4136\n",
      "exploration 0.117542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156000\n",
      "mean reward (50 episodes) 63.640000\n",
      "mean length (50 episodes) 20.740000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 35.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4159\n",
      "exploration 0.116333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156500\n",
      "mean reward (50 episodes) 64.720000\n",
      "mean length (50 episodes) 19.960000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4183\n",
      "exploration 0.115125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157000\n",
      "mean reward (50 episodes) 63.840000\n",
      "mean length (50 episodes) 20.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4206\n",
      "exploration 0.113917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157500\n",
      "mean reward (50 episodes) 63.700000\n",
      "mean length (50 episodes) 20.800000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4229\n",
      "exploration 0.112708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158000\n",
      "mean reward (50 episodes) 63.340000\n",
      "mean length (50 episodes) 20.840000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4252\n",
      "exploration 0.111500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158500\n",
      "mean reward (50 episodes) 64.180000\n",
      "mean length (50 episodes) 20.460000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 38.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4276\n",
      "exploration 0.110292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159000\n",
      "mean reward (50 episodes) 65.380000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 52.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4301\n",
      "exploration 0.109083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159500\n",
      "mean reward (50 episodes) 64.940000\n",
      "mean length (50 episodes) 19.880000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 40.000000\n",
      "max_episode_length (50 episodes) 47.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4324\n",
      "exploration 0.107875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160000\n",
      "mean reward (50 episodes) 64.360000\n",
      "mean length (50 episodes) 19.760000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 40.000000\n",
      "max_episode_length (50 episodes) 47.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4349\n",
      "exploration 0.106667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160500\n",
      "mean reward (50 episodes) 63.720000\n",
      "mean length (50 episodes) 19.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 41.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4373\n",
      "exploration 0.105458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 161000\n",
      "mean reward (50 episodes) 64.580000\n",
      "mean length (50 episodes) 20.020000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 41.000000\n",
      "max_episode_length (50 episodes) 34.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4397\n",
      "exploration 0.104250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 161500\n",
      "mean reward (50 episodes) 65.340000\n",
      "mean length (50 episodes) 19.800000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4421\n",
      "exploration 0.103042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162000\n",
      "mean reward (50 episodes) 65.180000\n",
      "mean length (50 episodes) 19.360000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 65.780000\n",
      "episodes 4446\n",
      "exploration 0.101833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162500\n",
      "mean reward (50 episodes) 66.360000\n",
      "mean length (50 episodes) 18.560000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4472\n",
      "exploration 0.100625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163000\n",
      "mean reward (50 episodes) 66.420000\n",
      "mean length (50 episodes) 18.980000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 56.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4496\n",
      "exploration 0.099417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163500\n",
      "mean reward (50 episodes) 66.020000\n",
      "mean length (50 episodes) 19.880000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 56.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4520\n",
      "exploration 0.098208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164000\n",
      "mean reward (50 episodes) 66.100000\n",
      "mean length (50 episodes) 19.680000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 57.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4544\n",
      "exploration 0.097000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164500\n",
      "mean reward (50 episodes) 65.540000\n",
      "mean length (50 episodes) 19.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4568\n",
      "exploration 0.095792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165000\n",
      "mean reward (50 episodes) 65.900000\n",
      "mean length (50 episodes) 19.340000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4593\n",
      "exploration 0.094583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165500\n",
      "mean reward (50 episodes) 66.200000\n",
      "mean length (50 episodes) 18.440000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 52.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4619\n",
      "exploration 0.093375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166000\n",
      "mean reward (50 episodes) 65.560000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 52.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4642\n",
      "exploration 0.092167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166500\n",
      "mean reward (50 episodes) 64.720000\n",
      "mean length (50 episodes) 20.160000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4666\n",
      "exploration 0.090958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167000\n",
      "mean reward (50 episodes) 64.360000\n",
      "mean length (50 episodes) 19.560000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4691\n",
      "exploration 0.089750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167500\n",
      "mean reward (50 episodes) 64.920000\n",
      "mean length (50 episodes) 18.900000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4716\n",
      "exploration 0.088542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168000\n",
      "mean reward (50 episodes) 65.320000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4740\n",
      "exploration 0.087333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168500\n",
      "mean reward (50 episodes) 66.060000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 55.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4763\n",
      "exploration 0.086125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169000\n",
      "mean reward (50 episodes) 66.580000\n",
      "mean length (50 episodes) 19.520000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4789\n",
      "exploration 0.084917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169500\n",
      "mean reward (50 episodes) 66.240000\n",
      "mean length (50 episodes) 18.960000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4813\n",
      "exploration 0.083708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170000\n",
      "mean reward (50 episodes) 66.220000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 66.620000\n",
      "episodes 4838\n",
      "exploration 0.082500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170500\n",
      "mean reward (50 episodes) 66.760000\n",
      "mean length (50 episodes) 18.480000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 55.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 66.900000\n",
      "episodes 4864\n",
      "exploration 0.081292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171000\n",
      "mean reward (50 episodes) 66.420000\n",
      "mean length (50 episodes) 18.500000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 66.900000\n",
      "episodes 4889\n",
      "exploration 0.080083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171500\n",
      "mean reward (50 episodes) 66.140000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 66.900000\n",
      "episodes 4913\n",
      "exploration 0.078875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172000\n",
      "mean reward (50 episodes) 65.860000\n",
      "mean length (50 episodes) 19.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.900000\n",
      "episodes 4937\n",
      "exploration 0.077667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172500\n",
      "mean reward (50 episodes) 65.420000\n",
      "mean length (50 episodes) 20.500000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 43.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 66.900000\n",
      "episodes 4959\n",
      "exploration 0.076458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 173000\n",
      "mean reward (50 episodes) 66.320000\n",
      "mean length (50 episodes) 20.380000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 43.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 66.900000\n",
      "episodes 4983\n",
      "exploration 0.075250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 173500\n",
      "mean reward (50 episodes) 66.460000\n",
      "mean length (50 episodes) 20.160000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 55.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.900000\n",
      "episodes 5007\n",
      "exploration 0.074042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174000\n",
      "mean reward (50 episodes) 66.200000\n",
      "mean length (50 episodes) 20.480000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 55.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 66.900000\n",
      "episodes 5030\n",
      "exploration 0.072833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174500\n",
      "mean reward (50 episodes) 66.120000\n",
      "mean length (50 episodes) 20.560000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 57.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 66.900000\n",
      "episodes 5053\n",
      "exploration 0.071625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175000\n",
      "mean reward (50 episodes) 66.100000\n",
      "mean length (50 episodes) 20.280000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 66.900000\n",
      "episodes 5077\n",
      "exploration 0.070417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175500\n",
      "mean reward (50 episodes) 66.100000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.900000\n",
      "episodes 5101\n",
      "exploration 0.069208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176000\n",
      "mean reward (50 episodes) 66.320000\n",
      "mean length (50 episodes) 20.080000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.900000\n",
      "episodes 5124\n",
      "exploration 0.068000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176500\n",
      "mean reward (50 episodes) 66.520000\n",
      "mean length (50 episodes) 20.280000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.900000\n",
      "episodes 5148\n",
      "exploration 0.066792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177000\n",
      "mean reward (50 episodes) 66.700000\n",
      "mean length (50 episodes) 20.440000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 66.980000\n",
      "episodes 5171\n",
      "exploration 0.065583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177500\n",
      "mean reward (50 episodes) 66.220000\n",
      "mean length (50 episodes) 20.160000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 66.980000\n",
      "episodes 5195\n",
      "exploration 0.064375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178000\n",
      "mean reward (50 episodes) 66.340000\n",
      "mean length (50 episodes) 18.800000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 66.980000\n",
      "episodes 5221\n",
      "exploration 0.063167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178500\n",
      "mean reward (50 episodes) 66.700000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 67.160000\n",
      "episodes 5245\n",
      "exploration 0.061958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179000\n",
      "mean reward (50 episodes) 66.300000\n",
      "mean length (50 episodes) 20.780000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 67.160000\n",
      "episodes 5267\n",
      "exploration 0.060750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179500\n",
      "mean reward (50 episodes) 66.700000\n",
      "mean length (50 episodes) 20.340000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 67.160000\n",
      "episodes 5292\n",
      "exploration 0.059542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180000\n",
      "mean reward (50 episodes) 67.140000\n",
      "mean length (50 episodes) 20.100000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 67.340000\n",
      "episodes 5314\n",
      "exploration 0.058333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180500\n",
      "mean reward (50 episodes) 67.140000\n",
      "mean length (50 episodes) 20.000000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 67.340000\n",
      "episodes 5339\n",
      "exploration 0.057125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181000\n",
      "mean reward (50 episodes) 67.500000\n",
      "mean length (50 episodes) 18.760000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 67.500000\n",
      "episodes 5365\n",
      "exploration 0.055917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181500\n",
      "mean reward (50 episodes) 67.020000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 67.520000\n",
      "episodes 5389\n",
      "exploration 0.054708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182000\n",
      "mean reward (50 episodes) 66.840000\n",
      "mean length (50 episodes) 19.760000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 67.520000\n",
      "episodes 5413\n",
      "exploration 0.053500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182500\n",
      "mean reward (50 episodes) 67.020000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 56.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 67.520000\n",
      "episodes 5438\n",
      "exploration 0.052292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183000\n",
      "mean reward (50 episodes) 67.560000\n",
      "mean length (50 episodes) 18.980000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 56.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 67.660000\n",
      "episodes 5463\n",
      "exploration 0.051083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183500\n",
      "mean reward (50 episodes) 67.540000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 50.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 67.760000\n",
      "episodes 5487\n",
      "exploration 0.049875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184000\n",
      "mean reward (50 episodes) 67.280000\n",
      "mean length (50 episodes) 19.100000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 50.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 67.760000\n",
      "episodes 5513\n",
      "exploration 0.048667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184500\n",
      "mean reward (50 episodes) 67.660000\n",
      "mean length (50 episodes) 18.880000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 55.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 67.760000\n",
      "episodes 5538\n",
      "exploration 0.047458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 185000\n",
      "mean reward (50 episodes) 67.920000\n",
      "mean length (50 episodes) 18.100000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 67.920000\n",
      "episodes 5565\n",
      "exploration 0.046250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 185500\n",
      "mean reward (50 episodes) 67.620000\n",
      "mean length (50 episodes) 18.380000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.020000\n",
      "episodes 5589\n",
      "exploration 0.045042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186000\n",
      "mean reward (50 episodes) 67.560000\n",
      "mean length (50 episodes) 19.700000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 56.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.020000\n",
      "episodes 5613\n",
      "exploration 0.043833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186500\n",
      "mean reward (50 episodes) 67.600000\n",
      "mean length (50 episodes) 19.880000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.020000\n",
      "episodes 5637\n",
      "exploration 0.042625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187000\n",
      "mean reward (50 episodes) 67.880000\n",
      "mean length (50 episodes) 18.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.020000\n",
      "episodes 5663\n",
      "exploration 0.041417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187500\n",
      "mean reward (50 episodes) 68.120000\n",
      "mean length (50 episodes) 18.100000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5689\n",
      "exploration 0.040208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188000\n",
      "mean reward (50 episodes) 67.640000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5712\n",
      "exploration 0.039000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188500\n",
      "mean reward (50 episodes) 67.720000\n",
      "mean length (50 episodes) 19.440000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5738\n",
      "exploration 0.037792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189000\n",
      "mean reward (50 episodes) 68.160000\n",
      "mean length (50 episodes) 17.740000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 57.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5765\n",
      "exploration 0.036583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189500\n",
      "mean reward (50 episodes) 68.200000\n",
      "mean length (50 episodes) 18.060000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5790\n",
      "exploration 0.035375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190000\n",
      "mean reward (50 episodes) 67.960000\n",
      "mean length (50 episodes) 19.040000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5814\n",
      "exploration 0.034167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190500\n",
      "mean reward (50 episodes) 67.760000\n",
      "mean length (50 episodes) 19.660000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5838\n",
      "exploration 0.032958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191000\n",
      "mean reward (50 episodes) 67.960000\n",
      "mean length (50 episodes) 19.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5863\n",
      "exploration 0.031750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191500\n",
      "mean reward (50 episodes) 68.240000\n",
      "mean length (50 episodes) 19.160000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5888\n",
      "exploration 0.030542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192000\n",
      "mean reward (50 episodes) 67.780000\n",
      "mean length (50 episodes) 18.840000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5913\n",
      "exploration 0.029333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192500\n",
      "mean reward (50 episodes) 67.460000\n",
      "mean length (50 episodes) 18.420000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5939\n",
      "exploration 0.028125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193000\n",
      "mean reward (50 episodes) 67.880000\n",
      "mean length (50 episodes) 18.040000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.360000\n",
      "episodes 5966\n",
      "exploration 0.026917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193500\n",
      "mean reward (50 episodes) 68.140000\n",
      "mean length (50 episodes) 18.840000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 68.400000\n",
      "episodes 5990\n",
      "exploration 0.025708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194000\n",
      "mean reward (50 episodes) 67.860000\n",
      "mean length (50 episodes) 19.500000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 68.400000\n",
      "episodes 6015\n",
      "exploration 0.024500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194500\n",
      "mean reward (50 episodes) 68.240000\n",
      "mean length (50 episodes) 18.020000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.400000\n",
      "episodes 6042\n",
      "exploration 0.023292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195000\n",
      "mean reward (50 episodes) 68.560000\n",
      "mean length (50 episodes) 18.380000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6067\n",
      "exploration 0.022083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195500\n",
      "mean reward (50 episodes) 68.620000\n",
      "mean length (50 episodes) 19.260000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6091\n",
      "exploration 0.020875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196000\n",
      "mean reward (50 episodes) 68.680000\n",
      "mean length (50 episodes) 19.000000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6117\n",
      "exploration 0.019667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196500\n",
      "mean reward (50 episodes) 68.180000\n",
      "mean length (50 episodes) 17.440000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6145\n",
      "exploration 0.018458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 197000\n",
      "mean reward (50 episodes) 68.040000\n",
      "mean length (50 episodes) 17.700000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6170\n",
      "exploration 0.017250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 197500\n",
      "mean reward (50 episodes) 68.540000\n",
      "mean length (50 episodes) 19.120000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6195\n",
      "exploration 0.016042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198000\n",
      "mean reward (50 episodes) 68.260000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6219\n",
      "exploration 0.014833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198500\n",
      "mean reward (50 episodes) 68.100000\n",
      "mean length (50 episodes) 19.160000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6244\n",
      "exploration 0.013625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199000\n",
      "mean reward (50 episodes) 68.640000\n",
      "mean length (50 episodes) 19.100000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.740000\n",
      "episodes 6269\n",
      "exploration 0.012417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199500\n",
      "mean reward (50 episodes) 68.820000\n",
      "mean length (50 episodes) 19.300000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.820000\n",
      "episodes 6293\n",
      "exploration 0.011208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 200000\n",
      "mean reward (50 episodes) 68.720000\n",
      "mean length (50 episodes) 19.380000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.840000\n",
      "episodes 6318\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 200500\n",
      "mean reward (50 episodes) 68.660000\n",
      "mean length (50 episodes) 19.260000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.840000\n",
      "episodes 6343\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 201000\n",
      "mean reward (50 episodes) 68.680000\n",
      "mean length (50 episodes) 18.660000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.840000\n",
      "episodes 6369\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 201500\n",
      "mean reward (50 episodes) 68.580000\n",
      "mean length (50 episodes) 17.660000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.840000\n",
      "episodes 6396\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 202000\n",
      "mean reward (50 episodes) 68.500000\n",
      "mean length (50 episodes) 18.300000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.840000\n",
      "episodes 6421\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 202500\n",
      "mean reward (50 episodes) 68.740000\n",
      "mean length (50 episodes) 18.680000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.840000\n",
      "episodes 6447\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 203000\n",
      "mean reward (50 episodes) 68.940000\n",
      "mean length (50 episodes) 17.360000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6475\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 203500\n",
      "mean reward (50 episodes) 68.780000\n",
      "mean length (50 episodes) 17.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6501\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 204000\n",
      "mean reward (50 episodes) 68.800000\n",
      "mean length (50 episodes) 18.620000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6526\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 204500\n",
      "mean reward (50 episodes) 68.760000\n",
      "mean length (50 episodes) 19.300000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6550\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 205000\n",
      "mean reward (50 episodes) 68.660000\n",
      "mean length (50 episodes) 19.760000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6574\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 205500\n",
      "mean reward (50 episodes) 68.760000\n",
      "mean length (50 episodes) 19.880000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6598\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 206000\n",
      "mean reward (50 episodes) 68.740000\n",
      "mean length (50 episodes) 18.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6625\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 206500\n",
      "mean reward (50 episodes) 68.860000\n",
      "mean length (50 episodes) 17.940000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 68.940000\n",
      "episodes 6651\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 207000\n",
      "mean reward (50 episodes) 68.940000\n",
      "mean length (50 episodes) 18.780000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 68.960000\n",
      "episodes 6676\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 207500\n",
      "mean reward (50 episodes) 68.820000\n",
      "mean length (50 episodes) 19.400000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6700\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 208000\n",
      "mean reward (50 episodes) 68.800000\n",
      "mean length (50 episodes) 18.720000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6726\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 208500\n",
      "mean reward (50 episodes) 68.600000\n",
      "mean length (50 episodes) 18.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6751\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 209000\n",
      "mean reward (50 episodes) 68.480000\n",
      "mean length (50 episodes) 19.080000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6776\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 209500\n",
      "mean reward (50 episodes) 68.760000\n",
      "mean length (50 episodes) 18.460000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6802\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 210000\n",
      "mean reward (50 episodes) 68.840000\n",
      "mean length (50 episodes) 17.980000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6828\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 210500\n",
      "mean reward (50 episodes) 67.340000\n",
      "mean length (50 episodes) 20.060000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 16.000000\n",
      "max_episode_length (50 episodes) 77.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6849\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 211000\n",
      "mean reward (50 episodes) 67.300000\n",
      "mean length (50 episodes) 20.460000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 16.000000\n",
      "max_episode_length (50 episodes) 77.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6875\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 211500\n",
      "mean reward (50 episodes) 68.820000\n",
      "mean length (50 episodes) 18.980000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6899\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 212000\n",
      "mean reward (50 episodes) 68.820000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6924\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 212500\n",
      "mean reward (50 episodes) 68.760000\n",
      "mean length (50 episodes) 19.040000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6949\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 213000\n",
      "mean reward (50 episodes) 68.860000\n",
      "mean length (50 episodes) 18.500000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 6975\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 213500\n",
      "mean reward (50 episodes) 68.820000\n",
      "mean length (50 episodes) 18.860000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 14.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7000\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 214000\n",
      "mean reward (50 episodes) 68.800000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7024\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 214500\n",
      "mean reward (50 episodes) 68.800000\n",
      "mean length (50 episodes) 19.120000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7049\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 215000\n",
      "mean reward (50 episodes) 68.400000\n",
      "mean length (50 episodes) 19.240000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7074\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 215500\n",
      "mean reward (50 episodes) 68.420000\n",
      "mean length (50 episodes) 18.540000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7100\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 216000\n",
      "mean reward (50 episodes) 68.840000\n",
      "mean length (50 episodes) 18.320000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7125\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 216500\n",
      "mean reward (50 episodes) 68.880000\n",
      "mean length (50 episodes) 18.080000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7153\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 217000\n",
      "mean reward (50 episodes) 68.740000\n",
      "mean length (50 episodes) 18.260000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7177\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 217500\n",
      "mean reward (50 episodes) 68.600000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7202\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 218000\n",
      "mean reward (50 episodes) 68.780000\n",
      "mean length (50 episodes) 18.080000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7229\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 218500\n",
      "mean reward (50 episodes) 68.540000\n",
      "mean length (50 episodes) 18.280000\n",
      "max_episode_reward (50 episodes) 71.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7254\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 219000\n",
      "mean reward (50 episodes) 68.160000\n",
      "mean length (50 episodes) 19.520000\n",
      "max_episode_reward (50 episodes) 71.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7278\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 219500\n",
      "mean reward (50 episodes) 68.380000\n",
      "mean length (50 episodes) 19.600000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7303\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 220000\n",
      "mean reward (50 episodes) 68.680000\n",
      "mean length (50 episodes) 19.800000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7326\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 220500\n",
      "mean reward (50 episodes) 68.820000\n",
      "mean length (50 episodes) 20.080000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7350\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 221000\n",
      "mean reward (50 episodes) 68.840000\n",
      "mean length (50 episodes) 19.280000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7375\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 221500\n",
      "mean reward (50 episodes) 68.660000\n",
      "mean length (50 episodes) 19.500000\n",
      "max_episode_reward (50 episodes) 72.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7399\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 222000\n",
      "mean reward (50 episodes) 68.820000\n",
      "mean length (50 episodes) 19.280000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7424\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 222500\n",
      "mean reward (50 episodes) 68.440000\n",
      "mean length (50 episodes) 18.900000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7450\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 223000\n",
      "mean reward (50 episodes) 68.140000\n",
      "mean length (50 episodes) 18.440000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7476\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 223500\n",
      "mean reward (50 episodes) 68.580000\n",
      "mean length (50 episodes) 18.300000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7501\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 224000\n",
      "mean reward (50 episodes) 68.280000\n",
      "mean length (50 episodes) 19.160000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7525\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 224500\n",
      "mean reward (50 episodes) 68.580000\n",
      "mean length (50 episodes) 19.680000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 48.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7550\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 225000\n",
      "mean reward (50 episodes) 69.020000\n",
      "mean length (50 episodes) 19.620000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7574\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 225500\n",
      "mean reward (50 episodes) 68.720000\n",
      "mean length (50 episodes) 19.420000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7599\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 226000\n",
      "mean reward (50 episodes) 68.920000\n",
      "mean length (50 episodes) 19.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7624\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 226500\n",
      "mean reward (50 episodes) 68.960000\n",
      "mean length (50 episodes) 19.080000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7648\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 227000\n",
      "mean reward (50 episodes) 68.920000\n",
      "mean length (50 episodes) 19.440000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7673\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 227500\n",
      "mean reward (50 episodes) 68.980000\n",
      "mean length (50 episodes) 18.660000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7699\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 228000\n",
      "mean reward (50 episodes) 68.880000\n",
      "mean length (50 episodes) 18.240000\n",
      "max_episode_reward (50 episodes) 69.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.060000\n",
      "episodes 7724\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 228500\n",
      "mean reward (50 episodes) 69.620000\n",
      "mean length (50 episodes) 19.080000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.620000\n",
      "episodes 7749\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 229000\n",
      "mean reward (50 episodes) 69.740000\n",
      "mean length (50 episodes) 18.860000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 69.780000\n",
      "episodes 7775\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 229500\n",
      "mean reward (50 episodes) 70.240000\n",
      "mean length (50 episodes) 19.020000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 70.300000\n",
      "episodes 7799\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 230000\n",
      "mean reward (50 episodes) 70.600000\n",
      "mean length (50 episodes) 19.980000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 70.640000\n",
      "episodes 7822\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 230500\n",
      "mean reward (50 episodes) 69.860000\n",
      "mean length (50 episodes) 20.140000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 70.640000\n",
      "episodes 7846\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 231000\n",
      "mean reward (50 episodes) 70.180000\n",
      "mean length (50 episodes) 20.020000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 70.640000\n",
      "episodes 7870\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 231500\n",
      "mean reward (50 episodes) 71.520000\n",
      "mean length (50 episodes) 19.720000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 71.520000\n",
      "episodes 7895\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 232000\n",
      "mean reward (50 episodes) 72.040000\n",
      "mean length (50 episodes) 19.760000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 72.040000\n",
      "episodes 7918\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 232500\n",
      "mean reward (50 episodes) 72.580000\n",
      "mean length (50 episodes) 19.440000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 72.580000\n",
      "episodes 7943\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 233000\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 18.260000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 72.800000\n",
      "episodes 7970\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 233500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 18.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.800000\n",
      "episodes 7994\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 234000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.140000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.800000\n",
      "episodes 8019\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 234500\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 18.120000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 64.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.800000\n",
      "episodes 8046\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 235000\n",
      "mean reward (50 episodes) 72.420000\n",
      "mean length (50 episodes) 17.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 72.800000\n",
      "episodes 8072\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 235500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 17.580000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 72.800000\n",
      "episodes 8099\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 236000\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 17.700000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8126\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 236500\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 18.800000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8150\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 237000\n",
      "mean reward (50 episodes) 72.840000\n",
      "mean length (50 episodes) 19.820000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8174\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 237500\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 20.260000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8197\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 238000\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 20.480000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8220\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 238500\n",
      "mean reward (50 episodes) 72.520000\n",
      "mean length (50 episodes) 20.700000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8243\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 239000\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 20.620000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8266\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 239500\n",
      "mean reward (50 episodes) 72.540000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8292\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 240000\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 18.180000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8319\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 240500\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 18.800000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8343\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 241000\n",
      "mean reward (50 episodes) 72.560000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8367\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 241500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.300000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8389\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 242000\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.280000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8414\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 242500\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8439\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 243000\n",
      "mean reward (50 episodes) 72.680000\n",
      "mean length (50 episodes) 19.700000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8462\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 243500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.460000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8485\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 244000\n",
      "mean reward (50 episodes) 72.520000\n",
      "mean length (50 episodes) 20.580000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8508\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 244500\n",
      "mean reward (50 episodes) 72.480000\n",
      "mean length (50 episodes) 20.140000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8533\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 245000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8558\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 245500\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8581\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 246000\n",
      "mean reward (50 episodes) 72.520000\n",
      "mean length (50 episodes) 19.720000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8606\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 246500\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 19.400000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.900000\n",
      "episodes 8630\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 247000\n",
      "mean reward (50 episodes) 72.920000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8654\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 247500\n",
      "mean reward (50 episodes) 72.880000\n",
      "mean length (50 episodes) 20.440000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8677\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 248000\n",
      "mean reward (50 episodes) 72.860000\n",
      "mean length (50 episodes) 20.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8701\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 248500\n",
      "mean reward (50 episodes) 72.780000\n",
      "mean length (50 episodes) 19.860000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8725\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 249000\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 20.160000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8748\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 249500\n",
      "mean reward (50 episodes) 72.780000\n",
      "mean length (50 episodes) 20.280000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8772\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 250000\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 20.300000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8795\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 250500\n",
      "mean reward (50 episodes) 72.560000\n",
      "mean length (50 episodes) 19.260000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8821\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 251000\n",
      "mean reward (50 episodes) 72.680000\n",
      "mean length (50 episodes) 18.540000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8846\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 251500\n",
      "mean reward (50 episodes) 72.620000\n",
      "mean length (50 episodes) 18.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8872\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 252000\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 18.360000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8898\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 252500\n",
      "mean reward (50 episodes) 72.560000\n",
      "mean length (50 episodes) 18.780000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 16.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8922\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 253000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8946\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 253500\n",
      "mean reward (50 episodes) 72.580000\n",
      "mean length (50 episodes) 20.020000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8970\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 254000\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 20.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.940000\n",
      "episodes 8993\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 254500\n",
      "mean reward (50 episodes) 72.620000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9017\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 255000\n",
      "mean reward (50 episodes) 72.240000\n",
      "mean length (50 episodes) 20.340000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9040\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 255500\n",
      "mean reward (50 episodes) 72.340000\n",
      "mean length (50 episodes) 20.560000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9064\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 256000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.080000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9088\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 256500\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9110\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 257000\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 20.600000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9134\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 257500\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 19.940000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9158\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 258000\n",
      "mean reward (50 episodes) 72.780000\n",
      "mean length (50 episodes) 19.500000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9183\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 258500\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 20.080000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9206\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 259000\n",
      "mean reward (50 episodes) 72.780000\n",
      "mean length (50 episodes) 20.000000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9230\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 259500\n",
      "mean reward (50 episodes) 72.940000\n",
      "mean length (50 episodes) 19.840000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9254\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 260000\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 19.760000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9278\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 260500\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 19.460000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.940000\n",
      "episodes 9303\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 261000\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 19.900000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9326\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 261500\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 20.420000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9349\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 262000\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 20.680000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9372\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 262500\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 19.960000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9397\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 263000\n",
      "mean reward (50 episodes) 72.780000\n",
      "mean length (50 episodes) 19.640000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9421\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 263500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9445\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 264000\n",
      "mean reward (50 episodes) 72.620000\n",
      "mean length (50 episodes) 19.480000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9470\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 264500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.980000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9493\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 265000\n",
      "mean reward (50 episodes) 72.900000\n",
      "mean length (50 episodes) 19.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9518\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 265500\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 19.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9542\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 266000\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9566\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 266500\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.620000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9589\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 267000\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 19.760000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9614\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 267500\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 18.480000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9640\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 268000\n",
      "mean reward (50 episodes) 72.880000\n",
      "mean length (50 episodes) 19.200000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9663\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 268500\n",
      "mean reward (50 episodes) 72.880000\n",
      "mean length (50 episodes) 20.300000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9687\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 269000\n",
      "mean reward (50 episodes) 72.840000\n",
      "mean length (50 episodes) 20.160000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9711\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 269500\n",
      "mean reward (50 episodes) 72.780000\n",
      "mean length (50 episodes) 19.860000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9735\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 270000\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9759\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 270500\n",
      "mean reward (50 episodes) 72.480000\n",
      "mean length (50 episodes) 19.340000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9784\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 271000\n",
      "mean reward (50 episodes) 72.540000\n",
      "mean length (50 episodes) 19.200000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9808\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 271500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.800000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9832\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 272000\n",
      "mean reward (50 episodes) 72.840000\n",
      "mean length (50 episodes) 18.780000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9859\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 272500\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 19.000000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9882\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 273000\n",
      "mean reward (50 episodes) 72.580000\n",
      "mean length (50 episodes) 20.220000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9905\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 273500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.260000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9929\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 274000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.260000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9952\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 274500\n",
      "mean reward (50 episodes) 72.880000\n",
      "mean length (50 episodes) 19.800000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.960000\n",
      "episodes 9977\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 275000\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 18.940000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10002\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 275500\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 18.360000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10029\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 276000\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 18.640000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10053\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 276500\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 18.900000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10079\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 277000\n",
      "mean reward (50 episodes) 72.440000\n",
      "mean length (50 episodes) 19.140000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10103\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 277500\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10127\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 278000\n",
      "mean reward (50 episodes) 72.820000\n",
      "mean length (50 episodes) 19.940000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10151\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 278500\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 20.420000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10174\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 279000\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 20.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10198\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 279500\n",
      "mean reward (50 episodes) 72.680000\n",
      "mean length (50 episodes) 20.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10221\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 280000\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 20.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10244\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 280500\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.320000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10268\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 281000\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 20.160000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10291\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 281500\n",
      "mean reward (50 episodes) 72.440000\n",
      "mean length (50 episodes) 20.160000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10315\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 282000\n",
      "mean reward (50 episodes) 72.340000\n",
      "mean length (50 episodes) 19.640000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10340\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 282500\n",
      "mean reward (50 episodes) 72.220000\n",
      "mean length (50 episodes) 20.080000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10363\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 283000\n",
      "mean reward (50 episodes) 72.300000\n",
      "mean length (50 episodes) 20.840000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10386\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 283500\n",
      "mean reward (50 episodes) 72.500000\n",
      "mean length (50 episodes) 20.600000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10409\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 284000\n",
      "mean reward (50 episodes) 72.820000\n",
      "mean length (50 episodes) 19.560000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10434\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 284500\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 19.620000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10458\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 285000\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 20.540000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 72.960000\n",
      "episodes 10481\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 285500\n",
      "mean reward (50 episodes) 73.000000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10506\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 286000\n",
      "mean reward (50 episodes) 72.940000\n",
      "mean length (50 episodes) 19.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10529\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 286500\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 20.340000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10553\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 287000\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 19.820000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10577\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 287500\n",
      "mean reward (50 episodes) 72.820000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10601\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 288000\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 19.900000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10625\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 288500\n",
      "mean reward (50 episodes) 72.340000\n",
      "mean length (50 episodes) 20.580000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10647\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 289000\n",
      "mean reward (50 episodes) 72.380000\n",
      "mean length (50 episodes) 20.760000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10670\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 289500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 20.420000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10694\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 290000\n",
      "mean reward (50 episodes) 72.520000\n",
      "mean length (50 episodes) 20.560000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10717\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 290500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.240000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10741\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 291000\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 19.560000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10766\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 291500\n",
      "mean reward (50 episodes) 72.820000\n",
      "mean length (50 episodes) 19.820000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10789\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 292000\n",
      "mean reward (50 episodes) 72.620000\n",
      "mean length (50 episodes) 20.480000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10812\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 292500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 20.720000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10835\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 293000\n",
      "mean reward (50 episodes) 72.500000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10861\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 293500\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 19.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10885\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 294000\n",
      "mean reward (50 episodes) 72.540000\n",
      "mean length (50 episodes) 20.040000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10908\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 294500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10932\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 295000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.260000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10955\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 295500\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 20.480000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 10978\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 296000\n",
      "mean reward (50 episodes) 72.620000\n",
      "mean length (50 episodes) 20.540000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11002\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 296500\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 19.680000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11027\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 297000\n",
      "mean reward (50 episodes) 72.560000\n",
      "mean length (50 episodes) 19.540000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11050\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 297500\n",
      "mean reward (50 episodes) 72.480000\n",
      "mean length (50 episodes) 20.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11074\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 298000\n",
      "mean reward (50 episodes) 72.580000\n",
      "mean length (50 episodes) 20.000000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11098\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 298500\n",
      "mean reward (50 episodes) 72.380000\n",
      "mean length (50 episodes) 20.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11121\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 299000\n",
      "mean reward (50 episodes) 72.380000\n",
      "mean length (50 episodes) 20.460000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11145\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 299500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11169\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 300000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.780000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11193\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 300500\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 19.700000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11217\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 301000\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 19.380000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11242\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 301500\n",
      "mean reward (50 episodes) 72.680000\n",
      "mean length (50 episodes) 19.640000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11265\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 302000\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 19.120000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11292\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 302500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.000000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11315\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 303000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.420000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11338\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 303500\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 20.180000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11363\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 304000\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 19.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11386\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 304500\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 20.180000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11410\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 305000\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 19.940000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11434\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 305500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11457\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 306000\n",
      "mean reward (50 episodes) 72.680000\n",
      "mean length (50 episodes) 20.380000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11481\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 306500\n",
      "mean reward (50 episodes) 72.760000\n",
      "mean length (50 episodes) 20.600000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11504\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 307000\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 20.600000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11527\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 307500\n",
      "mean reward (50 episodes) 72.500000\n",
      "mean length (50 episodes) 20.560000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11550\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 308000\n",
      "mean reward (50 episodes) 72.840000\n",
      "mean length (50 episodes) 19.320000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11576\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 308500\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 18.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11600\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 309000\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 19.760000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11624\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 309500\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11647\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 310000\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 20.100000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11671\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 310500\n",
      "mean reward (50 episodes) 72.440000\n",
      "mean length (50 episodes) 20.020000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11695\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 311000\n",
      "mean reward (50 episodes) 72.280000\n",
      "mean length (50 episodes) 20.360000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11718\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 311500\n",
      "mean reward (50 episodes) 72.620000\n",
      "mean length (50 episodes) 20.120000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11742\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 312000\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.200000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11765\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 312500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 20.340000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11789\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 313000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.140000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11813\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 313500\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 20.120000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11836\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 314000\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 20.320000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11860\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 314500\n",
      "mean reward (50 episodes) 72.320000\n",
      "mean length (50 episodes) 20.340000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11883\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 315000\n",
      "mean reward (50 episodes) 72.620000\n",
      "mean length (50 episodes) 20.180000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11907\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 315500\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.260000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11930\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 316000\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.020000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11954\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 316500\n",
      "mean reward (50 episodes) 72.800000\n",
      "mean length (50 episodes) 19.920000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 11978\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 317000\n",
      "mean reward (50 episodes) 72.720000\n",
      "mean length (50 episodes) 20.740000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12000\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 317500\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 21.300000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12022\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 318000\n",
      "mean reward (50 episodes) 72.700000\n",
      "mean length (50 episodes) 20.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12047\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 318500\n",
      "mean reward (50 episodes) 72.380000\n",
      "mean length (50 episodes) 20.060000\n",
      "max_episode_reward (50 episodes) 73.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12070\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 319000\n",
      "mean reward (50 episodes) 72.460000\n",
      "mean length (50 episodes) 20.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12092\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 319500\n",
      "mean reward (50 episodes) 72.820000\n",
      "mean length (50 episodes) 20.840000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12116\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 320000\n",
      "mean reward (50 episodes) 72.860000\n",
      "mean length (50 episodes) 19.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 22.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12141\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 320500\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 19.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12164\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 321000\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 20.340000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12187\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 321500\n",
      "mean reward (50 episodes) 72.840000\n",
      "mean length (50 episodes) 20.500000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12210\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 322000\n",
      "mean reward (50 episodes) 72.780000\n",
      "mean length (50 episodes) 20.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12233\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 322500\n",
      "mean reward (50 episodes) 72.680000\n",
      "mean length (50 episodes) 20.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12256\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 323000\n",
      "mean reward (50 episodes) 72.660000\n",
      "mean length (50 episodes) 20.700000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12279\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 323500\n",
      "mean reward (50 episodes) 72.820000\n",
      "mean length (50 episodes) 20.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12301\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 324000\n",
      "mean reward (50 episodes) 73.000000\n",
      "mean length (50 episodes) 20.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.000000\n",
      "episodes 12325\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 324500\n",
      "mean reward (50 episodes) 73.200000\n",
      "mean length (50 episodes) 21.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.200000\n",
      "episodes 12347\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 325000\n",
      "mean reward (50 episodes) 73.200000\n",
      "mean length (50 episodes) 21.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.260000\n",
      "episodes 12369\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 325500\n",
      "mean reward (50 episodes) 73.080000\n",
      "mean length (50 episodes) 21.060000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.260000\n",
      "episodes 12392\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 326000\n",
      "mean reward (50 episodes) 73.020000\n",
      "mean length (50 episodes) 20.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.260000\n",
      "episodes 12414\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 326500\n",
      "mean reward (50 episodes) 72.940000\n",
      "mean length (50 episodes) 21.560000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.260000\n",
      "episodes 12437\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 327000\n",
      "mean reward (50 episodes) 71.120000\n",
      "mean length (50 episodes) 23.340000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) -4.000000\n",
      "max_episode_length (50 episodes) 99.000000\n",
      "best mean reward 73.260000\n",
      "episodes 12454\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 327500\n",
      "mean reward (50 episodes) 71.540000\n",
      "mean length (50 episodes) 24.500000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) -4.000000\n",
      "max_episode_length (50 episodes) 99.000000\n",
      "best mean reward 73.260000\n",
      "episodes 12474\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 328000\n",
      "mean reward (50 episodes) 71.780000\n",
      "mean length (50 episodes) 25.420000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) -4.000000\n",
      "max_episode_length (50 episodes) 99.000000\n",
      "best mean reward 73.260000\n",
      "episodes 12495\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 328500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.700000\n",
      "episodes 12517\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 329000\n",
      "mean reward (50 episodes) 73.060000\n",
      "mean length (50 episodes) 21.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.700000\n",
      "episodes 12540\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 329500\n",
      "mean reward (50 episodes) 73.200000\n",
      "mean length (50 episodes) 21.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.700000\n",
      "episodes 12560\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 330000\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 23.200000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.700000\n",
      "episodes 12580\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 330500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.780000\n",
      "episodes 12601\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 331000\n",
      "mean reward (50 episodes) 73.800000\n",
      "mean length (50 episodes) 23.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.800000\n",
      "episodes 12621\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 331500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.140000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12642\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 332000\n",
      "mean reward (50 episodes) 73.320000\n",
      "mean length (50 episodes) 22.460000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12664\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 332500\n",
      "mean reward (50 episodes) 73.260000\n",
      "mean length (50 episodes) 22.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12685\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 333000\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 22.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12705\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 333500\n",
      "mean reward (50 episodes) 73.280000\n",
      "mean length (50 episodes) 23.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12724\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 334000\n",
      "mean reward (50 episodes) 73.180000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12745\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 334500\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 22.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12767\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 335000\n",
      "mean reward (50 episodes) 73.860000\n",
      "mean length (50 episodes) 22.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12788\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 335500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12808\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 336000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12828\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 336500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.700000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12848\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 337000\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12870\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 337500\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12890\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 338000\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 22.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12912\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 338500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.060000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12934\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 339000\n",
      "mean reward (50 episodes) 73.840000\n",
      "mean length (50 episodes) 22.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12955\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 339500\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 23.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12975\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 340000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 12997\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 340500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.480000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13017\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 341000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13037\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 341500\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 24.200000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13057\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 342000\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13078\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 342500\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.840000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13098\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 343000\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13118\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 343500\n",
      "mean reward (50 episodes) 73.300000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13139\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 344000\n",
      "mean reward (50 episodes) 73.140000\n",
      "mean length (50 episodes) 22.840000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13160\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 344500\n",
      "mean reward (50 episodes) 72.980000\n",
      "mean length (50 episodes) 23.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13180\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 345000\n",
      "mean reward (50 episodes) 73.280000\n",
      "mean length (50 episodes) 23.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13200\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 345500\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 22.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 63.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13222\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 346000\n",
      "mean reward (50 episodes) 73.400000\n",
      "mean length (50 episodes) 22.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13243\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 346500\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 23.300000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13263\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 347000\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13284\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 347500\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.060000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13305\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 348000\n",
      "mean reward (50 episodes) 73.400000\n",
      "mean length (50 episodes) 23.160000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13325\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 348500\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 23.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13345\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 349000\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 23.700000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13365\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 349500\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 22.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13388\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 350000\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 22.400000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 17.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13409\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 350500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.040000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13429\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 351000\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13449\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 351500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.400000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13470\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 352000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.980000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13491\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 352500\n",
      "mean reward (50 episodes) 73.400000\n",
      "mean length (50 episodes) 23.340000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13511\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 353000\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13531\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 353500\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13551\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 354000\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 23.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13573\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 354500\n",
      "mean reward (50 episodes) 73.260000\n",
      "mean length (50 episodes) 22.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 50.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13595\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 355000\n",
      "mean reward (50 episodes) 73.300000\n",
      "mean length (50 episodes) 22.100000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 50.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13616\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 355500\n",
      "mean reward (50 episodes) 73.300000\n",
      "mean length (50 episodes) 23.080000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 50.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13636\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 356000\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 23.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13656\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 356500\n",
      "mean reward (50 episodes) 73.140000\n",
      "mean length (50 episodes) 23.460000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13677\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 357000\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 21.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13700\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 357500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 21.560000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13721\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 358000\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 22.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13741\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 358500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13762\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 359000\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13782\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 359500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13802\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 360000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13823\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 360500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13843\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 361000\n",
      "mean reward (50 episodes) 73.880000\n",
      "mean length (50 episodes) 22.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13864\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 361500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.200000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13886\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 362000\n",
      "mean reward (50 episodes) 73.120000\n",
      "mean length (50 episodes) 21.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13908\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 362500\n",
      "mean reward (50 episodes) 73.220000\n",
      "mean length (50 episodes) 22.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13928\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 363000\n",
      "mean reward (50 episodes) 73.000000\n",
      "mean length (50 episodes) 23.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13948\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 363500\n",
      "mean reward (50 episodes) 73.160000\n",
      "mean length (50 episodes) 23.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13968\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 364000\n",
      "mean reward (50 episodes) 73.280000\n",
      "mean length (50 episodes) 22.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 13990\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 364500\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 22.500000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14011\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 365000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14032\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 365500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14053\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 366000\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 22.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14074\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 366500\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14095\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 367000\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.400000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14115\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 367500\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.820000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14135\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 368000\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.880000\n",
      "episodes 14156\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 368500\n",
      "mean reward (50 episodes) 73.940000\n",
      "mean length (50 episodes) 23.440000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14176\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 369000\n",
      "mean reward (50 episodes) 73.840000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14196\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 369500\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.540000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14217\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 370000\n",
      "mean reward (50 episodes) 73.320000\n",
      "mean length (50 episodes) 22.540000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14239\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 370500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14260\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 371000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 22.940000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14281\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 371500\n",
      "mean reward (50 episodes) 73.100000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14301\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 372000\n",
      "mean reward (50 episodes) 72.960000\n",
      "mean length (50 episodes) 23.020000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14322\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 372500\n",
      "mean reward (50 episodes) 73.100000\n",
      "mean length (50 episodes) 23.340000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14342\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 373000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.300000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14362\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 373500\n",
      "mean reward (50 episodes) 73.840000\n",
      "mean length (50 episodes) 23.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14383\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 374000\n",
      "mean reward (50 episodes) 73.860000\n",
      "mean length (50 episodes) 23.420000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14403\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 374500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 23.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14423\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 375000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14445\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 375500\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 22.160000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14467\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 376000\n",
      "mean reward (50 episodes) 73.840000\n",
      "mean length (50 episodes) 21.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14488\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 376500\n",
      "mean reward (50 episodes) 73.920000\n",
      "mean length (50 episodes) 22.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14509\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 377000\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14530\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 377500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 22.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14551\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 378000\n",
      "mean reward (50 episodes) 73.920000\n",
      "mean length (50 episodes) 22.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14573\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 378500\n",
      "mean reward (50 episodes) 73.900000\n",
      "mean length (50 episodes) 22.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14594\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 379000\n",
      "mean reward (50 episodes) 73.800000\n",
      "mean length (50 episodes) 22.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14615\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 379500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14636\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 380000\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14656\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 380500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14676\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 381000\n",
      "mean reward (50 episodes) 73.840000\n",
      "mean length (50 episodes) 23.100000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14698\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 381500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14718\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 382000\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.060000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14738\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 382500\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 22.940000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14760\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 383000\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 22.980000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14780\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 383500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.040000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14801\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 384000\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 23.080000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14822\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 384500\n",
      "mean reward (50 episodes) 73.920000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14843\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 385000\n",
      "mean reward (50 episodes) 73.900000\n",
      "mean length (50 episodes) 23.040000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14863\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 385500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14884\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 386000\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.180000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14905\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 386500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.020000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14925\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 387000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14946\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 387500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14966\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 388000\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 14986\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 388500\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15007\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 389000\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.060000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15028\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 389500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 22.540000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15049\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 390000\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.100000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15069\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 390500\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 23.040000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15091\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 391000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.320000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15110\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 391500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.700000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15131\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 392000\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15151\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 392500\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15171\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 393000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.100000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15193\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 393500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15214\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 394000\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 22.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15234\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 394500\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15255\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 395000\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15276\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 395500\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 22.440000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15298\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 396000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15320\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 396500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.120000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15341\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 397000\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 22.980000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15361\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 397500\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15381\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 398000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15401\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 398500\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 23.420000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15422\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 399000\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15442\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 399500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15462\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 400000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.940000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15485\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 400500\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.000000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15504\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 401000\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.100000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15525\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 401500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.980000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15546\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 402000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 22.840000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15567\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 402500\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15588\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 403000\n",
      "mean reward (50 episodes) 73.800000\n",
      "mean length (50 episodes) 22.320000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15610\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 403500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 22.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15630\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 404000\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 23.500000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15650\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 404500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15670\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 405000\n",
      "mean reward (50 episodes) 73.880000\n",
      "mean length (50 episodes) 22.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15692\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 405500\n",
      "mean reward (50 episodes) 73.880000\n",
      "mean length (50 episodes) 22.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15712\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 406000\n",
      "mean reward (50 episodes) 73.840000\n",
      "mean length (50 episodes) 23.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15733\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 406500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.480000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15753\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 407000\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 23.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15774\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 407500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15794\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 408000\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.820000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15814\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 408500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 24.160000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15834\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 409000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.940000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15854\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 409500\n",
      "mean reward (50 episodes) 73.300000\n",
      "mean length (50 episodes) 23.420000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15875\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 410000\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.040000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15896\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 410500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15916\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 411000\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15937\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 411500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.540000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15959\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 412000\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 23.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15978\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 412500\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 23.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 15999\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 413000\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16019\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 413500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.160000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16040\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 414000\n",
      "mean reward (50 episodes) 73.380000\n",
      "mean length (50 episodes) 22.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16061\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 414500\n",
      "mean reward (50 episodes) 73.400000\n",
      "mean length (50 episodes) 23.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16081\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 415000\n",
      "mean reward (50 episodes) 73.280000\n",
      "mean length (50 episodes) 23.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16101\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 415500\n",
      "mean reward (50 episodes) 73.060000\n",
      "mean length (50 episodes) 24.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16121\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 416000\n",
      "mean reward (50 episodes) 73.420000\n",
      "mean length (50 episodes) 24.140000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16140\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 416500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 24.040000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16161\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 417000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16181\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 417500\n",
      "mean reward (50 episodes) 73.200000\n",
      "mean length (50 episodes) 24.000000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16201\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 418000\n",
      "mean reward (50 episodes) 73.080000\n",
      "mean length (50 episodes) 23.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16221\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 418500\n",
      "mean reward (50 episodes) 73.220000\n",
      "mean length (50 episodes) 23.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16241\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 419000\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16262\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 419500\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 23.200000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16283\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 420000\n",
      "mean reward (50 episodes) 73.420000\n",
      "mean length (50 episodes) 23.400000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16303\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 420500\n",
      "mean reward (50 episodes) 73.360000\n",
      "mean length (50 episodes) 23.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16323\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 421000\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16345\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 421500\n",
      "mean reward (50 episodes) 73.880000\n",
      "mean length (50 episodes) 21.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16367\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 422000\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.440000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16388\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 422500\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 22.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16409\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 423000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 22.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16430\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 423500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16450\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 424000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.840000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16471\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 424500\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 22.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16492\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 425000\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16513\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 425500\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16533\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 426000\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 23.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16553\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 426500\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16573\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 427000\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16593\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 427500\n",
      "mean reward (50 episodes) 73.380000\n",
      "mean length (50 episodes) 23.300000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16614\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 428000\n",
      "mean reward (50 episodes) 73.360000\n",
      "mean length (50 episodes) 22.320000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16636\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 428500\n",
      "mean reward (50 episodes) 73.300000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16656\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 429000\n",
      "mean reward (50 episodes) 73.360000\n",
      "mean length (50 episodes) 23.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16676\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 429500\n",
      "mean reward (50 episodes) 73.420000\n",
      "mean length (50 episodes) 23.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16697\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 430000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.340000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16718\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 430500\n",
      "mean reward (50 episodes) 73.300000\n",
      "mean length (50 episodes) 23.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16738\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 431000\n",
      "mean reward (50 episodes) 73.380000\n",
      "mean length (50 episodes) 23.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16758\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 431500\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.940000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16778\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 432000\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16799\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 432500\n",
      "mean reward (50 episodes) 73.800000\n",
      "mean length (50 episodes) 23.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16819\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 433000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.400000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16839\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 433500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16860\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 434000\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16881\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 434500\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 22.100000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16903\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 435000\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 22.300000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16924\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 435500\n",
      "mean reward (50 episodes) 73.880000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16944\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 436000\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 64.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16964\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 436500\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 64.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 16984\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 437000\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 64.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17004\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 437500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.940000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17025\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 438000\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17045\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 438500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17067\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 439000\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 21.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17089\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 439500\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 22.120000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17110\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 440000\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.740000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17131\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 440500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17152\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 441000\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 22.960000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17172\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 441500\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 23.740000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17192\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 442000\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.080000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17214\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 442500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17234\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 443000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17254\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 443500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 22.460000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17276\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 444000\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 22.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17297\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 444500\n",
      "mean reward (50 episodes) 73.800000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17317\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 445000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17337\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 445500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17357\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 446000\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17378\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 446500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 22.980000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17399\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 447000\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.080000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17420\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 447500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17440\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 448000\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 23.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17460\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 448500\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17482\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 449000\n",
      "mean reward (50 episodes) 73.580000\n",
      "mean length (50 episodes) 22.740000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17503\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 449500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 22.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17523\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 450000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17543\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 450500\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.180000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17565\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 451000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.780000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17585\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 451500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17605\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 452000\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.020000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17627\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 452500\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 22.180000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17649\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 453000\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 22.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17669\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 453500\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17689\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 454000\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.820000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17709\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 454500\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 22.840000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17731\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 455000\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 22.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17752\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 455500\n",
      "mean reward (50 episodes) 73.380000\n",
      "mean length (50 episodes) 22.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17773\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 456000\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 23.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17793\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 456500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 22.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17815\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 457000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 22.820000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17835\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 457500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.620000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17855\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 458000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17875\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 458500\n",
      "mean reward (50 episodes) 73.240000\n",
      "mean length (50 episodes) 23.140000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17897\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 459000\n",
      "mean reward (50 episodes) 73.020000\n",
      "mean length (50 episodes) 22.200000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 53.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17919\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 459500\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 22.140000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17940\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 460000\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 23.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17960\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 460500\n",
      "mean reward (50 episodes) 73.400000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 17980\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 461000\n",
      "mean reward (50 episodes) 73.280000\n",
      "mean length (50 episodes) 22.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18002\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 461500\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 21.740000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18024\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 462000\n",
      "mean reward (50 episodes) 73.720000\n",
      "mean length (50 episodes) 22.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18044\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 462500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 23.340000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18065\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 463000\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 23.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18085\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 463500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18106\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 464000\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.460000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18126\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 464500\n",
      "mean reward (50 episodes) 73.380000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18146\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 465000\n",
      "mean reward (50 episodes) 72.980000\n",
      "mean length (50 episodes) 24.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18165\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 465500\n",
      "mean reward (50 episodes) 73.120000\n",
      "mean length (50 episodes) 24.000000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18186\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 466000\n",
      "mean reward (50 episodes) 73.140000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18205\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 466500\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.480000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 64.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18226\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 467000\n",
      "mean reward (50 episodes) 73.080000\n",
      "mean length (50 episodes) 22.700000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18248\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 467500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 22.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18269\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 468000\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 23.180000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18289\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 468500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.820000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18309\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 469000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.700000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18329\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 469500\n",
      "mean reward (50 episodes) 73.400000\n",
      "mean length (50 episodes) 23.560000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18350\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 470000\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 22.500000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18372\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 470500\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18392\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 471000\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 23.180000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18413\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 471500\n",
      "mean reward (50 episodes) 73.440000\n",
      "mean length (50 episodes) 23.740000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18433\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 472000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 22.920000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18455\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 472500\n",
      "mean reward (50 episodes) 73.080000\n",
      "mean length (50 episodes) 23.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18474\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 473000\n",
      "mean reward (50 episodes) 73.140000\n",
      "mean length (50 episodes) 23.380000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18496\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 473500\n",
      "mean reward (50 episodes) 73.300000\n",
      "mean length (50 episodes) 23.360000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 54.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18516\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 474000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.000000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18537\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 474500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.080000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18557\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 475000\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18578\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 475500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 72.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18599\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 476000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 22.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18621\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 476500\n",
      "mean reward (50 episodes) 73.220000\n",
      "mean length (50 episodes) 22.720000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18641\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 477000\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.440000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18661\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 477500\n",
      "mean reward (50 episodes) 73.420000\n",
      "mean length (50 episodes) 24.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18680\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 478000\n",
      "mean reward (50 episodes) 73.360000\n",
      "mean length (50 episodes) 23.400000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18702\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 478500\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18723\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 479000\n",
      "mean reward (50 episodes) 73.560000\n",
      "mean length (50 episodes) 23.320000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18743\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 479500\n",
      "mean reward (50 episodes) 73.840000\n",
      "mean length (50 episodes) 23.700000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18763\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 480000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18785\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 480500\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 22.740000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18805\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 481000\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.260000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18826\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 481500\n",
      "mean reward (50 episodes) 73.780000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18846\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 482000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 23.060000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18868\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 482500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 22.280000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18890\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 483000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.680000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18910\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 483500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18930\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 484000\n",
      "mean reward (50 episodes) 73.360000\n",
      "mean length (50 episodes) 23.980000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18950\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 484500\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18970\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 485000\n",
      "mean reward (50 episodes) 72.960000\n",
      "mean length (50 episodes) 22.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 18992\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 485500\n",
      "mean reward (50 episodes) 73.180000\n",
      "mean length (50 episodes) 22.560000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 18.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19013\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 486000\n",
      "mean reward (50 episodes) 73.180000\n",
      "mean length (50 episodes) 23.100000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 46.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19033\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 486500\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.080000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 27.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19055\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 487000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.660000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19075\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 487500\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.140000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19096\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 488000\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 23.240000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19117\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 488500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19136\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 489000\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19157\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 489500\n",
      "mean reward (50 episodes) 73.480000\n",
      "mean length (50 episodes) 23.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19177\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 490000\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 22.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19199\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 490500\n",
      "mean reward (50 episodes) 73.760000\n",
      "mean length (50 episodes) 22.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19219\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 491000\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 52.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19239\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 491500\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.540000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 52.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19260\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 492000\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 22.580000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19282\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 492500\n",
      "mean reward (50 episodes) 73.740000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19302\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 493000\n",
      "mean reward (50 episodes) 73.640000\n",
      "mean length (50 episodes) 23.640000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19322\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 493500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 23.220000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19343\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 494000\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19365\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 494500\n",
      "mean reward (50 episodes) 73.700000\n",
      "mean length (50 episodes) 22.880000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19385\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 495000\n",
      "mean reward (50 episodes) 73.520000\n",
      "mean length (50 episodes) 23.800000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 31.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19405\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 495500\n",
      "mean reward (50 episodes) 73.460000\n",
      "mean length (50 episodes) 23.980000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19425\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 496000\n",
      "mean reward (50 episodes) 73.280000\n",
      "mean length (50 episodes) 23.600000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19446\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 496500\n",
      "mean reward (50 episodes) 73.340000\n",
      "mean length (50 episodes) 23.560000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19466\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 497000\n",
      "mean reward (50 episodes) 73.620000\n",
      "mean length (50 episodes) 23.340000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19486\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 497500\n",
      "mean reward (50 episodes) 73.660000\n",
      "mean length (50 episodes) 23.420000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19507\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 498000\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 23.480000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 21.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19527\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 498500\n",
      "mean reward (50 episodes) 73.280000\n",
      "mean length (50 episodes) 23.900000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 22.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19547\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 499000\n",
      "mean reward (50 episodes) 73.220000\n",
      "mean length (50 episodes) 23.440000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 20.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19568\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 499500\n",
      "mean reward (50 episodes) 73.500000\n",
      "mean length (50 episodes) 22.760000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 19.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 73.940000\n",
      "episodes 19590\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Model saved in path: dqn_over_options_6_4/dqn_graph.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a23c09cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4b23e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # Get Atari games.\n",
    "    # benchmark = gym.benchmark_spec('Atari40M')\n",
    "    #\n",
    "    # # Change the index to select a different game.\n",
    "    # task = benchmark.tasks[3]\n",
    "    #\n",
    "    # # Run training\n",
    "    seed = 0  # Use a seed of zero (you may want to randomize the seed!)\n",
    "    set_global_seeds(seed)\n",
    "    # env = get_env(task, seed)\n",
    "    env = ArmEnvDQN(episode_max_length=200,\n",
    "                 size_x=6,\n",
    "                 size_y=4,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=100,\n",
    "                 tower_target_size=4)\n",
    "    session = get_session()\n",
    "    \n",
    "    def stop_cond1(env):\n",
    "        if env._arm_x+1 < env._size_x:\n",
    "            if env._grid[env._arm_x+1, env._arm_y] == 1 and env._arm_x+2 >= env._size_x:\n",
    "                return True\n",
    "            if env._grid[env._arm_x+1, env._arm_y] == 1 and env._arm_x+2 < env._size_x:\n",
    "                if env._grid[env._arm_x+2, env._arm_y] == 1:\n",
    "                    return True\n",
    "        else:\n",
    "            return True\n",
    "        return False\n",
    "    def stop_cond2(env):\n",
    "        if env._arm_x == 0 and env._grid[1, env._arm_y] == 1 and env._grid[2, env._arm_y] == 0:\n",
    "            return True\n",
    "        return False\n",
    "            \n",
    "    \n",
    "    # initialize options\n",
    "    options = [option(env, stop_cond1, path = \"option1_6_4_4/dqn_graph.ckpt\", import_scope = \"option1_6_4_4\"),\n",
    "              option(env, stop_cond2, path = \"option2/dqn_graph.ckpt\", import_scope = \"option2\")]\n",
    "    \n",
    "    ep_rew, ep_len = arm_learn(env, options, session, num_timesteps=500000)\n",
    "    \n",
    "    thefile = open('ep_rew_6_4_4.txt', 'w')\n",
    "    for item in ep_rew:\n",
    "        thefile.write(\"%s\\n\" % item)\n",
    "        \n",
    "    thefile2 = open('ep_len_6_4_4.txt', 'w')\n",
    "    for item in ep_len:\n",
    "        thefile2.write(\"%s\\n\" % item)\n",
    "        \n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=ep_len,\n",
    "        episode_rewards=ep_rew)\n",
    "    plotting.plot_episode_stats(stats)\n",
    "#     tf.summary.FileWriter(\"logs\", tf.get_default_graph()).close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir=logs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_observation(frame):\n",
    "    img_h, img_w = frame.shape[1], frame.shape[2]\n",
    "    return frame.transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n",
    "\n",
    "def main():\n",
    "    env = ArmEnvDQN(episode_max_length=200,\n",
    "                 size_x=5,\n",
    "                 size_y=5,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=400,\n",
    "                 tower_target_size=4)\n",
    "    # print(env.reset())\n",
    "    session = tf.Session()\n",
    "    # First let's load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph('my_test_model.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    frame_history_len = 1\n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_shape = (img_h, img_w, frame_history_len * img_c)  # size_x, size_y,\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "#     # placeholder for current observation (or state)\n",
    "#     obs_t_ph = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "#     # casting to float on GPU ensures lower data transfer times.\n",
    "#     obs_t_float = tf.cast(obs_t_ph, tf.float32) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "#     pred_q = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "#     pred_ac = tf.argmax(pred_q, axis=1)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    obs_t_float = graph.get_tensor_by_name(\"obs_t_ph:0\")\n",
    " \n",
    "    ## How to access saved operation\n",
    "    pred_ac = graph.get_tensor_by_name(\"pred_ac:0\")\n",
    "    \n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    last_obs = env.reset()\n",
    "\n",
    "    for t in itertools.count():\n",
    "\n",
    "        obs = encode_observation(np.array([last_obs]))\n",
    "        action = session.run(pred_ac, {obs_t_float: [obs]})[0]\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done or episode_length == 500:\n",
    "            break\n",
    "\n",
    "        last_obs = next_obs\n",
    "    print(episode_reward, episode_length)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
