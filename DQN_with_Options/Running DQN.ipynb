{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "#from utils import plotting\n",
    "\n",
    "import dqn\n",
    "from dqn_utils import *\n",
    "#from atari_wrappers import *\n",
    "#from environments.arm_env.arm_env import ArmEnv\n",
    "from arm_env_dqn import ArmEnvDQN\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arm_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=256,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def arm_learn(env, session, num_timesteps):\n",
    "    # This is just a rough estimate\n",
    "    num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "    lr_multiplier = 1.0\n",
    "    lr_schedule = PiecewiseSchedule([\n",
    "                                         (0,                   1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                    ],\n",
    "                                    outside_value=5e-5 * lr_multiplier)\n",
    "    optimizer = dqn.OptimizerSpec(\n",
    "        constructor=tf.train.AdamOptimizer,\n",
    "        kwargs=dict(epsilon=1e-4),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "\n",
    "    def stopping_criterion(env, t):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return t >= num_timesteps\n",
    "\n",
    "    exploration_schedule = PiecewiseSchedule(\n",
    "        [\n",
    "            (0, 1.0),\n",
    "            (80000, 0.3),\n",
    "            (200000, 0.01),\n",
    "        ], outside_value=0.01\n",
    "    )\n",
    "\n",
    "    dqn.learn(\n",
    "        env,\n",
    "        q_func=arm_model,\n",
    "        optimizer_spec=optimizer,\n",
    "        session=session,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=1000000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        learning_starts=5000,\n",
    "        learning_freq=1,\n",
    "        frame_history_len=1,\n",
    "        target_update_freq=200,\n",
    "        grad_norm_clipping=10\n",
    "    )\n",
    "    \n",
    "    ep_rew = env.get_episode_rewards()\n",
    "    ep_len = env.get_episode_lengths()\n",
    "    env.close()\n",
    "    return ep_rew, ep_len\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def set_global_seeds(i):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        pass\n",
    "    else:\n",
    "        tf.set_random_seed(i) \n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "def get_session():\n",
    "    tf.reset_default_graph()\n",
    "#     tf_config = tf.ConfigProto(\n",
    "#         inter_op_parallelism_threads=1,\n",
    "#         intra_op_parallelism_threads=1)\n",
    "    session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    print(\"AVAILABLE GPUS: \", get_available_gpus())\n",
    "    session = tf.Session()\n",
    "    return session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE GPUS:  []\n",
      "Timestep 5500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 27\n",
      "exploration 0.951875\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 6000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 30\n",
      "exploration 0.947500\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 6500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 32\n",
      "exploration 0.943125\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 7000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 35\n",
      "exploration 0.938750\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 7500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 37\n",
      "exploration 0.934375\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 8000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 40\n",
      "exploration 0.930000\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 8500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 42\n",
      "exploration 0.925625\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 9000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 45\n",
      "exploration 0.921250\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 9500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 47\n",
      "exploration 0.916875\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 10000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 50\n",
      "exploration 0.912500\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 10500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 52\n",
      "exploration 0.908125\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 11000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 55\n",
      "exploration 0.903750\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 11500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 57\n",
      "exploration 0.899375\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 12000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 60\n",
      "exploration 0.895000\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 12500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 62\n",
      "exploration 0.890625\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 13000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 65\n",
      "exploration 0.886250\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 13500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 67\n",
      "exploration 0.881875\n",
      "learning_rate 0.000099\n",
      "\n",
      "\n",
      "Timestep 14000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 70\n",
      "exploration 0.877500\n",
      "learning_rate 0.000099\n",
      "\n",
      "\n",
      "Timestep 14500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 72\n",
      "exploration 0.873125\n",
      "learning_rate 0.000098\n",
      "\n",
      "\n",
      "Timestep 15000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 75\n",
      "exploration 0.868750\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 15500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 77\n",
      "exploration 0.864375\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 16000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 80\n",
      "exploration 0.860000\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 16500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 82\n",
      "exploration 0.855625\n",
      "learning_rate 0.000096\n",
      "\n",
      "\n",
      "Timestep 17000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 85\n",
      "exploration 0.851250\n",
      "learning_rate 0.000096\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 17500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 87\n",
      "exploration 0.846875\n",
      "learning_rate 0.000095\n",
      "\n",
      "\n",
      "Timestep 18000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 90\n",
      "exploration 0.842500\n",
      "learning_rate 0.000095\n",
      "\n",
      "\n",
      "Timestep 18500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 92\n",
      "exploration 0.838125\n",
      "learning_rate 0.000094\n",
      "\n",
      "\n",
      "Timestep 19000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 95\n",
      "exploration 0.833750\n",
      "learning_rate 0.000094\n",
      "\n",
      "\n",
      "Timestep 19500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 97\n",
      "exploration 0.829375\n",
      "learning_rate 0.000093\n",
      "\n",
      "\n",
      "Timestep 20000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 100\n",
      "exploration 0.825000\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 20500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 102\n",
      "exploration 0.820625\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 21000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 105\n",
      "exploration 0.816250\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 21500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 107\n",
      "exploration 0.811875\n",
      "learning_rate 0.000091\n",
      "\n",
      "\n",
      "Timestep 22000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 110\n",
      "exploration 0.807500\n",
      "learning_rate 0.000091\n",
      "\n",
      "\n",
      "Timestep 22500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 112\n",
      "exploration 0.803125\n",
      "learning_rate 0.000090\n",
      "\n",
      "\n",
      "Timestep 23000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 115\n",
      "exploration 0.798750\n",
      "learning_rate 0.000090\n",
      "\n",
      "\n",
      "Timestep 23500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 117\n",
      "exploration 0.794375\n",
      "learning_rate 0.000089\n",
      "\n",
      "\n",
      "Timestep 24000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 120\n",
      "exploration 0.790000\n",
      "learning_rate 0.000089\n",
      "\n",
      "\n",
      "Timestep 24500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 122\n",
      "exploration 0.785625\n",
      "learning_rate 0.000088\n",
      "\n",
      "\n",
      "Timestep 25000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 125\n",
      "exploration 0.781250\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 25500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 127\n",
      "exploration 0.776875\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 26000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 130\n",
      "exploration 0.772500\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 26500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 132\n",
      "exploration 0.768125\n",
      "learning_rate 0.000086\n",
      "\n",
      "\n",
      "Timestep 27000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 135\n",
      "exploration 0.763750\n",
      "learning_rate 0.000086\n",
      "\n",
      "\n",
      "Timestep 27500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 137\n",
      "exploration 0.759375\n",
      "learning_rate 0.000085\n",
      "\n",
      "\n",
      "Timestep 28000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 140\n",
      "exploration 0.755000\n",
      "learning_rate 0.000085\n",
      "\n",
      "\n",
      "Timestep 28500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 142\n",
      "exploration 0.750625\n",
      "learning_rate 0.000084\n",
      "\n",
      "\n",
      "Timestep 29000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 145\n",
      "exploration 0.746250\n",
      "learning_rate 0.000083\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 29500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 147\n",
      "exploration 0.741875\n",
      "learning_rate 0.000083\n",
      "\n",
      "\n",
      "Timestep 30000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 150\n",
      "exploration 0.737500\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 30500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 152\n",
      "exploration 0.733125\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 31000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 155\n",
      "exploration 0.728750\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 31500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 157\n",
      "exploration 0.724375\n",
      "learning_rate 0.000081\n",
      "\n",
      "\n",
      "Timestep 32000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 160\n",
      "exploration 0.720000\n",
      "learning_rate 0.000081\n",
      "\n",
      "\n",
      "Timestep 32500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 162\n",
      "exploration 0.715625\n",
      "learning_rate 0.000080\n",
      "\n",
      "\n",
      "Timestep 33000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 165\n",
      "exploration 0.711250\n",
      "learning_rate 0.000080\n",
      "\n",
      "\n",
      "Timestep 33500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 167\n",
      "exploration 0.706875\n",
      "learning_rate 0.000079\n",
      "\n",
      "\n",
      "Timestep 34000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 170\n",
      "exploration 0.702500\n",
      "learning_rate 0.000079\n",
      "\n",
      "\n",
      "Timestep 34500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 172\n",
      "exploration 0.698125\n",
      "learning_rate 0.000078\n",
      "\n",
      "\n",
      "Timestep 35000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 175\n",
      "exploration 0.693750\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 35500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 177\n",
      "exploration 0.689375\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 36000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 180\n",
      "exploration 0.685000\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 36500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 182\n",
      "exploration 0.680625\n",
      "learning_rate 0.000076\n",
      "\n",
      "\n",
      "Timestep 37000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 185\n",
      "exploration 0.676250\n",
      "learning_rate 0.000076\n",
      "\n",
      "\n",
      "Timestep 37500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 187\n",
      "exploration 0.671875\n",
      "learning_rate 0.000075\n",
      "\n",
      "\n",
      "Timestep 38000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 190\n",
      "exploration 0.667500\n",
      "learning_rate 0.000074\n",
      "\n",
      "\n",
      "Timestep 38500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 192\n",
      "exploration 0.663125\n",
      "learning_rate 0.000074\n",
      "\n",
      "\n",
      "Timestep 39000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 195\n",
      "exploration 0.658750\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 39500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 197\n",
      "exploration 0.654375\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 40000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 200\n",
      "exploration 0.650000\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 40500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 202\n",
      "exploration 0.645625\n",
      "learning_rate 0.000072\n",
      "\n",
      "\n",
      "Timestep 41000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 205\n",
      "exploration 0.641250\n",
      "learning_rate 0.000072\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 41500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 207\n",
      "exploration 0.636875\n",
      "learning_rate 0.000071\n",
      "\n",
      "\n",
      "Timestep 42000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 210\n",
      "exploration 0.632500\n",
      "learning_rate 0.000071\n",
      "\n",
      "\n",
      "Timestep 42500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 212\n",
      "exploration 0.628125\n",
      "learning_rate 0.000070\n",
      "\n",
      "\n",
      "Timestep 43000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 215\n",
      "exploration 0.623750\n",
      "learning_rate 0.000070\n",
      "\n",
      "\n",
      "Timestep 43500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 217\n",
      "exploration 0.619375\n",
      "learning_rate 0.000069\n",
      "\n",
      "\n",
      "Timestep 44000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 220\n",
      "exploration 0.615000\n",
      "learning_rate 0.000069\n",
      "\n",
      "\n",
      "Timestep 44500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 222\n",
      "exploration 0.610625\n",
      "learning_rate 0.000068\n",
      "\n",
      "\n",
      "Timestep 45000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 225\n",
      "exploration 0.606250\n",
      "learning_rate 0.000068\n",
      "\n",
      "\n",
      "Timestep 45500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 227\n",
      "exploration 0.601875\n",
      "learning_rate 0.000067\n",
      "\n",
      "\n",
      "Timestep 46000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 230\n",
      "exploration 0.597500\n",
      "learning_rate 0.000067\n",
      "\n",
      "\n",
      "Timestep 46500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 232\n",
      "exploration 0.593125\n",
      "learning_rate 0.000066\n",
      "\n",
      "\n",
      "Timestep 47000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 235\n",
      "exploration 0.588750\n",
      "learning_rate 0.000066\n",
      "\n",
      "\n",
      "Timestep 47500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 237\n",
      "exploration 0.584375\n",
      "learning_rate 0.000065\n",
      "\n",
      "\n",
      "Timestep 48000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 240\n",
      "exploration 0.580000\n",
      "learning_rate 0.000064\n",
      "\n",
      "\n",
      "Timestep 48500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 242\n",
      "exploration 0.575625\n",
      "learning_rate 0.000064\n",
      "\n",
      "\n",
      "Timestep 49000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 245\n",
      "exploration 0.571250\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 49500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 247\n",
      "exploration 0.566875\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 50000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 250\n",
      "exploration 0.562500\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 50500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 252\n",
      "exploration 0.558125\n",
      "learning_rate 0.000062\n",
      "\n",
      "\n",
      "Timestep 51000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 255\n",
      "exploration 0.553750\n",
      "learning_rate 0.000062\n",
      "\n",
      "\n",
      "Timestep 51500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 257\n",
      "exploration 0.549375\n",
      "learning_rate 0.000061\n",
      "\n",
      "\n",
      "Timestep 52000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 260\n",
      "exploration 0.545000\n",
      "learning_rate 0.000061\n",
      "\n",
      "\n",
      "Timestep 52500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 262\n",
      "exploration 0.540625\n",
      "learning_rate 0.000060\n",
      "\n",
      "\n",
      "Timestep 53000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 265\n",
      "exploration 0.536250\n",
      "learning_rate 0.000060\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 53500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 267\n",
      "exploration 0.531875\n",
      "learning_rate 0.000059\n",
      "\n",
      "\n",
      "Timestep 54000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 270\n",
      "exploration 0.527500\n",
      "learning_rate 0.000059\n",
      "\n",
      "\n",
      "Timestep 54500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 272\n",
      "exploration 0.523125\n",
      "learning_rate 0.000058\n",
      "\n",
      "\n",
      "Timestep 55000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 275\n",
      "exploration 0.518750\n",
      "learning_rate 0.000058\n",
      "\n",
      "\n",
      "Timestep 55500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 277\n",
      "exploration 0.514375\n",
      "learning_rate 0.000057\n",
      "\n",
      "\n",
      "Timestep 56000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 280\n",
      "exploration 0.510000\n",
      "learning_rate 0.000057\n",
      "\n",
      "\n",
      "Timestep 56500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 282\n",
      "exploration 0.505625\n",
      "learning_rate 0.000056\n",
      "\n",
      "\n",
      "Timestep 57000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 285\n",
      "exploration 0.501250\n",
      "learning_rate 0.000056\n",
      "\n",
      "\n",
      "Timestep 57500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 287\n",
      "exploration 0.496875\n",
      "learning_rate 0.000055\n",
      "\n",
      "\n",
      "Timestep 58000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 290\n",
      "exploration 0.492500\n",
      "learning_rate 0.000055\n",
      "\n",
      "\n",
      "Timestep 58500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 292\n",
      "exploration 0.488125\n",
      "learning_rate 0.000054\n",
      "\n",
      "\n",
      "Timestep 59000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 295\n",
      "exploration 0.483750\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 59500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 297\n",
      "exploration 0.479375\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 60000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 300\n",
      "exploration 0.475000\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 60500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 302\n",
      "exploration 0.470625\n",
      "learning_rate 0.000052\n",
      "\n",
      "\n",
      "Timestep 61000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 305\n",
      "exploration 0.466250\n",
      "learning_rate 0.000052\n",
      "\n",
      "\n",
      "Timestep 61500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 307\n",
      "exploration 0.461875\n",
      "learning_rate 0.000051\n",
      "\n",
      "\n",
      "Timestep 62000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 310\n",
      "exploration 0.457500\n",
      "learning_rate 0.000051\n",
      "\n",
      "\n",
      "Timestep 62500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 312\n",
      "exploration 0.453125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 315\n",
      "exploration 0.448750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -200.000000\n",
      "episodes 317\n",
      "exploration 0.444375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 320\n",
      "exploration 0.440000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 322\n",
      "exploration 0.435625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 65000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 325\n",
      "exploration 0.431250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 65500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 327\n",
      "exploration 0.426875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 330\n",
      "exploration 0.422500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 332\n",
      "exploration 0.418125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 335\n",
      "exploration 0.413750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 337\n",
      "exploration 0.409375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 340\n",
      "exploration 0.405000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 342\n",
      "exploration 0.400625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 345\n",
      "exploration 0.396250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 347\n",
      "exploration 0.391875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 350\n",
      "exploration 0.387500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 352\n",
      "exploration 0.383125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 355\n",
      "exploration 0.378750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 357\n",
      "exploration 0.374375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 360\n",
      "exploration 0.370000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 362\n",
      "exploration 0.365625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73000\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 365\n",
      "exploration 0.361250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73500\n",
      "mean reward (50 episodes) -197.740000\n",
      "mean length (50 episodes) 199.740000\n",
      "max_episode_reward (50 episodes) -87.000000\n",
      "min_episode_length (50 episodes) 187.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 367\n",
      "exploration 0.356875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 370\n",
      "exploration 0.352500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 372\n",
      "exploration 0.348125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 375\n",
      "exploration 0.343750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 377\n",
      "exploration 0.339375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 380\n",
      "exploration 0.335000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 382\n",
      "exploration 0.330625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 77000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 385\n",
      "exploration 0.326250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 77500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 387\n",
      "exploration 0.321875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78000\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 390\n",
      "exploration 0.317500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78500\n",
      "mean reward (50 episodes) -200.000000\n",
      "mean length (50 episodes) 200.000000\n",
      "max_episode_reward (50 episodes) -200.000000\n",
      "min_episode_length (50 episodes) 200.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -197.740000\n",
      "episodes 392\n",
      "exploration 0.313125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79000\n",
      "mean reward (50 episodes) -196.820000\n",
      "mean length (50 episodes) 198.820000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -196.820000\n",
      "episodes 395\n",
      "exploration 0.308750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79500\n",
      "mean reward (50 episodes) -196.820000\n",
      "mean length (50 episodes) 198.820000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -196.820000\n",
      "episodes 397\n",
      "exploration 0.304375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80000\n",
      "mean reward (50 episodes) -196.820000\n",
      "mean length (50 episodes) 198.820000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -196.820000\n",
      "episodes 400\n",
      "exploration 0.300000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80500\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 403\n",
      "exploration 0.298792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81000\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 405\n",
      "exploration 0.297583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81500\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 408\n",
      "exploration 0.296375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82000\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 410\n",
      "exploration 0.295167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82500\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 413\n",
      "exploration 0.293958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83000\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 415\n",
      "exploration 0.292750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83500\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 418\n",
      "exploration 0.291542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84000\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 420\n",
      "exploration 0.290333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84500\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 423\n",
      "exploration 0.289125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85000\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 425\n",
      "exploration 0.287917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85500\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 428\n",
      "exploration 0.286708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86000\n",
      "mean reward (50 episodes) -194.240000\n",
      "mean length (50 episodes) 198.240000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -194.240000\n",
      "episodes 430\n",
      "exploration 0.285500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86500\n",
      "mean reward (50 episodes) -191.960000\n",
      "mean length (50 episodes) 197.960000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -191.960000\n",
      "episodes 433\n",
      "exploration 0.284292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87000\n",
      "mean reward (50 episodes) -189.680000\n",
      "mean length (50 episodes) 197.680000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 435\n",
      "exploration 0.283083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87500\n",
      "mean reward (50 episodes) -189.680000\n",
      "mean length (50 episodes) 197.680000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 438\n",
      "exploration 0.281875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88000\n",
      "mean reward (50 episodes) -189.680000\n",
      "mean length (50 episodes) 197.680000\n",
      "max_episode_reward (50 episodes) -41.000000\n",
      "min_episode_length (50 episodes) 141.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 440\n",
      "exploration 0.280667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88500\n",
      "mean reward (50 episodes) -192.860000\n",
      "mean length (50 episodes) 198.860000\n",
      "max_episode_reward (50 episodes) -71.000000\n",
      "min_episode_length (50 episodes) 171.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 443\n",
      "exploration 0.279458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 89000\n",
      "mean reward (50 episodes) -192.860000\n",
      "mean length (50 episodes) 198.860000\n",
      "max_episode_reward (50 episodes) -71.000000\n",
      "min_episode_length (50 episodes) 171.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 445\n",
      "exploration 0.278250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 89500\n",
      "mean reward (50 episodes) -192.860000\n",
      "mean length (50 episodes) 198.860000\n",
      "max_episode_reward (50 episodes) -71.000000\n",
      "min_episode_length (50 episodes) 171.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 448\n",
      "exploration 0.277042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90000\n",
      "mean reward (50 episodes) -192.860000\n",
      "mean length (50 episodes) 198.860000\n",
      "max_episode_reward (50 episodes) -71.000000\n",
      "min_episode_length (50 episodes) 171.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 450\n",
      "exploration 0.275833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90500\n",
      "mean reward (50 episodes) -195.440000\n",
      "mean length (50 episodes) 199.440000\n",
      "max_episode_reward (50 episodes) -86.000000\n",
      "min_episode_length (50 episodes) 186.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 453\n",
      "exploration 0.274625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91000\n",
      "mean reward (50 episodes) -195.440000\n",
      "mean length (50 episodes) 199.440000\n",
      "max_episode_reward (50 episodes) -86.000000\n",
      "min_episode_length (50 episodes) 186.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 455\n",
      "exploration 0.273417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91500\n",
      "mean reward (50 episodes) -195.440000\n",
      "mean length (50 episodes) 199.440000\n",
      "max_episode_reward (50 episodes) -86.000000\n",
      "min_episode_length (50 episodes) 186.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 458\n",
      "exploration 0.272208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92000\n",
      "mean reward (50 episodes) -195.440000\n",
      "mean length (50 episodes) 199.440000\n",
      "max_episode_reward (50 episodes) -86.000000\n",
      "min_episode_length (50 episodes) 186.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 460\n",
      "exploration 0.271000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92500\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 463\n",
      "exploration 0.269792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93000\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 465\n",
      "exploration 0.268583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93500\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 468\n",
      "exploration 0.267375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94000\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 470\n",
      "exploration 0.266167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94500\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 473\n",
      "exploration 0.264958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95000\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 475\n",
      "exploration 0.263750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95500\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 478\n",
      "exploration 0.262542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96000\n",
      "mean reward (50 episodes) -193.000000\n",
      "mean length (50 episodes) 199.000000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 480\n",
      "exploration 0.261333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96500\n",
      "mean reward (50 episodes) -195.280000\n",
      "mean length (50 episodes) 199.280000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 483\n",
      "exploration 0.260125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97000\n",
      "mean reward (50 episodes) -197.560000\n",
      "mean length (50 episodes) 199.560000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 485\n",
      "exploration 0.258917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97500\n",
      "mean reward (50 episodes) -197.560000\n",
      "mean length (50 episodes) 199.560000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 488\n",
      "exploration 0.257708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98000\n",
      "mean reward (50 episodes) -197.560000\n",
      "mean length (50 episodes) 199.560000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 490\n",
      "exploration 0.256500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98500\n",
      "mean reward (50 episodes) -197.560000\n",
      "mean length (50 episodes) 199.560000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 493\n",
      "exploration 0.255292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99000\n",
      "mean reward (50 episodes) -197.560000\n",
      "mean length (50 episodes) 199.560000\n",
      "max_episode_reward (50 episodes) -78.000000\n",
      "min_episode_length (50 episodes) 178.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 495\n",
      "exploration 0.254083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99500\n",
      "mean reward (50 episodes) -194.660000\n",
      "mean length (50 episodes) 198.660000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 498\n",
      "exploration 0.252875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100000\n",
      "mean reward (50 episodes) -194.660000\n",
      "mean length (50 episodes) 198.660000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 500\n",
      "exploration 0.251667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100500\n",
      "mean reward (50 episodes) -194.660000\n",
      "mean length (50 episodes) 198.660000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 503\n",
      "exploration 0.250458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 101000\n",
      "mean reward (50 episodes) -194.660000\n",
      "mean length (50 episodes) 198.660000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 505\n",
      "exploration 0.249250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 101500\n",
      "mean reward (50 episodes) -194.660000\n",
      "mean length (50 episodes) 198.660000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 508\n",
      "exploration 0.248042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102000\n",
      "mean reward (50 episodes) -194.660000\n",
      "mean length (50 episodes) 198.660000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 510\n",
      "exploration 0.246833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102500\n",
      "mean reward (50 episodes) -197.100000\n",
      "mean length (50 episodes) 199.100000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 513\n",
      "exploration 0.245625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103000\n",
      "mean reward (50 episodes) -197.100000\n",
      "mean length (50 episodes) 199.100000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 515\n",
      "exploration 0.244417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103500\n",
      "mean reward (50 episodes) -197.100000\n",
      "mean length (50 episodes) 199.100000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 518\n",
      "exploration 0.243208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104000\n",
      "mean reward (50 episodes) -197.100000\n",
      "mean length (50 episodes) 199.100000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 520\n",
      "exploration 0.242000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104500\n",
      "mean reward (50 episodes) -197.100000\n",
      "mean length (50 episodes) 199.100000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 523\n",
      "exploration 0.240792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105000\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 526\n",
      "exploration 0.239583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105500\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 528\n",
      "exploration 0.238375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106000\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 531\n",
      "exploration 0.237167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106500\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 533\n",
      "exploration 0.235958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107000\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 536\n",
      "exploration 0.234750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107500\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 538\n",
      "exploration 0.233542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108000\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 541\n",
      "exploration 0.232333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108500\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 543\n",
      "exploration 0.231125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109000\n",
      "mean reward (50 episodes) -194.800000\n",
      "mean length (50 episodes) 198.800000\n",
      "max_episode_reward (50 episodes) -55.000000\n",
      "min_episode_length (50 episodes) 155.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 546\n",
      "exploration 0.229917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109500\n",
      "mean reward (50 episodes) -197.700000\n",
      "mean length (50 episodes) 199.700000\n",
      "max_episode_reward (50 episodes) -85.000000\n",
      "min_episode_length (50 episodes) 185.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 548\n",
      "exploration 0.228708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110000\n",
      "mean reward (50 episodes) -197.700000\n",
      "mean length (50 episodes) 199.700000\n",
      "max_episode_reward (50 episodes) -85.000000\n",
      "min_episode_length (50 episodes) 185.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 551\n",
      "exploration 0.227500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110500\n",
      "mean reward (50 episodes) -197.700000\n",
      "mean length (50 episodes) 199.700000\n",
      "max_episode_reward (50 episodes) -85.000000\n",
      "min_episode_length (50 episodes) 185.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 553\n",
      "exploration 0.226292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111000\n",
      "mean reward (50 episodes) -194.640000\n",
      "mean length (50 episodes) 198.640000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 556\n",
      "exploration 0.225083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111500\n",
      "mean reward (50 episodes) -194.640000\n",
      "mean length (50 episodes) 198.640000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 558\n",
      "exploration 0.223875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112000\n",
      "mean reward (50 episodes) -194.640000\n",
      "mean length (50 episodes) 198.640000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 561\n",
      "exploration 0.222667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112500\n",
      "mean reward (50 episodes) -194.640000\n",
      "mean length (50 episodes) 198.640000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 563\n",
      "exploration 0.221458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 113000\n",
      "mean reward (50 episodes) -194.640000\n",
      "mean length (50 episodes) 198.640000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 566\n",
      "exploration 0.220250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 113500\n",
      "mean reward (50 episodes) -194.640000\n",
      "mean length (50 episodes) 198.640000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 568\n",
      "exploration 0.219042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114000\n",
      "mean reward (50 episodes) -192.560000\n",
      "mean length (50 episodes) 198.560000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 571\n",
      "exploration 0.217833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114500\n",
      "mean reward (50 episodes) -192.560000\n",
      "mean length (50 episodes) 198.560000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 573\n",
      "exploration 0.216625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115000\n",
      "mean reward (50 episodes) -194.860000\n",
      "mean length (50 episodes) 198.860000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 576\n",
      "exploration 0.215417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115500\n",
      "mean reward (50 episodes) -194.860000\n",
      "mean length (50 episodes) 198.860000\n",
      "max_episode_reward (50 episodes) -47.000000\n",
      "min_episode_length (50 episodes) 147.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 578\n",
      "exploration 0.214208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116000\n",
      "mean reward (50 episodes) -191.500000\n",
      "mean length (50 episodes) 197.500000\n",
      "max_episode_reward (50 episodes) -32.000000\n",
      "min_episode_length (50 episodes) 132.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -189.680000\n",
      "episodes 581\n",
      "exploration 0.213000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116500\n",
      "mean reward (50 episodes) -188.340000\n",
      "mean length (50 episodes) 196.340000\n",
      "max_episode_reward (50 episodes) -32.000000\n",
      "min_episode_length (50 episodes) 132.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -188.340000\n",
      "episodes 584\n",
      "exploration 0.211792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117000\n",
      "mean reward (50 episodes) -188.340000\n",
      "mean length (50 episodes) 196.340000\n",
      "max_episode_reward (50 episodes) -32.000000\n",
      "min_episode_length (50 episodes) 132.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -188.340000\n",
      "episodes 586\n",
      "exploration 0.210583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117500\n",
      "mean reward (50 episodes) -184.660000\n",
      "mean length (50 episodes) 194.660000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 589\n",
      "exploration 0.209375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118000\n",
      "mean reward (50 episodes) -184.660000\n",
      "mean length (50 episodes) 194.660000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 592\n",
      "exploration 0.208167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118500\n",
      "mean reward (50 episodes) -184.660000\n",
      "mean length (50 episodes) 194.660000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 594\n",
      "exploration 0.206958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119000\n",
      "mean reward (50 episodes) -184.660000\n",
      "mean length (50 episodes) 194.660000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 597\n",
      "exploration 0.205750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119500\n",
      "mean reward (50 episodes) -184.660000\n",
      "mean length (50 episodes) 194.660000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 599\n",
      "exploration 0.204542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120000\n",
      "mean reward (50 episodes) -184.660000\n",
      "mean length (50 episodes) 194.660000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 602\n",
      "exploration 0.203333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120500\n",
      "mean reward (50 episodes) -187.720000\n",
      "mean length (50 episodes) 195.720000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 604\n",
      "exploration 0.202125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121000\n",
      "mean reward (50 episodes) -187.720000\n",
      "mean length (50 episodes) 195.720000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 607\n",
      "exploration 0.200917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121500\n",
      "mean reward (50 episodes) -187.720000\n",
      "mean length (50 episodes) 195.720000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 609\n",
      "exploration 0.199708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122000\n",
      "mean reward (50 episodes) -187.720000\n",
      "mean length (50 episodes) 195.720000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 612\n",
      "exploration 0.198500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122500\n",
      "mean reward (50 episodes) -187.720000\n",
      "mean length (50 episodes) 195.720000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -184.660000\n",
      "episodes 614\n",
      "exploration 0.197292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123000\n",
      "mean reward (50 episodes) -181.080000\n",
      "mean length (50 episodes) 193.080000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 618\n",
      "exploration 0.196083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123500\n",
      "mean reward (50 episodes) -183.160000\n",
      "mean length (50 episodes) 193.160000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 620\n",
      "exploration 0.194875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124000\n",
      "mean reward (50 episodes) -183.160000\n",
      "mean length (50 episodes) 193.160000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 623\n",
      "exploration 0.193667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124500\n",
      "mean reward (50 episodes) -183.160000\n",
      "mean length (50 episodes) 193.160000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 625\n",
      "exploration 0.192458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 125000\n",
      "mean reward (50 episodes) -183.160000\n",
      "mean length (50 episodes) 193.160000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 628\n",
      "exploration 0.191250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 125500\n",
      "mean reward (50 episodes) -183.160000\n",
      "mean length (50 episodes) 193.160000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 630\n",
      "exploration 0.190042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126000\n",
      "mean reward (50 episodes) -186.820000\n",
      "mean length (50 episodes) 194.820000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 633\n",
      "exploration 0.188833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126500\n",
      "mean reward (50 episodes) -186.820000\n",
      "mean length (50 episodes) 194.820000\n",
      "max_episode_reward (50 episodes) -16.000000\n",
      "min_episode_length (50 episodes) 116.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 635\n",
      "exploration 0.187625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127000\n",
      "mean reward (50 episodes) -190.500000\n",
      "mean length (50 episodes) 196.500000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 126.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 638\n",
      "exploration 0.186417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127500\n",
      "mean reward (50 episodes) -190.500000\n",
      "mean length (50 episodes) 196.500000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 126.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 640\n",
      "exploration 0.185208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128000\n",
      "mean reward (50 episodes) -190.500000\n",
      "mean length (50 episodes) 196.500000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 126.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 643\n",
      "exploration 0.184000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128500\n",
      "mean reward (50 episodes) -190.500000\n",
      "mean length (50 episodes) 196.500000\n",
      "max_episode_reward (50 episodes) -26.000000\n",
      "min_episode_length (50 episodes) 126.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 645\n",
      "exploration 0.182792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129000\n",
      "mean reward (50 episodes) -184.460000\n",
      "mean length (50 episodes) 194.460000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -181.080000\n",
      "episodes 648\n",
      "exploration 0.181583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129500\n",
      "mean reward (50 episodes) -178.720000\n",
      "mean length (50 episodes) 192.720000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 651\n",
      "exploration 0.180375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130000\n",
      "mean reward (50 episodes) -178.720000\n",
      "mean length (50 episodes) 192.720000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 654\n",
      "exploration 0.179167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130500\n",
      "mean reward (50 episodes) -178.720000\n",
      "mean length (50 episodes) 192.720000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 656\n",
      "exploration 0.177958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131000\n",
      "mean reward (50 episodes) -178.720000\n",
      "mean length (50 episodes) 192.720000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 659\n",
      "exploration 0.176750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131500\n",
      "mean reward (50 episodes) -178.720000\n",
      "mean length (50 episodes) 192.720000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 661\n",
      "exploration 0.175542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132000\n",
      "mean reward (50 episodes) -178.720000\n",
      "mean length (50 episodes) 192.720000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 664\n",
      "exploration 0.174333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132500\n",
      "mean reward (50 episodes) -178.720000\n",
      "mean length (50 episodes) 192.720000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 666\n",
      "exploration 0.173125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133000\n",
      "mean reward (50 episodes) -182.460000\n",
      "mean length (50 episodes) 194.460000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 669\n",
      "exploration 0.171917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133500\n",
      "mean reward (50 episodes) -182.460000\n",
      "mean length (50 episodes) 194.460000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 671\n",
      "exploration 0.170708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134000\n",
      "mean reward (50 episodes) -182.460000\n",
      "mean length (50 episodes) 194.460000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 674\n",
      "exploration 0.169500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134500\n",
      "mean reward (50 episodes) -179.400000\n",
      "mean length (50 episodes) 193.400000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 677\n",
      "exploration 0.168292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135000\n",
      "mean reward (50 episodes) -179.400000\n",
      "mean length (50 episodes) 193.400000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 679\n",
      "exploration 0.167083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135500\n",
      "mean reward (50 episodes) -179.400000\n",
      "mean length (50 episodes) 193.400000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 682\n",
      "exploration 0.165875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136000\n",
      "mean reward (50 episodes) -182.260000\n",
      "mean length (50 episodes) 194.260000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -178.720000\n",
      "episodes 684\n",
      "exploration 0.164667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136500\n",
      "mean reward (50 episodes) -176.180000\n",
      "mean length (50 episodes) 192.180000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -176.180000\n",
      "episodes 687\n",
      "exploration 0.163458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 137000\n",
      "mean reward (50 episodes) -174.140000\n",
      "mean length (50 episodes) 192.140000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 690\n",
      "exploration 0.162250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 137500\n",
      "mean reward (50 episodes) -174.140000\n",
      "mean length (50 episodes) 192.140000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 692\n",
      "exploration 0.161042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138000\n",
      "mean reward (50 episodes) -174.140000\n",
      "mean length (50 episodes) 192.140000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 695\n",
      "exploration 0.159833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138500\n",
      "mean reward (50 episodes) -176.680000\n",
      "mean length (50 episodes) 192.680000\n",
      "max_episode_reward (50 episodes) -25.000000\n",
      "min_episode_length (50 episodes) 125.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 697\n",
      "exploration 0.158625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139000\n",
      "mean reward (50 episodes) -185.920000\n",
      "mean length (50 episodes) 195.920000\n",
      "max_episode_reward (50 episodes) -46.000000\n",
      "min_episode_length (50 episodes) 146.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 700\n",
      "exploration 0.157417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139500\n",
      "mean reward (50 episodes) -185.920000\n",
      "mean length (50 episodes) 195.920000\n",
      "max_episode_reward (50 episodes) -46.000000\n",
      "min_episode_length (50 episodes) 146.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 702\n",
      "exploration 0.156208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140000\n",
      "mean reward (50 episodes) -181.780000\n",
      "mean length (50 episodes) 193.780000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 705\n",
      "exploration 0.155000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140500\n",
      "mean reward (50 episodes) -181.780000\n",
      "mean length (50 episodes) 193.780000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 708\n",
      "exploration 0.153792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141000\n",
      "mean reward (50 episodes) -179.540000\n",
      "mean length (50 episodes) 193.540000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 710\n",
      "exploration 0.152583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141500\n",
      "mean reward (50 episodes) -179.540000\n",
      "mean length (50 episodes) 193.540000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 713\n",
      "exploration 0.151375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142000\n",
      "mean reward (50 episodes) -179.540000\n",
      "mean length (50 episodes) 193.540000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 715\n",
      "exploration 0.150167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142500\n",
      "mean reward (50 episodes) -179.540000\n",
      "mean length (50 episodes) 193.540000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 718\n",
      "exploration 0.148958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143000\n",
      "mean reward (50 episodes) -182.440000\n",
      "mean length (50 episodes) 194.440000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 720\n",
      "exploration 0.147750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143500\n",
      "mean reward (50 episodes) -174.740000\n",
      "mean length (50 episodes) 190.740000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 724\n",
      "exploration 0.146542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144000\n",
      "mean reward (50 episodes) -177.800000\n",
      "mean length (50 episodes) 191.800000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 726\n",
      "exploration 0.145333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144500\n",
      "mean reward (50 episodes) -177.800000\n",
      "mean length (50 episodes) 191.800000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 729\n",
      "exploration 0.144125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145000\n",
      "mean reward (50 episodes) -175.620000\n",
      "mean length (50 episodes) 191.620000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 731\n",
      "exploration 0.142917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145500\n",
      "mean reward (50 episodes) -175.620000\n",
      "mean length (50 episodes) 191.620000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 734\n",
      "exploration 0.141708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146000\n",
      "mean reward (50 episodes) -178.700000\n",
      "mean length (50 episodes) 192.700000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 736\n",
      "exploration 0.140500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146500\n",
      "mean reward (50 episodes) -183.740000\n",
      "mean length (50 episodes) 193.740000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 739\n",
      "exploration 0.139292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147000\n",
      "mean reward (50 episodes) -180.820000\n",
      "mean length (50 episodes) 192.820000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 742\n",
      "exploration 0.138083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147500\n",
      "mean reward (50 episodes) -180.820000\n",
      "mean length (50 episodes) 192.820000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 744\n",
      "exploration 0.136875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148000\n",
      "mean reward (50 episodes) -180.820000\n",
      "mean length (50 episodes) 192.820000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 747\n",
      "exploration 0.135667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148500\n",
      "mean reward (50 episodes) -180.820000\n",
      "mean length (50 episodes) 192.820000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 749\n",
      "exploration 0.134458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 149000\n",
      "mean reward (50 episodes) -177.640000\n",
      "mean length (50 episodes) 191.640000\n",
      "max_episode_reward (50 episodes) 7.000000\n",
      "min_episode_length (50 episodes) 93.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 752\n",
      "exploration 0.133250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 149500\n",
      "mean reward (50 episodes) -179.440000\n",
      "mean length (50 episodes) 193.440000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 97.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 754\n",
      "exploration 0.132042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150000\n",
      "mean reward (50 episodes) -177.400000\n",
      "mean length (50 episodes) 193.400000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 97.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -174.140000\n",
      "episodes 757\n",
      "exploration 0.130833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150500\n",
      "mean reward (50 episodes) -176.140000\n",
      "mean length (50 episodes) 192.140000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 97.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -173.900000\n",
      "episodes 760\n",
      "exploration 0.129625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151000\n",
      "mean reward (50 episodes) -173.320000\n",
      "mean length (50 episodes) 191.320000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 97.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -173.320000\n",
      "episodes 763\n",
      "exploration 0.128417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151500\n",
      "mean reward (50 episodes) -169.740000\n",
      "mean length (50 episodes) 189.740000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 97.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -169.740000\n",
      "episodes 765\n",
      "exploration 0.127208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152000\n",
      "mean reward (50 episodes) -162.480000\n",
      "mean length (50 episodes) 186.480000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 97.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -162.480000\n",
      "episodes 769\n",
      "exploration 0.126000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152500\n",
      "mean reward (50 episodes) -162.480000\n",
      "mean length (50 episodes) 186.480000\n",
      "max_episode_reward (50 episodes) 3.000000\n",
      "min_episode_length (50 episodes) 97.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -162.480000\n",
      "episodes 771\n",
      "exploration 0.124792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153000\n",
      "mean reward (50 episodes) -167.520000\n",
      "mean length (50 episodes) 189.520000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 774\n",
      "exploration 0.123583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153500\n",
      "mean reward (50 episodes) -167.520000\n",
      "mean length (50 episodes) 189.520000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 776\n",
      "exploration 0.122375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154000\n",
      "mean reward (50 episodes) -164.560000\n",
      "mean length (50 episodes) 188.560000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 779\n",
      "exploration 0.121167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154500\n",
      "mean reward (50 episodes) -164.000000\n",
      "mean length (50 episodes) 188.000000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 782\n",
      "exploration 0.119958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155000\n",
      "mean reward (50 episodes) -164.000000\n",
      "mean length (50 episodes) 188.000000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 784\n",
      "exploration 0.118750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155500\n",
      "mean reward (50 episodes) -164.000000\n",
      "mean length (50 episodes) 188.000000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 787\n",
      "exploration 0.117542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156000\n",
      "mean reward (50 episodes) -163.780000\n",
      "mean length (50 episodes) 187.780000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 790\n",
      "exploration 0.116333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156500\n",
      "mean reward (50 episodes) -160.500000\n",
      "mean length (50 episodes) 186.500000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -159.820000\n",
      "episodes 792\n",
      "exploration 0.115125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157000\n",
      "mean reward (50 episodes) -157.980000\n",
      "mean length (50 episodes) 185.980000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -157.980000\n",
      "episodes 795\n",
      "exploration 0.113917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157500\n",
      "mean reward (50 episodes) -157.980000\n",
      "mean length (50 episodes) 185.980000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -157.980000\n",
      "episodes 798\n",
      "exploration 0.112708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158000\n",
      "mean reward (50 episodes) -157.220000\n",
      "mean length (50 episodes) 185.220000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -157.220000\n",
      "episodes 801\n",
      "exploration 0.111500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158500\n",
      "mean reward (50 episodes) -159.560000\n",
      "mean length (50 episodes) 185.560000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -157.220000\n",
      "episodes 803\n",
      "exploration 0.110292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159000\n",
      "mean reward (50 episodes) -157.500000\n",
      "mean length (50 episodes) 185.500000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -157.220000\n",
      "episodes 806\n",
      "exploration 0.109083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159500\n",
      "mean reward (50 episodes) -156.860000\n",
      "mean length (50 episodes) 184.860000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -156.380000\n",
      "episodes 809\n",
      "exploration 0.107875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160000\n",
      "mean reward (50 episodes) -153.860000\n",
      "mean length (50 episodes) 183.860000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -153.860000\n",
      "episodes 811\n",
      "exploration 0.106667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160500\n",
      "mean reward (50 episodes) -156.680000\n",
      "mean length (50 episodes) 184.680000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -153.860000\n",
      "episodes 814\n",
      "exploration 0.105458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 161000\n",
      "mean reward (50 episodes) -158.340000\n",
      "mean length (50 episodes) 186.340000\n",
      "max_episode_reward (50 episodes) 1.000000\n",
      "min_episode_length (50 episodes) 99.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -153.860000\n",
      "episodes 817\n",
      "exploration 0.104250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 161500\n",
      "mean reward (50 episodes) -162.360000\n",
      "mean length (50 episodes) 188.360000\n",
      "max_episode_reward (50 episodes) -3.000000\n",
      "min_episode_length (50 episodes) 103.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -153.860000\n",
      "episodes 819\n",
      "exploration 0.103042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162000\n",
      "mean reward (50 episodes) -160.980000\n",
      "mean length (50 episodes) 186.980000\n",
      "max_episode_reward (50 episodes) 2.000000\n",
      "min_episode_length (50 episodes) 98.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -153.860000\n",
      "episodes 822\n",
      "exploration 0.101833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162500\n",
      "mean reward (50 episodes) -157.620000\n",
      "mean length (50 episodes) 185.620000\n",
      "max_episode_reward (50 episodes) 2.000000\n",
      "min_episode_length (50 episodes) 98.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -153.860000\n",
      "episodes 825\n",
      "exploration 0.100625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163000\n",
      "mean reward (50 episodes) -152.940000\n",
      "mean length (50 episodes) 182.940000\n",
      "max_episode_reward (50 episodes) 5.000000\n",
      "min_episode_length (50 episodes) 95.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -152.940000\n",
      "episodes 828\n",
      "exploration 0.099417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163500\n",
      "mean reward (50 episodes) -152.940000\n",
      "mean length (50 episodes) 182.940000\n",
      "max_episode_reward (50 episodes) 5.000000\n",
      "min_episode_length (50 episodes) 95.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -152.940000\n",
      "episodes 831\n",
      "exploration 0.098208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164000\n",
      "mean reward (50 episodes) -149.660000\n",
      "mean length (50 episodes) 181.660000\n",
      "max_episode_reward (50 episodes) 5.000000\n",
      "min_episode_length (50 episodes) 95.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -149.660000\n",
      "episodes 834\n",
      "exploration 0.097000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164500\n",
      "mean reward (50 episodes) -145.480000\n",
      "mean length (50 episodes) 179.480000\n",
      "max_episode_reward (50 episodes) 9.000000\n",
      "min_episode_length (50 episodes) 91.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -145.480000\n",
      "episodes 837\n",
      "exploration 0.095792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165000\n",
      "mean reward (50 episodes) -138.800000\n",
      "mean length (50 episodes) 176.800000\n",
      "max_episode_reward (50 episodes) 9.000000\n",
      "min_episode_length (50 episodes) 91.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -138.800000\n",
      "episodes 840\n",
      "exploration 0.094583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165500\n",
      "mean reward (50 episodes) -137.540000\n",
      "mean length (50 episodes) 175.540000\n",
      "max_episode_reward (50 episodes) 37.000000\n",
      "min_episode_length (50 episodes) 63.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -135.020000\n",
      "episodes 844\n",
      "exploration 0.093375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166000\n",
      "mean reward (50 episodes) -133.200000\n",
      "mean length (50 episodes) 173.200000\n",
      "max_episode_reward (50 episodes) 37.000000\n",
      "min_episode_length (50 episodes) 63.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -133.200000\n",
      "episodes 847\n",
      "exploration 0.092167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166500\n",
      "mean reward (50 episodes) -120.080000\n",
      "mean length (50 episodes) 166.080000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -120.080000\n",
      "episodes 852\n",
      "exploration 0.090958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167000\n",
      "mean reward (50 episodes) -111.720000\n",
      "mean length (50 episodes) 161.720000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -111.720000\n",
      "episodes 855\n",
      "exploration 0.089750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167500\n",
      "mean reward (50 episodes) -114.840000\n",
      "mean length (50 episodes) 162.840000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -108.660000\n",
      "episodes 858\n",
      "exploration 0.088542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168000\n",
      "mean reward (50 episodes) -112.420000\n",
      "mean length (50 episodes) 162.420000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -108.660000\n",
      "episodes 860\n",
      "exploration 0.087333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168500\n",
      "mean reward (50 episodes) -104.900000\n",
      "mean length (50 episodes) 158.900000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -104.900000\n",
      "episodes 864\n",
      "exploration 0.086125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169000\n",
      "mean reward (50 episodes) -99.040000\n",
      "mean length (50 episodes) 155.040000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -99.040000\n",
      "episodes 868\n",
      "exploration 0.084917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169500\n",
      "mean reward (50 episodes) -99.040000\n",
      "mean length (50 episodes) 155.040000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -99.040000\n",
      "episodes 870\n",
      "exploration 0.083708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170000\n",
      "mean reward (50 episodes) -98.020000\n",
      "mean length (50 episodes) 156.020000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -96.560000\n",
      "episodes 873\n",
      "exploration 0.082500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170500\n",
      "mean reward (50 episodes) -102.820000\n",
      "mean length (50 episodes) 158.820000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -96.560000\n",
      "episodes 876\n",
      "exploration 0.081292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171000\n",
      "mean reward (50 episodes) -99.800000\n",
      "mean length (50 episodes) 157.800000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -96.560000\n",
      "episodes 879\n",
      "exploration 0.080083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171500\n",
      "mean reward (50 episodes) -99.800000\n",
      "mean length (50 episodes) 157.800000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -96.560000\n",
      "episodes 881\n",
      "exploration 0.078875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172000\n",
      "mean reward (50 episodes) -100.540000\n",
      "mean length (50 episodes) 158.540000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -96.560000\n",
      "episodes 884\n",
      "exploration 0.077667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172500\n",
      "mean reward (50 episodes) -100.820000\n",
      "mean length (50 episodes) 158.820000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -96.560000\n",
      "episodes 888\n",
      "exploration 0.076458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 173000\n",
      "mean reward (50 episodes) -94.580000\n",
      "mean length (50 episodes) 154.580000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -94.580000\n",
      "episodes 892\n",
      "exploration 0.075250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 173500\n",
      "mean reward (50 episodes) -90.680000\n",
      "mean length (50 episodes) 154.680000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -90.680000\n",
      "episodes 895\n",
      "exploration 0.074042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174000\n",
      "mean reward (50 episodes) -94.760000\n",
      "mean length (50 episodes) 156.760000\n",
      "max_episode_reward (50 episodes) 44.000000\n",
      "min_episode_length (50 episodes) 56.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -90.680000\n",
      "episodes 898\n",
      "exploration 0.072833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174500\n",
      "mean reward (50 episodes) -95.300000\n",
      "mean length (50 episodes) 159.300000\n",
      "max_episode_reward (50 episodes) 33.000000\n",
      "min_episode_length (50 episodes) 67.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -90.680000\n",
      "episodes 902\n",
      "exploration 0.071625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175000\n",
      "mean reward (50 episodes) -96.880000\n",
      "mean length (50 episodes) 158.880000\n",
      "max_episode_reward (50 episodes) 33.000000\n",
      "min_episode_length (50 episodes) 67.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -90.680000\n",
      "episodes 905\n",
      "exploration 0.070417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175500\n",
      "mean reward (50 episodes) -87.200000\n",
      "mean length (50 episodes) 153.200000\n",
      "max_episode_reward (50 episodes) 33.000000\n",
      "min_episode_length (50 episodes) 67.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -87.200000\n",
      "episodes 910\n",
      "exploration 0.069208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176000\n",
      "mean reward (50 episodes) -93.600000\n",
      "mean length (50 episodes) 155.600000\n",
      "max_episode_reward (50 episodes) 29.000000\n",
      "min_episode_length (50 episodes) 71.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -87.200000\n",
      "episodes 913\n",
      "exploration 0.068000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176500\n",
      "mean reward (50 episodes) -91.160000\n",
      "mean length (50 episodes) 155.160000\n",
      "max_episode_reward (50 episodes) 29.000000\n",
      "min_episode_length (50 episodes) 71.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -87.200000\n",
      "episodes 915\n",
      "exploration 0.066792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177000\n",
      "mean reward (50 episodes) -90.160000\n",
      "mean length (50 episodes) 156.160000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 72.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -87.200000\n",
      "episodes 919\n",
      "exploration 0.065583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177500\n",
      "mean reward (50 episodes) -82.300000\n",
      "mean length (50 episodes) 152.300000\n",
      "max_episode_reward (50 episodes) 28.000000\n",
      "min_episode_length (50 episodes) 72.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -82.300000\n",
      "episodes 922\n",
      "exploration 0.064375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178000\n",
      "mean reward (50 episodes) -77.720000\n",
      "mean length (50 episodes) 147.720000\n",
      "max_episode_reward (50 episodes) 39.000000\n",
      "min_episode_length (50 episodes) 61.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -77.720000\n",
      "episodes 927\n",
      "exploration 0.063167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178500\n",
      "mean reward (50 episodes) -69.380000\n",
      "mean length (50 episodes) 143.380000\n",
      "max_episode_reward (50 episodes) 39.000000\n",
      "min_episode_length (50 episodes) 61.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -69.380000\n",
      "episodes 931\n",
      "exploration 0.061958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179000\n",
      "mean reward (50 episodes) -57.760000\n",
      "mean length (50 episodes) 135.760000\n",
      "max_episode_reward (50 episodes) 39.000000\n",
      "min_episode_length (50 episodes) 61.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -57.760000\n",
      "episodes 936\n",
      "exploration 0.060750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179500\n",
      "mean reward (50 episodes) -54.540000\n",
      "mean length (50 episodes) 134.540000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.760000\n",
      "episodes 940\n",
      "exploration 0.059542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180000\n",
      "mean reward (50 episodes) -56.980000\n",
      "mean length (50 episodes) 136.980000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.760000\n",
      "episodes 943\n",
      "exploration 0.058333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180500\n",
      "mean reward (50 episodes) -55.300000\n",
      "mean length (50 episodes) 135.300000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.760000\n",
      "episodes 946\n",
      "exploration 0.057125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181000\n",
      "mean reward (50 episodes) -56.200000\n",
      "mean length (50 episodes) 136.200000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.520000\n",
      "episodes 950\n",
      "exploration 0.055917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181500\n",
      "mean reward (50 episodes) -53.400000\n",
      "mean length (50 episodes) 133.400000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.520000\n",
      "episodes 954\n",
      "exploration 0.054708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182000\n",
      "mean reward (50 episodes) -59.420000\n",
      "mean length (50 episodes) 137.420000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.520000\n",
      "episodes 957\n",
      "exploration 0.053500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182500\n",
      "mean reward (50 episodes) -57.760000\n",
      "mean length (50 episodes) 135.760000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.520000\n",
      "episodes 961\n",
      "exploration 0.052292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183000\n",
      "mean reward (50 episodes) -57.920000\n",
      "mean length (50 episodes) 135.920000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.520000\n",
      "episodes 964\n",
      "exploration 0.051083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183500\n",
      "mean reward (50 episodes) -55.920000\n",
      "mean length (50 episodes) 133.920000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.520000\n",
      "episodes 968\n",
      "exploration 0.049875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184000\n",
      "mean reward (50 episodes) -52.720000\n",
      "mean length (50 episodes) 130.720000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -52.520000\n",
      "episodes 972\n",
      "exploration 0.048667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184500\n",
      "mean reward (50 episodes) -47.440000\n",
      "mean length (50 episodes) 127.440000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -47.440000\n",
      "episodes 978\n",
      "exploration 0.047458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 185000\n",
      "mean reward (50 episodes) -45.280000\n",
      "mean length (50 episodes) 125.280000\n",
      "max_episode_reward (50 episodes) 50.000000\n",
      "min_episode_length (50 episodes) 50.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -45.280000\n",
      "episodes 983\n",
      "exploration 0.046250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 185500\n",
      "mean reward (50 episodes) -48.280000\n",
      "mean length (50 episodes) 126.280000\n",
      "max_episode_reward (50 episodes) 49.000000\n",
      "min_episode_length (50 episodes) 51.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -43.960000\n",
      "episodes 988\n",
      "exploration 0.045042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186000\n",
      "mean reward (50 episodes) -46.200000\n",
      "mean length (50 episodes) 124.200000\n",
      "max_episode_reward (50 episodes) 49.000000\n",
      "min_episode_length (50 episodes) 51.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -43.960000\n",
      "episodes 992\n",
      "exploration 0.043833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186500\n",
      "mean reward (50 episodes) -38.200000\n",
      "mean length (50 episodes) 118.200000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -38.200000\n",
      "episodes 997\n",
      "exploration 0.042625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187000\n",
      "mean reward (50 episodes) -32.840000\n",
      "mean length (50 episodes) 114.840000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -32.840000\n",
      "episodes 1001\n",
      "exploration 0.041417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187500\n",
      "mean reward (50 episodes) -29.560000\n",
      "mean length (50 episodes) 113.560000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -29.560000\n",
      "episodes 1006\n",
      "exploration 0.040208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188000\n",
      "mean reward (50 episodes) -23.520000\n",
      "mean length (50 episodes) 111.520000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -21.920000\n",
      "episodes 1010\n",
      "exploration 0.039000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188500\n",
      "mean reward (50 episodes) -11.940000\n",
      "mean length (50 episodes) 103.940000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -11.940000\n",
      "episodes 1016\n",
      "exploration 0.037792\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189000\n",
      "mean reward (50 episodes) -14.480000\n",
      "mean length (50 episodes) 104.480000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1020\n",
      "exploration 0.036583\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189500\n",
      "mean reward (50 episodes) -17.780000\n",
      "mean length (50 episodes) 105.780000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1023\n",
      "exploration 0.035375\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190000\n",
      "mean reward (50 episodes) -24.160000\n",
      "mean length (50 episodes) 110.160000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1028\n",
      "exploration 0.034167\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190500\n",
      "mean reward (50 episodes) -21.280000\n",
      "mean length (50 episodes) 109.280000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1033\n",
      "exploration 0.032958\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191000\n",
      "mean reward (50 episodes) -29.500000\n",
      "mean length (50 episodes) 115.500000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1036\n",
      "exploration 0.031750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191500\n",
      "mean reward (50 episodes) -28.080000\n",
      "mean length (50 episodes) 116.080000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1040\n",
      "exploration 0.030542\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192000\n",
      "mean reward (50 episodes) -28.540000\n",
      "mean length (50 episodes) 114.540000\n",
      "max_episode_reward (50 episodes) 59.000000\n",
      "min_episode_length (50 episodes) 41.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1043\n",
      "exploration 0.029333\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192500\n",
      "mean reward (50 episodes) -36.980000\n",
      "mean length (50 episodes) 120.980000\n",
      "max_episode_reward (50 episodes) 53.000000\n",
      "min_episode_length (50 episodes) 47.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1047\n",
      "exploration 0.028125\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193000\n",
      "mean reward (50 episodes) -37.780000\n",
      "mean length (50 episodes) 121.780000\n",
      "max_episode_reward (50 episodes) 49.000000\n",
      "min_episode_length (50 episodes) 51.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1051\n",
      "exploration 0.026917\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193500\n",
      "mean reward (50 episodes) -42.780000\n",
      "mean length (50 episodes) 124.780000\n",
      "max_episode_reward (50 episodes) 49.000000\n",
      "min_episode_length (50 episodes) 51.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1054\n",
      "exploration 0.025708\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194000\n",
      "mean reward (50 episodes) -43.900000\n",
      "mean length (50 episodes) 123.900000\n",
      "max_episode_reward (50 episodes) 49.000000\n",
      "min_episode_length (50 episodes) 51.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1059\n",
      "exploration 0.024500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194500\n",
      "mean reward (50 episodes) -50.980000\n",
      "mean length (50 episodes) 126.980000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1062\n",
      "exploration 0.023292\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195000\n",
      "mean reward (50 episodes) -57.120000\n",
      "mean length (50 episodes) 131.120000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1065\n",
      "exploration 0.022083\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195500\n",
      "mean reward (50 episodes) -66.820000\n",
      "mean length (50 episodes) 136.820000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1068\n",
      "exploration 0.020875\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196000\n",
      "mean reward (50 episodes) -71.440000\n",
      "mean length (50 episodes) 139.440000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1070\n",
      "exploration 0.019667\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196500\n",
      "mean reward (50 episodes) -75.580000\n",
      "mean length (50 episodes) 141.580000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1073\n",
      "exploration 0.018458\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 197000\n",
      "mean reward (50 episodes) -78.680000\n",
      "mean length (50 episodes) 142.680000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1075\n",
      "exploration 0.017250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 197500\n",
      "mean reward (50 episodes) -91.220000\n",
      "mean length (50 episodes) 149.220000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1078\n",
      "exploration 0.016042\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198000\n",
      "mean reward (50 episodes) -99.200000\n",
      "mean length (50 episodes) 153.200000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1080\n",
      "exploration 0.014833\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198500\n",
      "mean reward (50 episodes) -111.500000\n",
      "mean length (50 episodes) 159.500000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1083\n",
      "exploration 0.013625\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199000\n",
      "mean reward (50 episodes) -116.520000\n",
      "mean length (50 episodes) 160.520000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1085\n",
      "exploration 0.012417\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199500\n",
      "mean reward (50 episodes) -123.080000\n",
      "mean length (50 episodes) 163.080000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1088\n",
      "exploration 0.011208\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 200000\n",
      "mean reward (50 episodes) -127.840000\n",
      "mean length (50 episodes) 165.840000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1091\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 200500\n",
      "mean reward (50 episodes) -135.920000\n",
      "mean length (50 episodes) 169.920000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1093\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 201000\n",
      "mean reward (50 episodes) -140.700000\n",
      "mean length (50 episodes) 170.700000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1096\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 201500\n",
      "mean reward (50 episodes) -149.040000\n",
      "mean length (50 episodes) 175.040000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1098\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 202000\n",
      "mean reward (50 episodes) -149.120000\n",
      "mean length (50 episodes) 175.120000\n",
      "max_episode_reward (50 episodes) 57.000000\n",
      "min_episode_length (50 episodes) 43.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 200.000000\n",
      "best mean reward -10.120000\n",
      "episodes 1103\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Get Atari games.\n",
    "    # benchmark = gym.benchmark_spec('Atari40M')\n",
    "    #\n",
    "    # # Change the index to select a different game.\n",
    "    # task = benchmark.tasks[3]\n",
    "    #\n",
    "    # # Run training\n",
    "    seed = 0  # Use a seed of zero (you may want to randomize the seed!)\n",
    "    set_global_seeds(seed)\n",
    "    # env = get_env(task, seed)\n",
    "    env = ArmEnvDQN(episode_max_length=200,\n",
    "                 size_x=6,\n",
    "                 size_y=4,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=100,\n",
    "                 tower_target_size=4)\n",
    "    session = get_session()\n",
    "    ep_rew, ep_len = arm_learn(env, session, num_timesteps=500000)\n",
    "    \n",
    "    thefile = open('ep_rew_6_4_4_plain_dqn.txt', 'w')\n",
    "    for item in ep_rew:\n",
    "        thefile.write(\"%s\\n\" % item)\n",
    "        \n",
    "    thefile2 = open('ep_len_6_4_4_plain_dqn.txt', 'w')\n",
    "    for item in ep_len:\n",
    "        thefile2.write(\"%s\\n\" % item)\n",
    "        \n",
    "#     stats = plotting.EpisodeStats(\n",
    "#         episode_lengths=ep_len,\n",
    "#         episode_rewards=ep_rew)\n",
    "#     plotting.plot_episode_stats(stats)\n",
    "#     tf.summary.FileWriter(\"logs1\", tf.get_default_graph()).close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir=logs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_observation(frame):\n",
    "    img_h, img_w = frame.shape[1], frame.shape[2]\n",
    "    return frame.transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n",
    "\n",
    "def main():\n",
    "    env = ArmEnvDQN(episode_max_length=200,\n",
    "                 size_x=5,\n",
    "                 size_y=5,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=400,\n",
    "                 tower_target_size=4)\n",
    "    # print(env.reset())\n",
    "    session = tf.Session()\n",
    "    # First let's load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph('my_test_model.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    frame_history_len = 1\n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_shape = (img_h, img_w, frame_history_len * img_c)  # size_x, size_y,\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "#     # placeholder for current observation (or state)\n",
    "#     obs_t_ph = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "#     # casting to float on GPU ensures lower data transfer times.\n",
    "#     obs_t_float = tf.cast(obs_t_ph, tf.float32) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "#     pred_q = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "#     pred_ac = tf.argmax(pred_q, axis=1)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    obs_t_float = graph.get_tensor_by_name(\"obs_t_ph:0\")\n",
    " \n",
    "    ## How to access saved operation\n",
    "    pred_ac = graph.get_tensor_by_name(\"pred_ac:0\")\n",
    "    \n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    last_obs = env.reset()\n",
    "\n",
    "    for t in itertools.count():\n",
    "\n",
    "        obs = encode_observation(np.array([last_obs]))\n",
    "        action = session.run(pred_ac, {obs_t_float: [obs]})[0]\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done or episode_length == 500:\n",
    "            break\n",
    "\n",
    "        last_obs = next_obs\n",
    "    print(episode_reward, episode_length)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ep_rew_4_3_3.txt') as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ep_rew_4_3_3.txt') as f:\n",
    "    array = []\n",
    "    for line in f: # read rest of lines\n",
    "        array.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ep_rew_4_3_3_plain_dqn.txt') as f:\n",
    "    array2 = []\n",
    "    for line in f: # read rest of lines\n",
    "        array2.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-243,\n",
       " -93,\n",
       " -10,\n",
       " -239,\n",
       " -263,\n",
       " -232,\n",
       " -144,\n",
       " -247,\n",
       " -128,\n",
       " -31,\n",
       " -243,\n",
       " -20,\n",
       " -14,\n",
       " -238,\n",
       " -234,\n",
       " -239,\n",
       " -281,\n",
       " -142,\n",
       " -72,\n",
       " 20,\n",
       " -107,\n",
       " -6,\n",
       " -60,\n",
       " -15,\n",
       " -227,\n",
       " -249,\n",
       " -233,\n",
       " -223,\n",
       " -136,\n",
       " -96,\n",
       " -246,\n",
       " -244,\n",
       " -236,\n",
       " -6,\n",
       " -253,\n",
       " -302,\n",
       " -115,\n",
       " -222,\n",
       " 4,\n",
       " -94,\n",
       " -221,\n",
       " -218,\n",
       " -235,\n",
       " -234,\n",
       " -221,\n",
       " -237,\n",
       " -227,\n",
       " -217,\n",
       " -238,\n",
       " -86,\n",
       " -72,\n",
       " -110,\n",
       " -221,\n",
       " -91,\n",
       " -224,\n",
       " -223,\n",
       " -222,\n",
       " 50,\n",
       " -89,\n",
       " -234,\n",
       " -220,\n",
       " -117,\n",
       " -220,\n",
       " -218,\n",
       " -227,\n",
       " -210,\n",
       " -127,\n",
       " -219,\n",
       " 49,\n",
       " 31,\n",
       " -50,\n",
       " -215,\n",
       " -233,\n",
       " 77,\n",
       " 9,\n",
       " -222,\n",
       " -220,\n",
       " -220,\n",
       " 44,\n",
       " -103,\n",
       " -130,\n",
       " -173,\n",
       " -217,\n",
       " -215,\n",
       " 55,\n",
       " -62,\n",
       " -219,\n",
       " 51,\n",
       " 5,\n",
       " -219,\n",
       " 62,\n",
       " -214,\n",
       " 64,\n",
       " 28,\n",
       " 36,\n",
       " -34,\n",
       " -67,\n",
       " 69,\n",
       " 40,\n",
       " -59,\n",
       " -78,\n",
       " -218,\n",
       " -225,\n",
       " 57,\n",
       " 10,\n",
       " 73,\n",
       " 39,\n",
       " 68,\n",
       " 0,\n",
       " -33,\n",
       " 26,\n",
       " -89,\n",
       " -130,\n",
       " 67,\n",
       " -219,\n",
       " 61,\n",
       " 58,\n",
       " 64,\n",
       " 35,\n",
       " -7,\n",
       " -67,\n",
       " 17,\n",
       " -96,\n",
       " -1,\n",
       " -17,\n",
       " 2,\n",
       " 52,\n",
       " -214,\n",
       " -216,\n",
       " 54,\n",
       " -214,\n",
       " 66,\n",
       " 65,\n",
       " 43,\n",
       " -9,\n",
       " 72,\n",
       " -213,\n",
       " -34,\n",
       " 21,\n",
       " 78,\n",
       " -20,\n",
       " 70,\n",
       " -210,\n",
       " 53,\n",
       " -64,\n",
       " -1,\n",
       " 34,\n",
       " 37,\n",
       " 12,\n",
       " 61,\n",
       " 77,\n",
       " -7,\n",
       " 75,\n",
       " -93,\n",
       " 73,\n",
       " 44,\n",
       " 21,\n",
       " 80,\n",
       " -32,\n",
       " 60,\n",
       " -189,\n",
       " 54,\n",
       " 33,\n",
       " 48,\n",
       " 84,\n",
       " 11,\n",
       " 36,\n",
       " 65,\n",
       " 52,\n",
       " 75,\n",
       " 82,\n",
       " 79,\n",
       " 75,\n",
       " -79,\n",
       " 50,\n",
       " 23,\n",
       " 34,\n",
       " 75,\n",
       " 63,\n",
       " 78,\n",
       " 77,\n",
       " 84,\n",
       " -12,\n",
       " 42,\n",
       " -17,\n",
       " 62,\n",
       " 56,\n",
       " -20,\n",
       " -214,\n",
       " 65,\n",
       " 59,\n",
       " 38,\n",
       " -7,\n",
       " 59,\n",
       " -105,\n",
       " 63,\n",
       " 37,\n",
       " 73,\n",
       " 37,\n",
       " 49,\n",
       " 80,\n",
       " 55,\n",
       " 73,\n",
       " 54,\n",
       " 73,\n",
       " 48,\n",
       " 49,\n",
       " 14,\n",
       " 78,\n",
       " 72,\n",
       " 75,\n",
       " 84,\n",
       " 58,\n",
       " 71,\n",
       " 49,\n",
       " 64,\n",
       " 62,\n",
       " 80,\n",
       " 57,\n",
       " 55,\n",
       " 83,\n",
       " 79,\n",
       " 63,\n",
       " 80,\n",
       " 81,\n",
       " 84,\n",
       " 78,\n",
       " 83,\n",
       " 41,\n",
       " 82,\n",
       " 78,\n",
       " 85,\n",
       " 81,\n",
       " 77,\n",
       " 43,\n",
       " 84,\n",
       " 70,\n",
       " 81,\n",
       " 59,\n",
       " 81,\n",
       " 82,\n",
       " 81,\n",
       " 82,\n",
       " 73,\n",
       " 63,\n",
       " 80,\n",
       " 78,\n",
       " 69,\n",
       " 82,\n",
       " 81,\n",
       " 47,\n",
       " 64,\n",
       " 73,\n",
       " 84,\n",
       " 80,\n",
       " -84,\n",
       " 85,\n",
       " 64,\n",
       " 85,\n",
       " 77,\n",
       " 85,\n",
       " 82,\n",
       " 73,\n",
       " 84,\n",
       " 74,\n",
       " 53,\n",
       " 75,\n",
       " 81,\n",
       " 78,\n",
       " 83,\n",
       " 81,\n",
       " 85,\n",
       " 81,\n",
       " 67,\n",
       " 85,\n",
       " 40,\n",
       " 62,\n",
       " 81,\n",
       " 79,\n",
       " 85,\n",
       " 61,\n",
       " 71,\n",
       " 20,\n",
       " 47,\n",
       " 58,\n",
       " 82,\n",
       " 80,\n",
       " 71,\n",
       " 85,\n",
       " 84,\n",
       " 67,\n",
       " 80,\n",
       " 77,\n",
       " 70,\n",
       " 72,\n",
       " 85,\n",
       " 66,\n",
       " 64,\n",
       " 79,\n",
       " 46,\n",
       " 84,\n",
       " 69,\n",
       " 46,\n",
       " 83,\n",
       " 70,\n",
       " 74,\n",
       " 72,\n",
       " 41,\n",
       " 64,\n",
       " 72,\n",
       " 61,\n",
       " 72,\n",
       " 61,\n",
       " 84,\n",
       " 79,\n",
       " 83,\n",
       " 80,\n",
       " 58,\n",
       " 57,\n",
       " 69,\n",
       " 76,\n",
       " 87,\n",
       " 41,\n",
       " 71,\n",
       " 54,\n",
       " 71,\n",
       " 85,\n",
       " 1,\n",
       " 54,\n",
       " -6,\n",
       " 77,\n",
       " 12,\n",
       " 70,\n",
       " 50,\n",
       " -32,\n",
       " 79,\n",
       " 69,\n",
       " 67,\n",
       " 83,\n",
       " 59,\n",
       " 59,\n",
       " 83,\n",
       " 83,\n",
       " 85,\n",
       " 79,\n",
       " 78,\n",
       " 58,\n",
       " 71,\n",
       " 82,\n",
       " 24,\n",
       " 74,\n",
       " 61,\n",
       " 69,\n",
       " 12,\n",
       " 82,\n",
       " 73,\n",
       " 46,\n",
       " 79,\n",
       " 71,\n",
       " 79,\n",
       " 69,\n",
       " 84,\n",
       " 85,\n",
       " 83,\n",
       " 73,\n",
       " 84,\n",
       " 83,\n",
       " 83,\n",
       " 79,\n",
       " 78,\n",
       " 82,\n",
       " 81,\n",
       " 77,\n",
       " 85,\n",
       " 80,\n",
       " 77,\n",
       " 83,\n",
       " 82,\n",
       " 78,\n",
       " 53,\n",
       " 85,\n",
       " 84,\n",
       " 66,\n",
       " 80,\n",
       " 67,\n",
       " 81,\n",
       " 85,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 73,\n",
       " 85,\n",
       " 81,\n",
       " 65,\n",
       " 84,\n",
       " 85,\n",
       " 83,\n",
       " 82,\n",
       " 85,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 82,\n",
       " 70,\n",
       " 49,\n",
       " 66,\n",
       " 52,\n",
       " 80,\n",
       " 85,\n",
       " 85,\n",
       " 83,\n",
       " 79,\n",
       " 71,\n",
       " 74,\n",
       " 81,\n",
       " 77,\n",
       " 81,\n",
       " 64,\n",
       " 81,\n",
       " 84,\n",
       " -5,\n",
       " 84,\n",
       " 80,\n",
       " 84,\n",
       " 84,\n",
       " 82,\n",
       " 83,\n",
       " 72,\n",
       " 59,\n",
       " 82,\n",
       " 84,\n",
       " 77,\n",
       " 82,\n",
       " 85,\n",
       " 77,\n",
       " 83,\n",
       " 84,\n",
       " 79,\n",
       " 75,\n",
       " 82,\n",
       " 85,\n",
       " 83,\n",
       " 84,\n",
       " 80,\n",
       " 79,\n",
       " 85,\n",
       " 85,\n",
       " 67,\n",
       " 52,\n",
       " 84,\n",
       " 85,\n",
       " 81,\n",
       " 85,\n",
       " 82,\n",
       " 73,\n",
       " 85,\n",
       " 84,\n",
       " 83,\n",
       " 84,\n",
       " 46,\n",
       " 84,\n",
       " 81,\n",
       " 79,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 80,\n",
       " 85,\n",
       " 65,\n",
       " 85,\n",
       " 85,\n",
       " 38,\n",
       " 85,\n",
       " 85,\n",
       " 75,\n",
       " 29,\n",
       " 70,\n",
       " 69,\n",
       " 58,\n",
       " 59,\n",
       " 84,\n",
       " 60,\n",
       " 28,\n",
       " 79,\n",
       " 79,\n",
       " 85,\n",
       " 80,\n",
       " 24,\n",
       " 83,\n",
       " 84,\n",
       " 83,\n",
       " 84,\n",
       " 78,\n",
       " 85,\n",
       " 60,\n",
       " 77,\n",
       " 79,\n",
       " 83,\n",
       " 69,\n",
       " 64,\n",
       " 83,\n",
       " 85,\n",
       " 61,\n",
       " 85,\n",
       " 77,\n",
       " 79,\n",
       " 83,\n",
       " 85,\n",
       " 85,\n",
       " 80,\n",
       " 78,\n",
       " 84,\n",
       " 83,\n",
       " 68,\n",
       " 85,\n",
       " 85,\n",
       " 64,\n",
       " 61,\n",
       " 74,\n",
       " 83,\n",
       " 73,\n",
       " 81,\n",
       " 79,\n",
       " 79,\n",
       " 84,\n",
       " 84,\n",
       " 85,\n",
       " 81,\n",
       " 84,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 45,\n",
       " 75,\n",
       " 83,\n",
       " 79,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 82,\n",
       " 84,\n",
       " 84,\n",
       " 82,\n",
       " 85,\n",
       " 84,\n",
       " 85,\n",
       " 82,\n",
       " 78,\n",
       " 54,\n",
       " 77,\n",
       " 83,\n",
       " 85,\n",
       " 40,\n",
       " 82,\n",
       " 66,\n",
       " 82,\n",
       " 85,\n",
       " 81,\n",
       " 77,\n",
       " 84,\n",
       " 83,\n",
       " 63,\n",
       " 82,\n",
       " 79,\n",
       " 79,\n",
       " 72,\n",
       " 83,\n",
       " 79,\n",
       " 73,\n",
       " 79,\n",
       " 83,\n",
       " 64,\n",
       " 85,\n",
       " 62,\n",
       " 82,\n",
       " 83,\n",
       " 85,\n",
       " 82,\n",
       " 81,\n",
       " 78,\n",
       " 81,\n",
       " 84,\n",
       " 64,\n",
       " 83,\n",
       " 85,\n",
       " 80,\n",
       " 81,\n",
       " 79,\n",
       " 83,\n",
       " 67,\n",
       " 48,\n",
       " 62,\n",
       " 36,\n",
       " 37,\n",
       " 84,\n",
       " 0,\n",
       " 55,\n",
       " 34,\n",
       " 81,\n",
       " 35,\n",
       " 50,\n",
       " 31,\n",
       " 77,\n",
       " 75,\n",
       " 85,\n",
       " 39,\n",
       " 80,\n",
       " 78,\n",
       " 82,\n",
       " 80,\n",
       " 66,\n",
       " 81,\n",
       " 84,\n",
       " 83,\n",
       " -351,\n",
       " 78,\n",
       " 84,\n",
       " 28,\n",
       " 45,\n",
       " 72,\n",
       " 83,\n",
       " 78,\n",
       " 16,\n",
       " 84,\n",
       " 75,\n",
       " 82,\n",
       " 79,\n",
       " 85,\n",
       " 84,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 83,\n",
       " 76,\n",
       " 82,\n",
       " 56,\n",
       " 83,\n",
       " 81,\n",
       " 84,\n",
       " 85,\n",
       " 84,\n",
       " 83,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 57,\n",
       " -76,\n",
       " -150,\n",
       " 6,\n",
       " 17,\n",
       " -80,\n",
       " 79,\n",
       " 32,\n",
       " 84,\n",
       " 65,\n",
       " 85,\n",
       " 42,\n",
       " 82,\n",
       " 85,\n",
       " 80,\n",
       " 85,\n",
       " 84,\n",
       " 49,\n",
       " 82,\n",
       " 85,\n",
       " 57,\n",
       " 84,\n",
       " 82,\n",
       " 78,\n",
       " 84,\n",
       " 85,\n",
       " 84,\n",
       " 77,\n",
       " 81,\n",
       " -209,\n",
       " 85,\n",
       " 81,\n",
       " 82,\n",
       " 85,\n",
       " 82,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 70,\n",
       " 82,\n",
       " 82,\n",
       " 74,\n",
       " 60,\n",
       " 83,\n",
       " -360,\n",
       " 85,\n",
       " 58,\n",
       " 82,\n",
       " 81,\n",
       " 84,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 84,\n",
       " 79,\n",
       " 60,\n",
       " 58,\n",
       " 77,\n",
       " 80,\n",
       " 83,\n",
       " 78,\n",
       " 81,\n",
       " 84,\n",
       " 23,\n",
       " 81,\n",
       " 83,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 76,\n",
       " -167,\n",
       " 85,\n",
       " 78,\n",
       " 81,\n",
       " 85,\n",
       " 84,\n",
       " 85,\n",
       " 82,\n",
       " 85,\n",
       " 83,\n",
       " 85,\n",
       " 85,\n",
       " 83,\n",
       " 69,\n",
       " 82,\n",
       " 64,\n",
       " 84,\n",
       " 76,\n",
       " 72,\n",
       " 83,\n",
       " 85,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 82,\n",
       " 85,\n",
       " 76,\n",
       " 85,\n",
       " 72,\n",
       " 85,\n",
       " 82,\n",
       " -233,\n",
       " 85,\n",
       " 84,\n",
       " 85,\n",
       " 77,\n",
       " 85,\n",
       " 74,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 77,\n",
       " 84,\n",
       " 85,\n",
       " 82,\n",
       " 83,\n",
       " 85,\n",
       " 83,\n",
       " 49,\n",
       " 63,\n",
       " 83,\n",
       " 81,\n",
       " 56,\n",
       " 73,\n",
       " 82,\n",
       " 83,\n",
       " -53,\n",
       " 68,\n",
       " 85,\n",
       " 83,\n",
       " 85,\n",
       " 85,\n",
       " 79,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 75,\n",
       " 83,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 52,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 83,\n",
       " 85,\n",
       " 69,\n",
       " 85,\n",
       " 81,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 81,\n",
       " 79,\n",
       " 85,\n",
       " 83,\n",
       " 85,\n",
       " -150,\n",
       " 80,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 82,\n",
       " 78,\n",
       " 83,\n",
       " 83,\n",
       " 82,\n",
       " 82,\n",
       " 84,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 85,\n",
       " 84,\n",
       " 79,\n",
       " 80,\n",
       " 83,\n",
       " 75,\n",
       " 85,\n",
       " 85,\n",
       " 84,\n",
       " 84,\n",
       " 73,\n",
       " 83,\n",
       " 33,\n",
       " 83,\n",
       " 81,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 83,\n",
       " 82,\n",
       " 83,\n",
       " 85,\n",
       " 63,\n",
       " 84,\n",
       " 84,\n",
       " 81,\n",
       " 85,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 83,\n",
       " 85,\n",
       " 80,\n",
       " 77,\n",
       " 82,\n",
       " 62,\n",
       " 83,\n",
       " 85,\n",
       " 80,\n",
       " 83,\n",
       " 80,\n",
       " 85,\n",
       " 84,\n",
       " 78,\n",
       " 85,\n",
       " 85,\n",
       " 79,\n",
       " 85,\n",
       " 80,\n",
       " 79,\n",
       " 83,\n",
       " 80,\n",
       " 85,\n",
       " 81,\n",
       " 75,\n",
       " 85,\n",
       " 79,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 83,\n",
       " 82,\n",
       " 77,\n",
       " 85,\n",
       " 61,\n",
       " 84,\n",
       " 82,\n",
       " 60,\n",
       " 84,\n",
       " 85,\n",
       " 84,\n",
       " 84,\n",
       " 60,\n",
       " 85,\n",
       " 77,\n",
       " 83,\n",
       " 84,\n",
       " 84,\n",
       " 85,\n",
       " 83,\n",
       " 63,\n",
       " 85,\n",
       " 85,\n",
       " 81,\n",
       " 85,\n",
       " 80,\n",
       " 84,\n",
       " 77,\n",
       " 85,\n",
       " 81,\n",
       " 85,\n",
       " 85,\n",
       " 79,\n",
       " 82,\n",
       " 85,\n",
       " 81,\n",
       " 83,\n",
       " 83,\n",
       " 85,\n",
       " 84,\n",
       " 75,\n",
       " 83,\n",
       " 79,\n",
       " 84,\n",
       " 85,\n",
       " 84,\n",
       " 85,\n",
       " 83,\n",
       " 78,\n",
       " 83,\n",
       " 84,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 79,\n",
       " 82,\n",
       " 47,\n",
       " 85,\n",
       " 72,\n",
       " 84,\n",
       " 68,\n",
       " 78,\n",
       " 71,\n",
       " 84,\n",
       " 83,\n",
       " 83,\n",
       " 85,\n",
       " 76,\n",
       " 85,\n",
       " 77,\n",
       " 80,\n",
       " 83,\n",
       " 82,\n",
       " 84,\n",
       " 85,\n",
       " 78,\n",
       " 85,\n",
       " 81,\n",
       " 85,\n",
       " 77,\n",
       " 85,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 77,\n",
       " 82,\n",
       " 84,\n",
       " 82,\n",
       " 84,\n",
       " 74,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 83,\n",
       " 84,\n",
       " 84,\n",
       " 57,\n",
       " 79,\n",
       " 85,\n",
       " 83,\n",
       " 85,\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a257649b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotting.plot_multi_test(smoothing_window=30,\n",
    "                             x_label=\"episode\",\n",
    "                             y_label=\"smoothed rewards\",\n",
    "                             curve_to_draw=[array,\n",
    "                                            array2],\n",
    "                             labels=[\"options\", \"dqn\"]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ep_len_4_3_3.txt') as f:\n",
    "    array3 = []\n",
    "    for line in f: # read rest of lines\n",
    "        array3.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ep_len_4_3_3_plain_dqn.txt') as f:\n",
    "    array4 = []\n",
    "    for line in f: # read rest of lines\n",
    "        array4.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a257b0940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotting.plot_multi_test(smoothing_window=30,\n",
    "                             x_label=\"episode\",\n",
    "                             y_label=\"smoothed rewards\",\n",
    "                             curve_to_draw=[array3,\n",
    "                                            array4],\n",
    "                             labels=[\"options\", \"dqn\"]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
