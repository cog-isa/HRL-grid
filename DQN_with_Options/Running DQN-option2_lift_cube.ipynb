{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "import plotting\n",
    "\n",
    "import dqn\n",
    "from dqn_utils import *\n",
    "#from atari_wrappers import *\n",
    "#from environments.arm_env.arm_env import ArmEnv\n",
    "from arm_env_dqn_lift_cube import ArmEnvDQN_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def arm_model(img_in, num_actions, scope, reuse=False):\n",
    "    # as described in https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = img_in\n",
    "        with tf.variable_scope(\"convnet\"):\n",
    "            # original architecture\n",
    "            out = layers.convolution2d(out, num_outputs=32, kernel_size=8, stride=4, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=4, stride=2, activation_fn=tf.nn.relu)\n",
    "            out = layers.convolution2d(out, num_outputs=64, kernel_size=3, stride=1, activation_fn=tf.nn.relu)\n",
    "        out = layers.flatten(out)\n",
    "        with tf.variable_scope(\"action_value\"):\n",
    "            out = layers.fully_connected(out, num_outputs=256,         activation_fn=tf.nn.relu)\n",
    "            out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def arm_learn(env, session, num_timesteps):\n",
    "    # This is just a rough estimate\n",
    "    num_iterations = float(num_timesteps) / 4.0\n",
    "\n",
    "    lr_multiplier = 1.0\n",
    "    lr_schedule = PiecewiseSchedule([\n",
    "                                         (0,                   1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 10, 1e-4 * lr_multiplier),\n",
    "                                         (num_iterations / 2,  5e-5 * lr_multiplier),\n",
    "                                    ],\n",
    "                                    outside_value=5e-5 * lr_multiplier)\n",
    "    optimizer = dqn.OptimizerSpec(\n",
    "        constructor=tf.train.AdamOptimizer,\n",
    "        kwargs=dict(epsilon=1e-4),\n",
    "        lr_schedule=lr_schedule\n",
    "    )\n",
    "\n",
    "    def stopping_criterion(env, t):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return t >= num_timesteps\n",
    "\n",
    "    exploration_schedule = PiecewiseSchedule(\n",
    "        [\n",
    "            (0, 1.0),\n",
    "            (8e3, 0.3),\n",
    "            (num_iterations, 0.01),\n",
    "        ], outside_value=0.01\n",
    "    )\n",
    "\n",
    "    dqn.learn(\n",
    "        env,\n",
    "        q_func=arm_model,\n",
    "        optimizer_spec=optimizer,\n",
    "        session=session,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=1000000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        learning_starts=5000,\n",
    "        learning_freq=1,\n",
    "        frame_history_len=1,\n",
    "        target_update_freq=200,\n",
    "        grad_norm_clipping=10\n",
    "    )\n",
    "    \n",
    "    ep_rew = env.get_episode_rewards()\n",
    "    ep_len = env.get_episode_lengths()\n",
    "    env.close()\n",
    "    return ep_rew, ep_len\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.physical_device_desc for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def set_global_seeds(i):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        pass\n",
    "    else:\n",
    "        tf.set_random_seed(i) \n",
    "    np.random.seed(i)\n",
    "    random.seed(i)\n",
    "\n",
    "def get_session():\n",
    "    tf.reset_default_graph()\n",
    "#     tf_config = tf.ConfigProto(\n",
    "#         inter_op_parallelism_threads=1,\n",
    "#         intra_op_parallelism_threads=1)\n",
    "    session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    print(\"AVAILABLE GPUS: \", get_available_gpus())\n",
    "    session = tf.Session()\n",
    "    return session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVAILABLE GPUS:  []\n",
      "Timestep 5500\n",
      "mean reward (50 episodes) -383.320000\n",
      "mean length (50 episodes) 94.120000\n",
      "max_episode_reward (50 episodes) 37.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -530.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -383.320000\n",
      "episodes 57\n",
      "exploration 0.518750\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 6000\n",
      "mean reward (50 episodes) -362.480000\n",
      "mean length (50 episodes) 93.880000\n",
      "max_episode_reward (50 episodes) 37.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -530.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -362.480000\n",
      "episodes 63\n",
      "exploration 0.475000\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 6500\n",
      "mean reward (50 episodes) -347.080000\n",
      "mean length (50 episodes) 93.880000\n",
      "max_episode_reward (50 episodes) 37.000000\n",
      "min_episode_length (50 episodes) 23.000000\n",
      "min_episode_reward (50 episodes) -530.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -347.080000\n",
      "episodes 68\n",
      "exploration 0.431250\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 7000\n",
      "mean reward (50 episodes) -328.740000\n",
      "mean length (50 episodes) 94.340000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -530.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -328.740000\n",
      "episodes 74\n",
      "exploration 0.387500\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 7500\n",
      "mean reward (50 episodes) -307.600000\n",
      "mean length (50 episodes) 94.200000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -530.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -307.600000\n",
      "episodes 79\n",
      "exploration 0.343750\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 8000\n",
      "mean reward (50 episodes) -262.800000\n",
      "mean length (50 episodes) 92.000000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -530.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -262.800000\n",
      "episodes 86\n",
      "exploration 0.300000\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 8500\n",
      "mean reward (50 episodes) -237.420000\n",
      "mean length (50 episodes) 91.420000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -530.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -237.420000\n",
      "episodes 91\n",
      "exploration 0.298424\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 9000\n",
      "mean reward (50 episodes) -213.200000\n",
      "mean length (50 episodes) 91.800000\n",
      "max_episode_reward (50 episodes) 34.000000\n",
      "min_episode_length (50 episodes) 26.000000\n",
      "min_episode_reward (50 episodes) -440.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -213.200000\n",
      "episodes 96\n",
      "exploration 0.296848\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 9500\n",
      "mean reward (50 episodes) -169.600000\n",
      "mean length (50 episodes) 88.000000\n",
      "max_episode_reward (50 episodes) 79.000000\n",
      "min_episode_length (50 episodes) 11.000000\n",
      "min_episode_reward (50 episodes) -360.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -169.600000\n",
      "episodes 103\n",
      "exploration 0.295272\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 10000\n",
      "mean reward (50 episodes) -160.400000\n",
      "mean length (50 episodes) 88.200000\n",
      "max_episode_reward (50 episodes) 79.000000\n",
      "min_episode_length (50 episodes) 11.000000\n",
      "min_episode_reward (50 episodes) -280.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -160.400000\n",
      "episodes 108\n",
      "exploration 0.293696\n",
      "learning_rate 0.000100\n",
      "\n",
      "\n",
      "Timestep 10500\n",
      "mean reward (50 episodes) -156.680000\n",
      "mean length (50 episodes) 89.680000\n",
      "max_episode_reward (50 episodes) 79.000000\n",
      "min_episode_length (50 episodes) 11.000000\n",
      "min_episode_reward (50 episodes) -280.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -156.680000\n",
      "episodes 113\n",
      "exploration 0.292120\n",
      "learning_rate 0.000099\n",
      "\n",
      "\n",
      "Timestep 11000\n",
      "mean reward (50 episodes) -128.840000\n",
      "mean length (50 episodes) 84.440000\n",
      "max_episode_reward (50 episodes) 90.000000\n",
      "min_episode_length (50 episodes) 10.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -128.840000\n",
      "episodes 122\n",
      "exploration 0.290543\n",
      "learning_rate 0.000099\n",
      "\n",
      "\n",
      "Timestep 11500\n",
      "mean reward (50 episodes) -101.580000\n",
      "mean length (50 episodes) 77.780000\n",
      "max_episode_reward (50 episodes) 90.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -240.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -100.840000\n",
      "episodes 131\n",
      "exploration 0.288967\n",
      "learning_rate 0.000098\n",
      "\n",
      "\n",
      "Timestep 12000\n",
      "mean reward (50 episodes) -89.920000\n",
      "mean length (50 episodes) 74.320000\n",
      "max_episode_reward (50 episodes) 90.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -220.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -89.920000\n",
      "episodes 139\n",
      "exploration 0.287391\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 12500\n",
      "mean reward (50 episodes) -72.380000\n",
      "mean length (50 episodes) 69.580000\n",
      "max_episode_reward (50 episodes) 90.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -72.380000\n",
      "episodes 146\n",
      "exploration 0.285815\n",
      "learning_rate 0.000097\n",
      "\n",
      "\n",
      "Timestep 13000\n",
      "mean reward (50 episodes) -71.900000\n",
      "mean length (50 episodes) 69.500000\n",
      "max_episode_reward (50 episodes) 90.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -67.460000\n",
      "episodes 153\n",
      "exploration 0.284239\n",
      "learning_rate 0.000096\n",
      "\n",
      "\n",
      "Timestep 13500\n",
      "mean reward (50 episodes) -53.740000\n",
      "mean length (50 episodes) 63.140000\n",
      "max_episode_reward (50 episodes) 90.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -220.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -53.740000\n",
      "episodes 162\n",
      "exploration 0.282663\n",
      "learning_rate 0.000096\n",
      "\n",
      "\n",
      "Timestep 14000\n",
      "mean reward (50 episodes) -56.540000\n",
      "mean length (50 episodes) 63.140000\n",
      "max_episode_reward (50 episodes) 90.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -260.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -53.740000\n",
      "episodes 167\n",
      "exploration 0.281087\n",
      "learning_rate 0.000095\n",
      "\n",
      "\n",
      "Timestep 14500\n",
      "mean reward (50 episodes) -62.720000\n",
      "mean length (50 episodes) 65.520000\n",
      "max_episode_reward (50 episodes) 74.000000\n",
      "min_episode_length (50 episodes) 15.000000\n",
      "min_episode_reward (50 episodes) -260.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -53.740000\n",
      "episodes 175\n",
      "exploration 0.279511\n",
      "learning_rate 0.000094\n",
      "\n",
      "\n",
      "Timestep 15000\n",
      "mean reward (50 episodes) -58.980000\n",
      "mean length (50 episodes) 65.180000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -260.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -53.740000\n",
      "episodes 185\n",
      "exploration 0.277935\n",
      "learning_rate 0.000094\n",
      "\n",
      "\n",
      "Timestep 15500\n",
      "mean reward (50 episodes) -38.280000\n",
      "mean length (50 episodes) 58.080000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -260.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -37.980000\n",
      "episodes 198\n",
      "exploration 0.276359\n",
      "learning_rate 0.000093\n",
      "\n",
      "\n",
      "Timestep 16000\n",
      "mean reward (50 episodes) -36.460000\n",
      "mean length (50 episodes) 57.260000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -260.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -33.020000\n",
      "episodes 205\n",
      "exploration 0.274783\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 16500\n",
      "mean reward (50 episodes) -35.080000\n",
      "mean length (50 episodes) 57.880000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -260.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -28.960000\n",
      "episodes 213\n",
      "exploration 0.273207\n",
      "learning_rate 0.000092\n",
      "\n",
      "\n",
      "Timestep 17000\n",
      "mean reward (50 episodes) -28.040000\n",
      "mean length (50 episodes) 55.840000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -26.580000\n",
      "episodes 220\n",
      "exploration 0.271630\n",
      "learning_rate 0.000091\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 17500\n",
      "mean reward (50 episodes) -25.820000\n",
      "mean length (50 episodes) 56.020000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -21.640000\n",
      "episodes 229\n",
      "exploration 0.270054\n",
      "learning_rate 0.000091\n",
      "\n",
      "\n",
      "Timestep 18000\n",
      "mean reward (50 episodes) -28.460000\n",
      "mean length (50 episodes) 56.660000\n",
      "max_episode_reward (50 episodes) 81.000000\n",
      "min_episode_length (50 episodes) 9.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -19.920000\n",
      "episodes 239\n",
      "exploration 0.268478\n",
      "learning_rate 0.000090\n",
      "\n",
      "\n",
      "Timestep 18500\n",
      "mean reward (50 episodes) -45.500000\n",
      "mean length (50 episodes) 62.100000\n",
      "max_episode_reward (50 episodes) 78.000000\n",
      "min_episode_length (50 episodes) 12.000000\n",
      "min_episode_reward (50 episodes) -240.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -19.920000\n",
      "episodes 246\n",
      "exploration 0.266902\n",
      "learning_rate 0.000089\n",
      "\n",
      "\n",
      "Timestep 19000\n",
      "mean reward (50 episodes) -31.260000\n",
      "mean length (50 episodes) 55.860000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 9.000000\n",
      "min_episode_reward (50 episodes) -240.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -19.920000\n",
      "episodes 260\n",
      "exploration 0.265326\n",
      "learning_rate 0.000089\n",
      "\n",
      "\n",
      "Timestep 19500\n",
      "mean reward (50 episodes) -7.100000\n",
      "mean length (50 episodes) 46.300000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -7.100000\n",
      "episodes 273\n",
      "exploration 0.263750\n",
      "learning_rate 0.000088\n",
      "\n",
      "\n",
      "Timestep 20000\n",
      "mean reward (50 episodes) -15.600000\n",
      "mean length (50 episodes) 49.200000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -7.100000\n",
      "episodes 281\n",
      "exploration 0.262174\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 20500\n",
      "mean reward (50 episodes) -11.020000\n",
      "mean length (50 episodes) 47.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward -7.100000\n",
      "episodes 290\n",
      "exploration 0.260598\n",
      "learning_rate 0.000087\n",
      "\n",
      "\n",
      "Timestep 21000\n",
      "mean reward (50 episodes) -11.680000\n",
      "mean length (50 episodes) 47.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 0.760000\n",
      "episodes 300\n",
      "exploration 0.259022\n",
      "learning_rate 0.000086\n",
      "\n",
      "\n",
      "Timestep 21500\n",
      "mean reward (50 episodes) -5.760000\n",
      "mean length (50 episodes) 46.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 0.760000\n",
      "episodes 311\n",
      "exploration 0.257446\n",
      "learning_rate 0.000086\n",
      "\n",
      "\n",
      "Timestep 22000\n",
      "mean reward (50 episodes) 2.340000\n",
      "mean length (50 episodes) 44.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 2.340000\n",
      "episodes 325\n",
      "exploration 0.255870\n",
      "learning_rate 0.000085\n",
      "\n",
      "\n",
      "Timestep 22500\n",
      "mean reward (50 episodes) -1.200000\n",
      "mean length (50 episodes) 46.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 5.440000\n",
      "episodes 335\n",
      "exploration 0.254293\n",
      "learning_rate 0.000084\n",
      "\n",
      "\n",
      "Timestep 23000\n",
      "mean reward (50 episodes) 21.620000\n",
      "mean length (50 episodes) 38.580000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 25.500000\n",
      "episodes 352\n",
      "exploration 0.252717\n",
      "learning_rate 0.000084\n",
      "\n",
      "\n",
      "Timestep 23500\n",
      "mean reward (50 episodes) 24.860000\n",
      "mean length (50 episodes) 38.140000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 26.440000\n",
      "episodes 365\n",
      "exploration 0.251141\n",
      "learning_rate 0.000083\n",
      "\n",
      "\n",
      "Timestep 24000\n",
      "mean reward (50 episodes) 40.900000\n",
      "mean length (50 episodes) 30.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 41.700000\n",
      "episodes 385\n",
      "exploration 0.249565\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 24500\n",
      "mean reward (50 episodes) 41.660000\n",
      "mean length (50 episodes) 30.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 41.700000\n",
      "episodes 401\n",
      "exploration 0.247989\n",
      "learning_rate 0.000082\n",
      "\n",
      "\n",
      "Timestep 25000\n",
      "mean reward (50 episodes) 34.620000\n",
      "mean length (50 episodes) 30.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 42.240000\n",
      "episodes 412\n",
      "exploration 0.246413\n",
      "learning_rate 0.000081\n",
      "\n",
      "\n",
      "Timestep 25500\n",
      "mean reward (50 episodes) 24.720000\n",
      "mean length (50 episodes) 33.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 42.240000\n",
      "episodes 428\n",
      "exploration 0.244837\n",
      "learning_rate 0.000081\n",
      "\n",
      "\n",
      "Timestep 26000\n",
      "mean reward (50 episodes) 22.180000\n",
      "mean length (50 episodes) 34.020000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 42.240000\n",
      "episodes 445\n",
      "exploration 0.243261\n",
      "learning_rate 0.000080\n",
      "\n",
      "\n",
      "Timestep 26500\n",
      "mean reward (50 episodes) 20.160000\n",
      "mean length (50 episodes) 35.440000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -250.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 42.240000\n",
      "episodes 459\n",
      "exploration 0.241685\n",
      "learning_rate 0.000079\n",
      "\n",
      "\n",
      "Timestep 27000\n",
      "mean reward (50 episodes) 23.520000\n",
      "mean length (50 episodes) 36.080000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -220.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 42.240000\n",
      "episodes 471\n",
      "exploration 0.240109\n",
      "learning_rate 0.000079\n",
      "\n",
      "\n",
      "Timestep 27500\n",
      "mean reward (50 episodes) 34.400000\n",
      "mean length (50 episodes) 34.600000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 42.240000\n",
      "episodes 485\n",
      "exploration 0.238533\n",
      "learning_rate 0.000078\n",
      "\n",
      "\n",
      "Timestep 28000\n",
      "mean reward (50 episodes) 27.260000\n",
      "mean length (50 episodes) 37.940000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 42.240000\n",
      "episodes 499\n",
      "exploration 0.236957\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 28500\n",
      "mean reward (50 episodes) 46.080000\n",
      "mean length (50 episodes) 31.520000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 518\n",
      "exploration 0.235380\n",
      "learning_rate 0.000077\n",
      "\n",
      "\n",
      "Timestep 29000\n",
      "mean reward (50 episodes) 39.760000\n",
      "mean length (50 episodes) 32.040000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 533\n",
      "exploration 0.233804\n",
      "learning_rate 0.000076\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 29500\n",
      "mean reward (50 episodes) 34.080000\n",
      "mean length (50 episodes) 32.920000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 546\n",
      "exploration 0.232228\n",
      "learning_rate 0.000076\n",
      "\n",
      "\n",
      "Timestep 30000\n",
      "mean reward (50 episodes) 31.560000\n",
      "mean length (50 episodes) 33.240000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 559\n",
      "exploration 0.230652\n",
      "learning_rate 0.000075\n",
      "\n",
      "\n",
      "Timestep 30500\n",
      "mean reward (50 episodes) 25.000000\n",
      "mean length (50 episodes) 35.400000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 573\n",
      "exploration 0.229076\n",
      "learning_rate 0.000074\n",
      "\n",
      "\n",
      "Timestep 31000\n",
      "mean reward (50 episodes) 20.120000\n",
      "mean length (50 episodes) 36.480000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 589\n",
      "exploration 0.227500\n",
      "learning_rate 0.000074\n",
      "\n",
      "\n",
      "Timestep 31500\n",
      "mean reward (50 episodes) 24.020000\n",
      "mean length (50 episodes) 37.380000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 600\n",
      "exploration 0.225924\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 32000\n",
      "mean reward (50 episodes) 42.720000\n",
      "mean length (50 episodes) 30.280000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 46.300000\n",
      "episodes 623\n",
      "exploration 0.224348\n",
      "learning_rate 0.000073\n",
      "\n",
      "\n",
      "Timestep 32500\n",
      "mean reward (50 episodes) 55.020000\n",
      "mean length (50 episodes) 26.380000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -41.000000\n",
      "max_episode_length (50 episodes) 91.000000\n",
      "best mean reward 56.340000\n",
      "episodes 644\n",
      "exploration 0.222772\n",
      "learning_rate 0.000072\n",
      "\n",
      "\n",
      "Timestep 33000\n",
      "mean reward (50 episodes) 50.940000\n",
      "mean length (50 episodes) 26.060000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 659\n",
      "exploration 0.221196\n",
      "learning_rate 0.000071\n",
      "\n",
      "\n",
      "Timestep 33500\n",
      "mean reward (50 episodes) 43.480000\n",
      "mean length (50 episodes) 28.720000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 675\n",
      "exploration 0.219620\n",
      "learning_rate 0.000071\n",
      "\n",
      "\n",
      "Timestep 34000\n",
      "mean reward (50 episodes) 39.340000\n",
      "mean length (50 episodes) 30.060000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 693\n",
      "exploration 0.218043\n",
      "learning_rate 0.000070\n",
      "\n",
      "\n",
      "Timestep 34500\n",
      "mean reward (50 episodes) 45.780000\n",
      "mean length (50 episodes) 28.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 710\n",
      "exploration 0.216467\n",
      "learning_rate 0.000069\n",
      "\n",
      "\n",
      "Timestep 35000\n",
      "mean reward (50 episodes) 43.060000\n",
      "mean length (50 episodes) 30.540000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 724\n",
      "exploration 0.214891\n",
      "learning_rate 0.000069\n",
      "\n",
      "\n",
      "Timestep 35500\n",
      "mean reward (50 episodes) 41.560000\n",
      "mean length (50 episodes) 32.040000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 739\n",
      "exploration 0.213315\n",
      "learning_rate 0.000068\n",
      "\n",
      "\n",
      "Timestep 36000\n",
      "mean reward (50 episodes) 37.420000\n",
      "mean length (50 episodes) 34.580000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 753\n",
      "exploration 0.211739\n",
      "learning_rate 0.000068\n",
      "\n",
      "\n",
      "Timestep 36500\n",
      "mean reward (50 episodes) 41.680000\n",
      "mean length (50 episodes) 32.520000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 769\n",
      "exploration 0.210163\n",
      "learning_rate 0.000067\n",
      "\n",
      "\n",
      "Timestep 37000\n",
      "mean reward (50 episodes) 34.000000\n",
      "mean length (50 episodes) 35.000000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 779\n",
      "exploration 0.208587\n",
      "learning_rate 0.000066\n",
      "\n",
      "\n",
      "Timestep 37500\n",
      "mean reward (50 episodes) 25.460000\n",
      "mean length (50 episodes) 36.940000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 793\n",
      "exploration 0.207011\n",
      "learning_rate 0.000066\n",
      "\n",
      "\n",
      "Timestep 38000\n",
      "mean reward (50 episodes) 17.820000\n",
      "mean length (50 episodes) 38.780000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 803\n",
      "exploration 0.205435\n",
      "learning_rate 0.000065\n",
      "\n",
      "\n",
      "Timestep 38500\n",
      "mean reward (50 episodes) -0.160000\n",
      "mean length (50 episodes) 46.560000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 9.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 812\n",
      "exploration 0.203859\n",
      "learning_rate 0.000064\n",
      "\n",
      "\n",
      "Timestep 39000\n",
      "mean reward (50 episodes) -4.000000\n",
      "mean length (50 episodes) 48.400000\n",
      "max_episode_reward (50 episodes) 91.000000\n",
      "min_episode_length (50 episodes) 9.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 821\n",
      "exploration 0.202283\n",
      "learning_rate 0.000064\n",
      "\n",
      "\n",
      "Timestep 39500\n",
      "mean reward (50 episodes) -6.580000\n",
      "mean length (50 episodes) 49.780000\n",
      "max_episode_reward (50 episodes) 89.000000\n",
      "min_episode_length (50 episodes) 10.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 829\n",
      "exploration 0.200707\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 40000\n",
      "mean reward (50 episodes) -13.100000\n",
      "mean length (50 episodes) 53.100000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 838\n",
      "exploration 0.199130\n",
      "learning_rate 0.000063\n",
      "\n",
      "\n",
      "Timestep 40500\n",
      "mean reward (50 episodes) -18.040000\n",
      "mean length (50 episodes) 54.840000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 849\n",
      "exploration 0.197554\n",
      "learning_rate 0.000062\n",
      "\n",
      "\n",
      "Timestep 41000\n",
      "mean reward (50 episodes) -9.280000\n",
      "mean length (50 episodes) 50.880000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 858\n",
      "exploration 0.195978\n",
      "learning_rate 0.000061\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 41500\n",
      "mean reward (50 episodes) -12.980000\n",
      "mean length (50 episodes) 51.780000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 869\n",
      "exploration 0.194402\n",
      "learning_rate 0.000061\n",
      "\n",
      "\n",
      "Timestep 42000\n",
      "mean reward (50 episodes) -0.480000\n",
      "mean length (50 episodes) 47.280000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 883\n",
      "exploration 0.192826\n",
      "learning_rate 0.000060\n",
      "\n",
      "\n",
      "Timestep 42500\n",
      "mean reward (50 episodes) -4.560000\n",
      "mean length (50 episodes) 47.760000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 890\n",
      "exploration 0.191250\n",
      "learning_rate 0.000059\n",
      "\n",
      "\n",
      "Timestep 43000\n",
      "mean reward (50 episodes) -10.300000\n",
      "mean length (50 episodes) 52.100000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 897\n",
      "exploration 0.189674\n",
      "learning_rate 0.000059\n",
      "\n",
      "\n",
      "Timestep 43500\n",
      "mean reward (50 episodes) -3.220000\n",
      "mean length (50 episodes) 48.620000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 910\n",
      "exploration 0.188098\n",
      "learning_rate 0.000058\n",
      "\n",
      "\n",
      "Timestep 44000\n",
      "mean reward (50 episodes) -12.180000\n",
      "mean length (50 episodes) 52.780000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 916\n",
      "exploration 0.186522\n",
      "learning_rate 0.000058\n",
      "\n",
      "\n",
      "Timestep 44500\n",
      "mean reward (50 episodes) -9.720000\n",
      "mean length (50 episodes) 51.120000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 930\n",
      "exploration 0.184946\n",
      "learning_rate 0.000057\n",
      "\n",
      "\n",
      "Timestep 45000\n",
      "mean reward (50 episodes) -10.560000\n",
      "mean length (50 episodes) 50.960000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 940\n",
      "exploration 0.183370\n",
      "learning_rate 0.000056\n",
      "\n",
      "\n",
      "Timestep 45500\n",
      "mean reward (50 episodes) -13.540000\n",
      "mean length (50 episodes) 50.340000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 947\n",
      "exploration 0.181793\n",
      "learning_rate 0.000056\n",
      "\n",
      "\n",
      "Timestep 46000\n",
      "mean reward (50 episodes) -16.320000\n",
      "mean length (50 episodes) 51.720000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 954\n",
      "exploration 0.180217\n",
      "learning_rate 0.000055\n",
      "\n",
      "\n",
      "Timestep 46500\n",
      "mean reward (50 episodes) -26.700000\n",
      "mean length (50 episodes) 55.700000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 963\n",
      "exploration 0.178641\n",
      "learning_rate 0.000054\n",
      "\n",
      "\n",
      "Timestep 47000\n",
      "mean reward (50 episodes) -22.400000\n",
      "mean length (50 episodes) 54.800000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 971\n",
      "exploration 0.177065\n",
      "learning_rate 0.000054\n",
      "\n",
      "\n",
      "Timestep 47500\n",
      "mean reward (50 episodes) -33.320000\n",
      "mean length (50 episodes) 58.720000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 978\n",
      "exploration 0.175489\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 48000\n",
      "mean reward (50 episodes) -49.000000\n",
      "mean length (50 episodes) 65.400000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -210.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 985\n",
      "exploration 0.173913\n",
      "learning_rate 0.000053\n",
      "\n",
      "\n",
      "Timestep 48500\n",
      "mean reward (50 episodes) -37.860000\n",
      "mean length (50 episodes) 62.660000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 994\n",
      "exploration 0.172337\n",
      "learning_rate 0.000052\n",
      "\n",
      "\n",
      "Timestep 49000\n",
      "mean reward (50 episodes) -23.880000\n",
      "mean length (50 episodes) 59.880000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1004\n",
      "exploration 0.170761\n",
      "learning_rate 0.000051\n",
      "\n",
      "\n",
      "Timestep 49500\n",
      "mean reward (50 episodes) -26.920000\n",
      "mean length (50 episodes) 60.120000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1013\n",
      "exploration 0.169185\n",
      "learning_rate 0.000051\n",
      "\n",
      "\n",
      "Timestep 50000\n",
      "mean reward (50 episodes) -19.000000\n",
      "mean length (50 episodes) 56.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1023\n",
      "exploration 0.167609\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 50500\n",
      "mean reward (50 episodes) -0.540000\n",
      "mean length (50 episodes) 50.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1034\n",
      "exploration 0.166033\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 51000\n",
      "mean reward (50 episodes) -4.740000\n",
      "mean length (50 episodes) 52.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1040\n",
      "exploration 0.164457\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 51500\n",
      "mean reward (50 episodes) -8.000000\n",
      "mean length (50 episodes) 52.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1053\n",
      "exploration 0.162880\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 52000\n",
      "mean reward (50 episodes) 3.140000\n",
      "mean length (50 episodes) 48.460000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1065\n",
      "exploration 0.161304\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 52500\n",
      "mean reward (50 episodes) 1.180000\n",
      "mean length (50 episodes) 48.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1077\n",
      "exploration 0.159728\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 53000\n",
      "mean reward (50 episodes) 24.020000\n",
      "mean length (50 episodes) 38.180000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1092\n",
      "exploration 0.158152\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 53500\n",
      "mean reward (50 episodes) 10.560000\n",
      "mean length (50 episodes) 42.840000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1097\n",
      "exploration 0.156576\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 54000\n",
      "mean reward (50 episodes) 1.900000\n",
      "mean length (50 episodes) 45.700000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1108\n",
      "exploration 0.155000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 54500\n",
      "mean reward (50 episodes) 0.840000\n",
      "mean length (50 episodes) 46.560000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1117\n",
      "exploration 0.153424\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 55000\n",
      "mean reward (50 episodes) -6.160000\n",
      "mean length (50 episodes) 50.560000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1125\n",
      "exploration 0.151848\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 55500\n",
      "mean reward (50 episodes) -21.700000\n",
      "mean length (50 episodes) 54.900000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1133\n",
      "exploration 0.150272\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 56000\n",
      "mean reward (50 episodes) -6.500000\n",
      "mean length (50 episodes) 50.100000\n",
      "max_episode_reward (50 episodes) 92.000000\n",
      "min_episode_length (50 episodes) 8.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1147\n",
      "exploration 0.148696\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 56500\n",
      "mean reward (50 episodes) -8.280000\n",
      "mean length (50 episodes) 51.480000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1155\n",
      "exploration 0.147120\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 57000\n",
      "mean reward (50 episodes) 4.660000\n",
      "mean length (50 episodes) 46.740000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1170\n",
      "exploration 0.145543\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 57500\n",
      "mean reward (50 episodes) 8.120000\n",
      "mean length (50 episodes) 44.680000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1179\n",
      "exploration 0.143967\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 58000\n",
      "mean reward (50 episodes) 14.240000\n",
      "mean length (50 episodes) 43.760000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1190\n",
      "exploration 0.142391\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 58500\n",
      "mean reward (50 episodes) 9.440000\n",
      "mean length (50 episodes) 45.360000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1200\n",
      "exploration 0.140815\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 59000\n",
      "mean reward (50 episodes) 10.120000\n",
      "mean length (50 episodes) 42.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1214\n",
      "exploration 0.139239\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 59500\n",
      "mean reward (50 episodes) 3.660000\n",
      "mean length (50 episodes) 45.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1223\n",
      "exploration 0.137663\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 60000\n",
      "mean reward (50 episodes) 8.720000\n",
      "mean length (50 episodes) 43.480000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1236\n",
      "exploration 0.136087\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 60500\n",
      "mean reward (50 episodes) 23.360000\n",
      "mean length (50 episodes) 36.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1254\n",
      "exploration 0.134511\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 61000\n",
      "mean reward (50 episodes) 56.000000\n",
      "mean length (50 episodes) 24.400000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.340000\n",
      "episodes 1277\n",
      "exploration 0.132935\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 61500\n",
      "mean reward (50 episodes) 50.300000\n",
      "mean length (50 episodes) 26.700000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1292\n",
      "exploration 0.131359\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 62000\n",
      "mean reward (50 episodes) 43.320000\n",
      "mean length (50 episodes) 31.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1304\n",
      "exploration 0.129783\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 62500\n",
      "mean reward (50 episodes) 45.360000\n",
      "mean length (50 episodes) 30.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1322\n",
      "exploration 0.128207\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63000\n",
      "mean reward (50 episodes) 36.460000\n",
      "mean length (50 episodes) 34.540000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1333\n",
      "exploration 0.126630\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 63500\n",
      "mean reward (50 episodes) 38.900000\n",
      "mean length (50 episodes) 33.500000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1349\n",
      "exploration 0.125054\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64000\n",
      "mean reward (50 episodes) 36.500000\n",
      "mean length (50 episodes) 35.300000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1361\n",
      "exploration 0.123478\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 64500\n",
      "mean reward (50 episodes) 37.340000\n",
      "mean length (50 episodes) 35.460000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1377\n",
      "exploration 0.121902\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 65000\n",
      "mean reward (50 episodes) 33.480000\n",
      "mean length (50 episodes) 37.120000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1385\n",
      "exploration 0.120326\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 65500\n",
      "mean reward (50 episodes) 21.760000\n",
      "mean length (50 episodes) 42.640000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1398\n",
      "exploration 0.118750\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66000\n",
      "mean reward (50 episodes) 24.340000\n",
      "mean length (50 episodes) 41.660000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1408\n",
      "exploration 0.117174\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 66500\n",
      "mean reward (50 episodes) 26.980000\n",
      "mean length (50 episodes) 40.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1424\n",
      "exploration 0.115598\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67000\n",
      "mean reward (50 episodes) 20.120000\n",
      "mean length (50 episodes) 43.280000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1433\n",
      "exploration 0.114022\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 67500\n",
      "mean reward (50 episodes) 30.900000\n",
      "mean length (50 episodes) 38.700000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1447\n",
      "exploration 0.112446\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68000\n",
      "mean reward (50 episodes) 27.020000\n",
      "mean length (50 episodes) 38.980000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1459\n",
      "exploration 0.110870\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 68500\n",
      "mean reward (50 episodes) 32.740000\n",
      "mean length (50 episodes) 36.460000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1476\n",
      "exploration 0.109293\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69000\n",
      "mean reward (50 episodes) 37.040000\n",
      "mean length (50 episodes) 33.960000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1490\n",
      "exploration 0.107717\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 69500\n",
      "mean reward (50 episodes) 35.120000\n",
      "mean length (50 episodes) 34.480000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1504\n",
      "exploration 0.106141\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70000\n",
      "mean reward (50 episodes) 35.000000\n",
      "mean length (50 episodes) 36.000000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1514\n",
      "exploration 0.104565\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 70500\n",
      "mean reward (50 episodes) 33.040000\n",
      "mean length (50 episodes) 38.760000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1528\n",
      "exploration 0.102989\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71000\n",
      "mean reward (50 episodes) 20.460000\n",
      "mean length (50 episodes) 44.340000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1534\n",
      "exploration 0.101413\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 71500\n",
      "mean reward (50 episodes) 12.860000\n",
      "mean length (50 episodes) 45.540000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1545\n",
      "exploration 0.099837\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72000\n",
      "mean reward (50 episodes) 23.780000\n",
      "mean length (50 episodes) 41.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1562\n",
      "exploration 0.098261\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 72500\n",
      "mean reward (50 episodes) 39.020000\n",
      "mean length (50 episodes) 32.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -200.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 56.880000\n",
      "episodes 1583\n",
      "exploration 0.096685\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73000\n",
      "mean reward (50 episodes) 59.400000\n",
      "mean length (50 episodes) 25.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1600\n",
      "exploration 0.095109\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 73500\n",
      "mean reward (50 episodes) 44.320000\n",
      "mean length (50 episodes) 30.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1606\n",
      "exploration 0.093533\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74000\n",
      "mean reward (50 episodes) 36.260000\n",
      "mean length (50 episodes) 34.140000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1625\n",
      "exploration 0.091957\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 74500\n",
      "mean reward (50 episodes) 32.120000\n",
      "mean length (50 episodes) 36.280000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1640\n",
      "exploration 0.090380\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75000\n",
      "mean reward (50 episodes) 53.900000\n",
      "mean length (50 episodes) 28.500000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1658\n",
      "exploration 0.088804\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 75500\n",
      "mean reward (50 episodes) 43.940000\n",
      "mean length (50 episodes) 32.860000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1671\n",
      "exploration 0.087228\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76000\n",
      "mean reward (50 episodes) 44.100000\n",
      "mean length (50 episodes) 32.900000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1682\n",
      "exploration 0.085652\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 76500\n",
      "mean reward (50 episodes) 32.580000\n",
      "mean length (50 episodes) 37.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1693\n",
      "exploration 0.084076\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 77000\n",
      "mean reward (50 episodes) 17.620000\n",
      "mean length (50 episodes) 42.380000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1700\n",
      "exploration 0.082500\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 77500\n",
      "mean reward (50 episodes) 7.240000\n",
      "mean length (50 episodes) 46.960000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1711\n",
      "exploration 0.080924\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78000\n",
      "mean reward (50 episodes) 0.140000\n",
      "mean length (50 episodes) 50.060000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1720\n",
      "exploration 0.079348\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 78500\n",
      "mean reward (50 episodes) -1.580000\n",
      "mean length (50 episodes) 51.180000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1731\n",
      "exploration 0.077772\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79000\n",
      "mean reward (50 episodes) -4.320000\n",
      "mean length (50 episodes) 53.320000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -170.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1740\n",
      "exploration 0.076196\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 79500\n",
      "mean reward (50 episodes) 12.240000\n",
      "mean length (50 episodes) 46.560000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1754\n",
      "exploration 0.074620\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80000\n",
      "mean reward (50 episodes) 18.140000\n",
      "mean length (50 episodes) 44.660000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1765\n",
      "exploration 0.073043\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 80500\n",
      "mean reward (50 episodes) 21.040000\n",
      "mean length (50 episodes) 43.160000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1777\n",
      "exploration 0.071467\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81000\n",
      "mean reward (50 episodes) 23.760000\n",
      "mean length (50 episodes) 39.240000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1793\n",
      "exploration 0.069891\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 81500\n",
      "mean reward (50 episodes) 22.920000\n",
      "mean length (50 episodes) 38.880000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1802\n",
      "exploration 0.068315\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82000\n",
      "mean reward (50 episodes) 21.040000\n",
      "mean length (50 episodes) 40.360000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1813\n",
      "exploration 0.066739\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 82500\n",
      "mean reward (50 episodes) 21.500000\n",
      "mean length (50 episodes) 39.700000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -190.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1829\n",
      "exploration 0.065163\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83000\n",
      "mean reward (50 episodes) 30.140000\n",
      "mean length (50 episodes) 37.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -180.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1844\n",
      "exploration 0.063587\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 83500\n",
      "mean reward (50 episodes) 21.120000\n",
      "mean length (50 episodes) 39.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -610.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1851\n",
      "exploration 0.062011\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84000\n",
      "mean reward (50 episodes) 20.360000\n",
      "mean length (50 episodes) 39.040000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -610.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1866\n",
      "exploration 0.060435\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 84500\n",
      "mean reward (50 episodes) 22.320000\n",
      "mean length (50 episodes) 39.480000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -610.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1878\n",
      "exploration 0.058859\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85000\n",
      "mean reward (50 episodes) 10.560000\n",
      "mean length (50 episodes) 44.040000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -610.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1885\n",
      "exploration 0.057283\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 85500\n",
      "mean reward (50 episodes) 19.980000\n",
      "mean length (50 episodes) 43.020000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1899\n",
      "exploration 0.055707\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86000\n",
      "mean reward (50 episodes) 19.500000\n",
      "mean length (50 episodes) 42.300000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -150.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1908\n",
      "exploration 0.054130\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 86500\n",
      "mean reward (50 episodes) -10.720000\n",
      "mean length (50 episodes) 46.120000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -1010.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1921\n",
      "exploration 0.052554\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87000\n",
      "mean reward (50 episodes) -18.100000\n",
      "mean length (50 episodes) 45.100000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -1010.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1930\n",
      "exploration 0.050978\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 87500\n",
      "mean reward (50 episodes) -16.840000\n",
      "mean length (50 episodes) 47.440000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -1010.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1938\n",
      "exploration 0.049402\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88000\n",
      "mean reward (50 episodes) -22.800000\n",
      "mean length (50 episodes) 49.200000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -1010.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1949\n",
      "exploration 0.047826\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 88500\n",
      "mean reward (50 episodes) -23.900000\n",
      "mean length (50 episodes) 50.300000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -1010.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1957\n",
      "exploration 0.046250\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 89000\n",
      "mean reward (50 episodes) -3.720000\n",
      "mean length (50 episodes) 50.120000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -378.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1967\n",
      "exploration 0.044674\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 89500\n",
      "mean reward (50 episodes) -16.240000\n",
      "mean length (50 episodes) 56.240000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -378.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1974\n",
      "exploration 0.043098\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90000\n",
      "mean reward (50 episodes) 3.100000\n",
      "mean length (50 episodes) 48.100000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 1989\n",
      "exploration 0.041522\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 90500\n",
      "mean reward (50 episodes) 1.820000\n",
      "mean length (50 episodes) 49.580000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2000\n",
      "exploration 0.039946\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91000\n",
      "mean reward (50 episodes) 9.680000\n",
      "mean length (50 episodes) 46.520000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2010\n",
      "exploration 0.038370\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 91500\n",
      "mean reward (50 episodes) 15.620000\n",
      "mean length (50 episodes) 42.780000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2022\n",
      "exploration 0.036793\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92000\n",
      "mean reward (50 episodes) 23.480000\n",
      "mean length (50 episodes) 39.720000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2038\n",
      "exploration 0.035217\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 92500\n",
      "mean reward (50 episodes) 24.080000\n",
      "mean length (50 episodes) 39.920000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2050\n",
      "exploration 0.033641\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93000\n",
      "mean reward (50 episodes) 39.980000\n",
      "mean length (50 episodes) 32.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2066\n",
      "exploration 0.032065\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 93500\n",
      "mean reward (50 episodes) 34.760000\n",
      "mean length (50 episodes) 34.640000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2081\n",
      "exploration 0.030489\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94000\n",
      "mean reward (50 episodes) 43.540000\n",
      "mean length (50 episodes) 29.060000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2101\n",
      "exploration 0.028913\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 94500\n",
      "mean reward (50 episodes) 37.600000\n",
      "mean length (50 episodes) 32.200000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2113\n",
      "exploration 0.027337\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95000\n",
      "mean reward (50 episodes) 29.860000\n",
      "mean length (50 episodes) 36.740000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2122\n",
      "exploration 0.025761\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 95500\n",
      "mean reward (50 episodes) 26.120000\n",
      "mean length (50 episodes) 39.880000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2131\n",
      "exploration 0.024185\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96000\n",
      "mean reward (50 episodes) 23.680000\n",
      "mean length (50 episodes) 41.320000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2142\n",
      "exploration 0.022609\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 96500\n",
      "mean reward (50 episodes) 27.180000\n",
      "mean length (50 episodes) 40.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2161\n",
      "exploration 0.021033\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97000\n",
      "mean reward (50 episodes) 35.560000\n",
      "mean length (50 episodes) 35.240000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2176\n",
      "exploration 0.019457\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 97500\n",
      "mean reward (50 episodes) 32.700000\n",
      "mean length (50 episodes) 35.900000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2186\n",
      "exploration 0.017880\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98000\n",
      "mean reward (50 episodes) 44.760000\n",
      "mean length (50 episodes) 29.440000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2208\n",
      "exploration 0.016304\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 98500\n",
      "mean reward (50 episodes) 40.780000\n",
      "mean length (50 episodes) 31.820000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2226\n",
      "exploration 0.014728\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99000\n",
      "mean reward (50 episodes) 54.040000\n",
      "mean length (50 episodes) 24.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2248\n",
      "exploration 0.013152\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 99500\n",
      "mean reward (50 episodes) 37.400000\n",
      "mean length (50 episodes) 33.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2256\n",
      "exploration 0.011576\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100000\n",
      "mean reward (50 episodes) 47.540000\n",
      "mean length (50 episodes) 28.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -160.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2278\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 100500\n",
      "mean reward (50 episodes) 61.520000\n",
      "mean length (50 episodes) 23.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2302\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 101000\n",
      "mean reward (50 episodes) 53.420000\n",
      "mean length (50 episodes) 25.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2315\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 101500\n",
      "mean reward (50 episodes) 42.160000\n",
      "mean length (50 episodes) 31.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2327\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102000\n",
      "mean reward (50 episodes) 29.540000\n",
      "mean length (50 episodes) 36.460000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2342\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 102500\n",
      "mean reward (50 episodes) 30.840000\n",
      "mean length (50 episodes) 34.360000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2360\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103000\n",
      "mean reward (50 episodes) 36.240000\n",
      "mean length (50 episodes) 31.360000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2374\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 103500\n",
      "mean reward (50 episodes) 34.360000\n",
      "mean length (50 episodes) 33.240000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2385\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104000\n",
      "mean reward (50 episodes) 35.140000\n",
      "mean length (50 episodes) 34.260000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2404\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 104500\n",
      "mean reward (50 episodes) 42.520000\n",
      "mean length (50 episodes) 30.480000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2423\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105000\n",
      "mean reward (50 episodes) 51.620000\n",
      "mean length (50 episodes) 25.180000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2437\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 105500\n",
      "mean reward (50 episodes) 44.340000\n",
      "mean length (50 episodes) 28.460000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2453\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106000\n",
      "mean reward (50 episodes) 42.620000\n",
      "mean length (50 episodes) 30.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2472\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 106500\n",
      "mean reward (50 episodes) 50.960000\n",
      "mean length (50 episodes) 27.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2488\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107000\n",
      "mean reward (50 episodes) 47.960000\n",
      "mean length (50 episodes) 28.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2506\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 107500\n",
      "mean reward (50 episodes) 52.300000\n",
      "mean length (50 episodes) 24.700000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2532\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108000\n",
      "mean reward (50 episodes) 46.200000\n",
      "mean length (50 episodes) 28.600000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2544\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 108500\n",
      "mean reward (50 episodes) 58.240000\n",
      "mean length (50 episodes) 22.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2570\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109000\n",
      "mean reward (50 episodes) 49.560000\n",
      "mean length (50 episodes) 27.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 63.600000\n",
      "episodes 2586\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 109500\n",
      "mean reward (50 episodes) 59.480000\n",
      "mean length (50 episodes) 22.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 64.360000\n",
      "episodes 2608\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110000\n",
      "mean reward (50 episodes) 50.500000\n",
      "mean length (50 episodes) 26.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 64.360000\n",
      "episodes 2623\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 110500\n",
      "mean reward (50 episodes) 43.540000\n",
      "mean length (50 episodes) 29.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 64.360000\n",
      "episodes 2638\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111000\n",
      "mean reward (50 episodes) 40.120000\n",
      "mean length (50 episodes) 29.480000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 64.360000\n",
      "episodes 2660\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 111500\n",
      "mean reward (50 episodes) 53.360000\n",
      "mean length (50 episodes) 24.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 64.360000\n",
      "episodes 2683\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112000\n",
      "mean reward (50 episodes) 61.400000\n",
      "mean length (50 episodes) 21.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 64.360000\n",
      "episodes 2703\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 112500\n",
      "mean reward (50 episodes) 66.760000\n",
      "mean length (50 episodes) 18.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 66.760000\n",
      "episodes 2734\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 113000\n",
      "mean reward (50 episodes) 62.300000\n",
      "mean length (50 episodes) 20.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2752\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 113500\n",
      "mean reward (50 episodes) 49.640000\n",
      "mean length (50 episodes) 25.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2767\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114000\n",
      "mean reward (50 episodes) 41.220000\n",
      "mean length (50 episodes) 28.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2782\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 114500\n",
      "mean reward (50 episodes) 39.200000\n",
      "mean length (50 episodes) 30.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2802\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115000\n",
      "mean reward (50 episodes) 53.680000\n",
      "mean length (50 episodes) 23.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2822\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 115500\n",
      "mean reward (50 episodes) 47.320000\n",
      "mean length (50 episodes) 29.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2835\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116000\n",
      "mean reward (50 episodes) 57.920000\n",
      "mean length (50 episodes) 24.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2861\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 116500\n",
      "mean reward (50 episodes) 58.460000\n",
      "mean length (50 episodes) 23.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2881\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117000\n",
      "mean reward (50 episodes) 56.500000\n",
      "mean length (50 episodes) 24.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2894\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 117500\n",
      "mean reward (50 episodes) 50.500000\n",
      "mean length (50 episodes) 27.500000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2915\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118000\n",
      "mean reward (50 episodes) 61.560000\n",
      "mean length (50 episodes) 21.640000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 71.260000\n",
      "episodes 2943\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 118500\n",
      "mean reward (50 episodes) 75.660000\n",
      "mean length (50 episodes) 15.140000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 2974\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119000\n",
      "mean reward (50 episodes) 70.280000\n",
      "mean length (50 episodes) 18.920000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 2992\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 119500\n",
      "mean reward (50 episodes) 64.940000\n",
      "mean length (50 episodes) 21.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3022\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120000\n",
      "mean reward (50 episodes) 61.720000\n",
      "mean length (50 episodes) 21.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3041\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 120500\n",
      "mean reward (50 episodes) 61.320000\n",
      "mean length (50 episodes) 20.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3067\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121000\n",
      "mean reward (50 episodes) 51.880000\n",
      "mean length (50 episodes) 24.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3078\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 121500\n",
      "mean reward (50 episodes) 53.160000\n",
      "mean length (50 episodes) 24.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3105\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122000\n",
      "mean reward (50 episodes) 54.100000\n",
      "mean length (50 episodes) 24.700000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3120\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 122500\n",
      "mean reward (50 episodes) 50.880000\n",
      "mean length (50 episodes) 26.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3135\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123000\n",
      "mean reward (50 episodes) 52.500000\n",
      "mean length (50 episodes) 24.700000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3168\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 123500\n",
      "mean reward (50 episodes) 50.260000\n",
      "mean length (50 episodes) 25.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3178\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124000\n",
      "mean reward (50 episodes) 51.540000\n",
      "mean length (50 episodes) 25.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3197\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 124500\n",
      "mean reward (50 episodes) 54.000000\n",
      "mean length (50 episodes) 26.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3222\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 125000\n",
      "mean reward (50 episodes) 65.180000\n",
      "mean length (50 episodes) 19.620000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3248\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 125500\n",
      "mean reward (50 episodes) 56.440000\n",
      "mean length (50 episodes) 24.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3260\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126000\n",
      "mean reward (50 episodes) 54.740000\n",
      "mean length (50 episodes) 24.260000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3285\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 126500\n",
      "mean reward (50 episodes) 62.880000\n",
      "mean length (50 episodes) 17.920000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3312\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127000\n",
      "mean reward (50 episodes) 61.940000\n",
      "mean length (50 episodes) 19.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3337\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 127500\n",
      "mean reward (50 episodes) 60.960000\n",
      "mean length (50 episodes) 20.640000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3355\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128000\n",
      "mean reward (50 episodes) 53.860000\n",
      "mean length (50 episodes) 24.940000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3377\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 128500\n",
      "mean reward (50 episodes) 71.180000\n",
      "mean length (50 episodes) 16.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3409\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129000\n",
      "mean reward (50 episodes) 57.520000\n",
      "mean length (50 episodes) 23.480000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3417\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 129500\n",
      "mean reward (50 episodes) 54.340000\n",
      "mean length (50 episodes) 23.860000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3450\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130000\n",
      "mean reward (50 episodes) 67.260000\n",
      "mean length (50 episodes) 18.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3471\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 130500\n",
      "mean reward (50 episodes) 58.340000\n",
      "mean length (50 episodes) 22.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3487\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131000\n",
      "mean reward (50 episodes) 60.860000\n",
      "mean length (50 episodes) 20.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3517\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 131500\n",
      "mean reward (50 episodes) 59.840000\n",
      "mean length (50 episodes) 21.560000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3536\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132000\n",
      "mean reward (50 episodes) 63.860000\n",
      "mean length (50 episodes) 21.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3560\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 132500\n",
      "mean reward (50 episodes) 65.340000\n",
      "mean length (50 episodes) 20.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3586\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133000\n",
      "mean reward (50 episodes) 68.880000\n",
      "mean length (50 episodes) 16.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3615\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 133500\n",
      "mean reward (50 episodes) 65.880000\n",
      "mean length (50 episodes) 18.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3639\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134000\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 13.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 76.220000\n",
      "episodes 3681\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 134500\n",
      "mean reward (50 episodes) 68.720000\n",
      "mean length (50 episodes) 16.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3697\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135000\n",
      "mean reward (50 episodes) 59.780000\n",
      "mean length (50 episodes) 20.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3725\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 135500\n",
      "mean reward (50 episodes) 57.520000\n",
      "mean length (50 episodes) 21.880000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3745\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136000\n",
      "mean reward (50 episodes) 57.840000\n",
      "mean length (50 episodes) 22.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3771\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 136500\n",
      "mean reward (50 episodes) 61.260000\n",
      "mean length (50 episodes) 21.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3795\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 137000\n",
      "mean reward (50 episodes) 62.920000\n",
      "mean length (50 episodes) 19.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3822\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 137500\n",
      "mean reward (50 episodes) 55.460000\n",
      "mean length (50 episodes) 24.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3837\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138000\n",
      "mean reward (50 episodes) 55.740000\n",
      "mean length (50 episodes) 22.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3859\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 138500\n",
      "mean reward (50 episodes) 58.000000\n",
      "mean length (50 episodes) 21.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3885\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139000\n",
      "mean reward (50 episodes) 69.380000\n",
      "mean length (50 episodes) 16.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3919\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 139500\n",
      "mean reward (50 episodes) 65.900000\n",
      "mean length (50 episodes) 17.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3947\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140000\n",
      "mean reward (50 episodes) 65.140000\n",
      "mean length (50 episodes) 17.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3971\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 140500\n",
      "mean reward (50 episodes) 63.820000\n",
      "mean length (50 episodes) 18.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 3998\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141000\n",
      "mean reward (50 episodes) 72.600000\n",
      "mean length (50 episodes) 12.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4037\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 141500\n",
      "mean reward (50 episodes) 73.920000\n",
      "mean length (50 episodes) 12.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4072\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142000\n",
      "mean reward (50 episodes) 75.000000\n",
      "mean length (50 episodes) 14.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4104\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 142500\n",
      "mean reward (50 episodes) 57.900000\n",
      "mean length (50 episodes) 19.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -480.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4122\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143000\n",
      "mean reward (50 episodes) 69.760000\n",
      "mean length (50 episodes) 14.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4170\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 143500\n",
      "mean reward (50 episodes) 53.720000\n",
      "mean length (50 episodes) 15.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -670.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4188\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144000\n",
      "mean reward (50 episodes) 12.100000\n",
      "mean length (50 episodes) 20.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -1050.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4213\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 144500\n",
      "mean reward (50 episodes) 17.940000\n",
      "mean length (50 episodes) 21.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -1050.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4236\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145000\n",
      "mean reward (50 episodes) 54.560000\n",
      "mean length (50 episodes) 21.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -310.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4258\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 145500\n",
      "mean reward (50 episodes) 71.500000\n",
      "mean length (50 episodes) 17.500000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4291\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146000\n",
      "mean reward (50 episodes) 65.940000\n",
      "mean length (50 episodes) 19.060000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4314\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 146500\n",
      "mean reward (50 episodes) 74.160000\n",
      "mean length (50 episodes) 14.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4351\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147000\n",
      "mean reward (50 episodes) 68.240000\n",
      "mean length (50 episodes) 16.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4382\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 147500\n",
      "mean reward (50 episodes) 73.540000\n",
      "mean length (50 episodes) 14.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4416\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148000\n",
      "mean reward (50 episodes) 70.660000\n",
      "mean length (50 episodes) 16.340000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4442\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 148500\n",
      "mean reward (50 episodes) 66.780000\n",
      "mean length (50 episodes) 18.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4467\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 149000\n",
      "mean reward (50 episodes) 61.860000\n",
      "mean length (50 episodes) 20.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4491\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 149500\n",
      "mean reward (50 episodes) 63.360000\n",
      "mean length (50 episodes) 20.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4517\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150000\n",
      "mean reward (50 episodes) 54.960000\n",
      "mean length (50 episodes) 25.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 77.180000\n",
      "episodes 4534\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 150500\n",
      "mean reward (50 episodes) 82.000000\n",
      "mean length (50 episodes) 11.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 82.000000\n",
      "episodes 4581\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151000\n",
      "mean reward (50 episodes) 75.300000\n",
      "mean length (50 episodes) 12.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4611\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 151500\n",
      "mean reward (50 episodes) 68.440000\n",
      "mean length (50 episodes) 16.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4640\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152000\n",
      "mean reward (50 episodes) 68.280000\n",
      "mean length (50 episodes) 16.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4671\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 152500\n",
      "mean reward (50 episodes) 62.640000\n",
      "mean length (50 episodes) 18.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4694\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153000\n",
      "mean reward (50 episodes) 72.740000\n",
      "mean length (50 episodes) 14.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4732\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 153500\n",
      "mean reward (50 episodes) 64.920000\n",
      "mean length (50 episodes) 18.480000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4752\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154000\n",
      "mean reward (50 episodes) 57.460000\n",
      "mean length (50 episodes) 22.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4774\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 154500\n",
      "mean reward (50 episodes) 59.500000\n",
      "mean length (50 episodes) 21.900000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4793\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155000\n",
      "mean reward (50 episodes) 80.960000\n",
      "mean length (50 episodes) 12.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4841\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 155500\n",
      "mean reward (50 episodes) 77.580000\n",
      "mean length (50 episodes) 12.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4868\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156000\n",
      "mean reward (50 episodes) 68.580000\n",
      "mean length (50 episodes) 16.620000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4900\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 156500\n",
      "mean reward (50 episodes) 61.680000\n",
      "mean length (50 episodes) 19.720000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4919\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157000\n",
      "mean reward (50 episodes) 59.860000\n",
      "mean length (50 episodes) 21.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4943\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 157500\n",
      "mean reward (50 episodes) 65.260000\n",
      "mean length (50 episodes) 20.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 4968\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158000\n",
      "mean reward (50 episodes) 72.200000\n",
      "mean length (50 episodes) 14.600000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5002\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 158500\n",
      "mean reward (50 episodes) 70.880000\n",
      "mean length (50 episodes) 15.320000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5031\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159000\n",
      "mean reward (50 episodes) 65.540000\n",
      "mean length (50 episodes) 19.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5053\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 159500\n",
      "mean reward (50 episodes) 58.340000\n",
      "mean length (50 episodes) 22.460000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5073\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160000\n",
      "mean reward (50 episodes) 66.900000\n",
      "mean length (50 episodes) 17.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5105\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 160500\n",
      "mean reward (50 episodes) 71.580000\n",
      "mean length (50 episodes) 16.020000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5132\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 161000\n",
      "mean reward (50 episodes) 61.800000\n",
      "mean length (50 episodes) 20.600000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5151\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 161500\n",
      "mean reward (50 episodes) 71.160000\n",
      "mean length (50 episodes) 15.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5191\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162000\n",
      "mean reward (50 episodes) 76.220000\n",
      "mean length (50 episodes) 12.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5231\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 162500\n",
      "mean reward (50 episodes) 76.420000\n",
      "mean length (50 episodes) 12.380000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5269\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163000\n",
      "mean reward (50 episodes) 67.460000\n",
      "mean length (50 episodes) 16.740000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5291\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 163500\n",
      "mean reward (50 episodes) 75.960000\n",
      "mean length (50 episodes) 12.640000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5335\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164000\n",
      "mean reward (50 episodes) 72.580000\n",
      "mean length (50 episodes) 14.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5359\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 164500\n",
      "mean reward (50 episodes) 60.620000\n",
      "mean length (50 episodes) 20.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5377\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165000\n",
      "mean reward (50 episodes) 63.260000\n",
      "mean length (50 episodes) 18.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5411\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 165500\n",
      "mean reward (50 episodes) 71.640000\n",
      "mean length (50 episodes) 13.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5443\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166000\n",
      "mean reward (50 episodes) 71.200000\n",
      "mean length (50 episodes) 14.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5477\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 166500\n",
      "mean reward (50 episodes) 70.280000\n",
      "mean length (50 episodes) 16.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5504\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167000\n",
      "mean reward (50 episodes) 59.200000\n",
      "mean length (50 episodes) 21.400000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5519\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 167500\n",
      "mean reward (50 episodes) 47.920000\n",
      "mean length (50 episodes) 26.880000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -140.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5538\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168000\n",
      "mean reward (50 episodes) 45.920000\n",
      "mean length (50 episodes) 25.280000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -153.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5560\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 168500\n",
      "mean reward (50 episodes) 57.480000\n",
      "mean length (50 episodes) 20.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -153.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5585\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169000\n",
      "mean reward (50 episodes) 64.780000\n",
      "mean length (50 episodes) 18.620000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5609\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 169500\n",
      "mean reward (50 episodes) 60.400000\n",
      "mean length (50 episodes) 21.800000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5634\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170000\n",
      "mean reward (50 episodes) 67.220000\n",
      "mean length (50 episodes) 17.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5660\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 170500\n",
      "mean reward (50 episodes) 68.860000\n",
      "mean length (50 episodes) 15.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5695\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171000\n",
      "mean reward (50 episodes) 70.080000\n",
      "mean length (50 episodes) 15.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5722\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 171500\n",
      "mean reward (50 episodes) 62.100000\n",
      "mean length (50 episodes) 18.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5754\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172000\n",
      "mean reward (50 episodes) 82.200000\n",
      "mean length (50 episodes) 9.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 55.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5803\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 172500\n",
      "mean reward (50 episodes) 77.240000\n",
      "mean length (50 episodes) 13.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5841\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 173000\n",
      "mean reward (50 episodes) 75.160000\n",
      "mean length (50 episodes) 12.640000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5876\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 173500\n",
      "mean reward (50 episodes) 70.140000\n",
      "mean length (50 episodes) 17.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5902\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174000\n",
      "mean reward (50 episodes) 74.860000\n",
      "mean length (50 episodes) 13.940000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5944\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 174500\n",
      "mean reward (50 episodes) 74.740000\n",
      "mean length (50 episodes) 13.460000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 5974\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175000\n",
      "mean reward (50 episodes) 76.820000\n",
      "mean length (50 episodes) 13.180000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6015\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 175500\n",
      "mean reward (50 episodes) 73.680000\n",
      "mean length (50 episodes) 14.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6040\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176000\n",
      "mean reward (50 episodes) 58.780000\n",
      "mean length (50 episodes) 21.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6058\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 176500\n",
      "mean reward (50 episodes) 73.820000\n",
      "mean length (50 episodes) 15.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6098\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177000\n",
      "mean reward (50 episodes) 75.920000\n",
      "mean length (50 episodes) 12.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6131\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 177500\n",
      "mean reward (50 episodes) 70.800000\n",
      "mean length (50 episodes) 16.400000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6169\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178000\n",
      "mean reward (50 episodes) 72.200000\n",
      "mean length (50 episodes) 15.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6204\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 178500\n",
      "mean reward (50 episodes) 80.140000\n",
      "mean length (50 episodes) 10.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6250\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179000\n",
      "mean reward (50 episodes) 76.220000\n",
      "mean length (50 episodes) 12.580000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6281\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 179500\n",
      "mean reward (50 episodes) 65.380000\n",
      "mean length (50 episodes) 17.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6307\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180000\n",
      "mean reward (50 episodes) 56.820000\n",
      "mean length (50 episodes) 21.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6327\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 180500\n",
      "mean reward (50 episodes) 76.560000\n",
      "mean length (50 episodes) 14.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6372\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181000\n",
      "mean reward (50 episodes) 69.220000\n",
      "mean length (50 episodes) 16.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6395\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 181500\n",
      "mean reward (50 episodes) 76.400000\n",
      "mean length (50 episodes) 12.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6436\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182000\n",
      "mean reward (50 episodes) 71.780000\n",
      "mean length (50 episodes) 15.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6465\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 182500\n",
      "mean reward (50 episodes) 68.980000\n",
      "mean length (50 episodes) 16.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6498\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183000\n",
      "mean reward (50 episodes) 82.460000\n",
      "mean length (50 episodes) 8.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6557\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 183500\n",
      "mean reward (50 episodes) 78.080000\n",
      "mean length (50 episodes) 10.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6604\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184000\n",
      "mean reward (50 episodes) 75.300000\n",
      "mean length (50 episodes) 12.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6642\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 184500\n",
      "mean reward (50 episodes) 78.120000\n",
      "mean length (50 episodes) 12.280000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6678\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 185000\n",
      "mean reward (50 episodes) 72.500000\n",
      "mean length (50 episodes) 15.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6705\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 185500\n",
      "mean reward (50 episodes) 71.200000\n",
      "mean length (50 episodes) 15.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6744\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186000\n",
      "mean reward (50 episodes) 66.640000\n",
      "mean length (50 episodes) 18.560000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6763\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 186500\n",
      "mean reward (50 episodes) 56.540000\n",
      "mean length (50 episodes) 23.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6784\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187000\n",
      "mean reward (50 episodes) 59.080000\n",
      "mean length (50 episodes) 20.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6813\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 187500\n",
      "mean reward (50 episodes) 76.540000\n",
      "mean length (50 episodes) 12.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6851\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188000\n",
      "mean reward (50 episodes) 78.980000\n",
      "mean length (50 episodes) 11.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6889\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 188500\n",
      "mean reward (50 episodes) 80.120000\n",
      "mean length (50 episodes) 11.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6931\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189000\n",
      "mean reward (50 episodes) 72.200000\n",
      "mean length (50 episodes) 15.600000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6956\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 189500\n",
      "mean reward (50 episodes) 59.320000\n",
      "mean length (50 episodes) 21.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 6973\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190000\n",
      "mean reward (50 episodes) 71.760000\n",
      "mean length (50 episodes) 15.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7015\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 190500\n",
      "mean reward (50 episodes) 74.540000\n",
      "mean length (50 episodes) 12.460000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7052\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191000\n",
      "mean reward (50 episodes) 70.460000\n",
      "mean length (50 episodes) 15.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7080\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 191500\n",
      "mean reward (50 episodes) 81.520000\n",
      "mean length (50 episodes) 10.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7127\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192000\n",
      "mean reward (50 episodes) 79.240000\n",
      "mean length (50 episodes) 11.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7166\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 192500\n",
      "mean reward (50 episodes) 78.920000\n",
      "mean length (50 episodes) 12.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7213\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193000\n",
      "mean reward (50 episodes) 73.420000\n",
      "mean length (50 episodes) 14.180000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7248\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 193500\n",
      "mean reward (50 episodes) 70.140000\n",
      "mean length (50 episodes) 15.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7279\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194000\n",
      "mean reward (50 episodes) 82.360000\n",
      "mean length (50 episodes) 9.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7330\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 194500\n",
      "mean reward (50 episodes) 79.200000\n",
      "mean length (50 episodes) 12.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7361\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195000\n",
      "mean reward (50 episodes) 84.560000\n",
      "mean length (50 episodes) 8.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7422\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 195500\n",
      "mean reward (50 episodes) 83.060000\n",
      "mean length (50 episodes) 10.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7466\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196000\n",
      "mean reward (50 episodes) 77.740000\n",
      "mean length (50 episodes) 13.660000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7500\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 196500\n",
      "mean reward (50 episodes) 82.220000\n",
      "mean length (50 episodes) 10.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7543\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 197000\n",
      "mean reward (50 episodes) 77.880000\n",
      "mean length (50 episodes) 11.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7580\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 197500\n",
      "mean reward (50 episodes) 78.680000\n",
      "mean length (50 episodes) 12.320000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7617\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198000\n",
      "mean reward (50 episodes) 76.260000\n",
      "mean length (50 episodes) 12.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7661\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 198500\n",
      "mean reward (50 episodes) 76.620000\n",
      "mean length (50 episodes) 12.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7687\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199000\n",
      "mean reward (50 episodes) 73.160000\n",
      "mean length (50 episodes) 16.640000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7714\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 199500\n",
      "mean reward (50 episodes) 74.920000\n",
      "mean length (50 episodes) 13.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7754\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 200000\n",
      "mean reward (50 episodes) 70.280000\n",
      "mean length (50 episodes) 15.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7784\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 200500\n",
      "mean reward (50 episodes) 81.680000\n",
      "mean length (50 episodes) 10.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7833\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 201000\n",
      "mean reward (50 episodes) 83.660000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7883\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 201500\n",
      "mean reward (50 episodes) 74.440000\n",
      "mean length (50 episodes) 12.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7929\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 202000\n",
      "mean reward (50 episodes) 78.020000\n",
      "mean length (50 episodes) 11.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 7958\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 202500\n",
      "mean reward (50 episodes) 84.900000\n",
      "mean length (50 episodes) 8.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8018\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 203000\n",
      "mean reward (50 episodes) 74.120000\n",
      "mean length (50 episodes) 13.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8046\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 203500\n",
      "mean reward (50 episodes) 75.580000\n",
      "mean length (50 episodes) 13.620000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8083\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 204000\n",
      "mean reward (50 episodes) 83.840000\n",
      "mean length (50 episodes) 8.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8135\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 204500\n",
      "mean reward (50 episodes) 75.980000\n",
      "mean length (50 episodes) 11.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8170\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 205000\n",
      "mean reward (50 episodes) 79.820000\n",
      "mean length (50 episodes) 11.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8216\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 205500\n",
      "mean reward (50 episodes) 84.120000\n",
      "mean length (50 episodes) 10.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 13.000000\n",
      "max_episode_length (50 episodes) 87.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8256\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 206000\n",
      "mean reward (50 episodes) 83.080000\n",
      "mean length (50 episodes) 8.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8313\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 206500\n",
      "mean reward (50 episodes) 81.580000\n",
      "mean length (50 episodes) 10.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8363\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 207000\n",
      "mean reward (50 episodes) 78.940000\n",
      "mean length (50 episodes) 11.460000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8404\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 207500\n",
      "mean reward (50 episodes) 82.100000\n",
      "mean length (50 episodes) 10.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8441\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 208000\n",
      "mean reward (50 episodes) 70.300000\n",
      "mean length (50 episodes) 15.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8470\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 208500\n",
      "mean reward (50 episodes) 81.200000\n",
      "mean length (50 episodes) 9.200000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8521\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 209000\n",
      "mean reward (50 episodes) 84.880000\n",
      "mean length (50 episodes) 8.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 86.500000\n",
      "episodes 8578\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 209500\n",
      "mean reward (50 episodes) 79.500000\n",
      "mean length (50 episodes) 12.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8612\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 210000\n",
      "mean reward (50 episodes) 66.560000\n",
      "mean length (50 episodes) 18.640000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8636\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 210500\n",
      "mean reward (50 episodes) 71.900000\n",
      "mean length (50 episodes) 13.900000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8674\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 211000\n",
      "mean reward (50 episodes) 80.920000\n",
      "mean length (50 episodes) 11.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8717\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 211500\n",
      "mean reward (50 episodes) 77.500000\n",
      "mean length (50 episodes) 12.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8755\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 212000\n",
      "mean reward (50 episodes) 77.540000\n",
      "mean length (50 episodes) 10.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8804\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 212500\n",
      "mean reward (50 episodes) 82.460000\n",
      "mean length (50 episodes) 8.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 20.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8861\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 213000\n",
      "mean reward (50 episodes) 84.000000\n",
      "mean length (50 episodes) 9.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 42.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8918\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 213500\n",
      "mean reward (50 episodes) 79.460000\n",
      "mean length (50 episodes) 10.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 8962\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 214000\n",
      "mean reward (50 episodes) 79.780000\n",
      "mean length (50 episodes) 10.620000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9006\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 214500\n",
      "mean reward (50 episodes) 75.200000\n",
      "mean length (50 episodes) 12.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9047\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 215000\n",
      "mean reward (50 episodes) 82.940000\n",
      "mean length (50 episodes) 8.460000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9106\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 215500\n",
      "mean reward (50 episodes) 80.460000\n",
      "mean length (50 episodes) 10.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9152\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 216000\n",
      "mean reward (50 episodes) 73.600000\n",
      "mean length (50 episodes) 14.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9189\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 216500\n",
      "mean reward (50 episodes) 82.780000\n",
      "mean length (50 episodes) 8.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9240\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 217000\n",
      "mean reward (50 episodes) 71.020000\n",
      "mean length (50 episodes) 15.180000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9267\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 217500\n",
      "mean reward (50 episodes) 71.680000\n",
      "mean length (50 episodes) 14.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9308\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 218000\n",
      "mean reward (50 episodes) 84.720000\n",
      "mean length (50 episodes) 8.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 86.560000\n",
      "episodes 9366\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 218500\n",
      "mean reward (50 episodes) 85.340000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9427\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 219000\n",
      "mean reward (50 episodes) 82.300000\n",
      "mean length (50 episodes) 9.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 51.000000\n",
      "max_episode_length (50 episodes) 29.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9477\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 219500\n",
      "mean reward (50 episodes) 79.060000\n",
      "mean length (50 episodes) 11.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9518\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 220000\n",
      "mean reward (50 episodes) 85.060000\n",
      "mean length (50 episodes) 8.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9570\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 220500\n",
      "mean reward (50 episodes) 85.500000\n",
      "mean length (50 episodes) 8.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 44.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9622\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 221000\n",
      "mean reward (50 episodes) 81.100000\n",
      "mean length (50 episodes) 11.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9663\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 221500\n",
      "mean reward (50 episodes) 82.280000\n",
      "mean length (50 episodes) 11.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 11.000000\n",
      "max_episode_length (50 episodes) 79.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9701\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 222000\n",
      "mean reward (50 episodes) 85.160000\n",
      "mean length (50 episodes) 8.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9756\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 222500\n",
      "mean reward (50 episodes) 82.660000\n",
      "mean length (50 episodes) 9.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 26.000000\n",
      "max_episode_length (50 episodes) 64.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9805\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 223000\n",
      "mean reward (50 episodes) 75.040000\n",
      "mean length (50 episodes) 12.360000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9843\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 223500\n",
      "mean reward (50 episodes) 83.280000\n",
      "mean length (50 episodes) 8.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9902\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 224000\n",
      "mean reward (50 episodes) 86.760000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 87.100000\n",
      "episodes 9959\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 224500\n",
      "mean reward (50 episodes) 84.820000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10019\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 225000\n",
      "mean reward (50 episodes) 75.220000\n",
      "mean length (50 episodes) 13.180000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10051\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 225500\n",
      "mean reward (50 episodes) 86.140000\n",
      "mean length (50 episodes) 8.460000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10110\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 226000\n",
      "mean reward (50 episodes) 87.120000\n",
      "mean length (50 episodes) 8.480000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 26.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10170\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 226500\n",
      "mean reward (50 episodes) 80.900000\n",
      "mean length (50 episodes) 10.300000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10207\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 227000\n",
      "mean reward (50 episodes) 78.580000\n",
      "mean length (50 episodes) 12.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10256\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 227500\n",
      "mean reward (50 episodes) 82.160000\n",
      "mean length (50 episodes) 11.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 20.000000\n",
      "max_episode_length (50 episodes) 70.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10300\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 228000\n",
      "mean reward (50 episodes) 79.360000\n",
      "mean length (50 episodes) 11.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10340\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 228500\n",
      "mean reward (50 episodes) 69.640000\n",
      "mean length (50 episodes) 16.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10366\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 229000\n",
      "mean reward (50 episodes) 81.020000\n",
      "mean length (50 episodes) 10.180000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10415\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 229500\n",
      "mean reward (50 episodes) 83.120000\n",
      "mean length (50 episodes) 10.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -18.000000\n",
      "max_episode_length (50 episodes) 98.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10461\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 230000\n",
      "mean reward (50 episodes) 81.240000\n",
      "mean length (50 episodes) 10.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10505\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 230500\n",
      "mean reward (50 episodes) 84.200000\n",
      "mean length (50 episodes) 8.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10562\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 231000\n",
      "mean reward (50 episodes) 84.480000\n",
      "mean length (50 episodes) 10.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 87.260000\n",
      "episodes 10611\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 231500\n",
      "mean reward (50 episodes) 84.100000\n",
      "mean length (50 episodes) 10.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10660\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 232000\n",
      "mean reward (50 episodes) 86.520000\n",
      "mean length (50 episodes) 8.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 14.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10721\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 232500\n",
      "mean reward (50 episodes) 85.640000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10777\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 233000\n",
      "mean reward (50 episodes) 80.940000\n",
      "mean length (50 episodes) 11.260000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10825\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 233500\n",
      "mean reward (50 episodes) 77.200000\n",
      "mean length (50 episodes) 13.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10857\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 234000\n",
      "mean reward (50 episodes) 74.520000\n",
      "mean length (50 episodes) 15.680000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10891\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 234500\n",
      "mean reward (50 episodes) 78.080000\n",
      "mean length (50 episodes) 10.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10939\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 235000\n",
      "mean reward (50 episodes) 80.780000\n",
      "mean length (50 episodes) 10.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 10987\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 235500\n",
      "mean reward (50 episodes) 84.660000\n",
      "mean length (50 episodes) 8.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 37.000000\n",
      "max_episode_length (50 episodes) 43.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11043\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 236000\n",
      "mean reward (50 episodes) 84.820000\n",
      "mean length (50 episodes) 8.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11089\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 236500\n",
      "mean reward (50 episodes) 83.120000\n",
      "mean length (50 episodes) 8.880000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 30.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11141\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 237000\n",
      "mean reward (50 episodes) 75.600000\n",
      "mean length (50 episodes) 12.800000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11182\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 237500\n",
      "mean reward (50 episodes) 84.580000\n",
      "mean length (50 episodes) 8.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11230\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 238000\n",
      "mean reward (50 episodes) 86.180000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11286\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 238500\n",
      "mean reward (50 episodes) 84.780000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11348\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 239000\n",
      "mean reward (50 episodes) 82.480000\n",
      "mean length (50 episodes) 8.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11405\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 239500\n",
      "mean reward (50 episodes) 83.160000\n",
      "mean length (50 episodes) 10.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11455\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 240000\n",
      "mean reward (50 episodes) 85.580000\n",
      "mean length (50 episodes) 8.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 36.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11510\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 240500\n",
      "mean reward (50 episodes) 86.580000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11572\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 241000\n",
      "mean reward (50 episodes) 82.640000\n",
      "mean length (50 episodes) 9.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 6.000000\n",
      "max_episode_length (50 episodes) 84.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11623\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 241500\n",
      "mean reward (50 episodes) 82.420000\n",
      "mean length (50 episodes) 10.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11670\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 242000\n",
      "mean reward (50 episodes) 80.040000\n",
      "mean length (50 episodes) 9.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11717\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 242500\n",
      "mean reward (50 episodes) 80.300000\n",
      "mean length (50 episodes) 11.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11759\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 243000\n",
      "mean reward (50 episodes) 85.360000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11819\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 243500\n",
      "mean reward (50 episodes) 86.640000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11879\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 244000\n",
      "mean reward (50 episodes) 81.460000\n",
      "mean length (50 episodes) 10.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11929\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 244500\n",
      "mean reward (50 episodes) 81.000000\n",
      "mean length (50 episodes) 10.600000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 11975\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 245000\n",
      "mean reward (50 episodes) 74.440000\n",
      "mean length (50 episodes) 13.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12013\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 245500\n",
      "mean reward (50 episodes) 83.740000\n",
      "mean length (50 episodes) 8.460000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12072\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 246000\n",
      "mean reward (50 episodes) 79.400000\n",
      "mean length (50 episodes) 12.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12111\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 246500\n",
      "mean reward (50 episodes) 73.080000\n",
      "mean length (50 episodes) 15.120000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12141\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 247000\n",
      "mean reward (50 episodes) 81.920000\n",
      "mean length (50 episodes) 9.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 27.000000\n",
      "max_episode_length (50 episodes) 63.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12192\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 247500\n",
      "mean reward (50 episodes) 83.780000\n",
      "mean length (50 episodes) 9.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 24.000000\n",
      "max_episode_length (50 episodes) 46.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12247\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 248000\n",
      "mean reward (50 episodes) 81.420000\n",
      "mean length (50 episodes) 10.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12295\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 248500\n",
      "mean reward (50 episodes) 82.700000\n",
      "mean length (50 episodes) 8.700000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12352\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 249000\n",
      "mean reward (50 episodes) 78.100000\n",
      "mean length (50 episodes) 11.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12392\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 249500\n",
      "mean reward (50 episodes) 85.580000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12450\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 250000\n",
      "mean reward (50 episodes) 78.620000\n",
      "mean length (50 episodes) 11.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12491\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 250500\n",
      "mean reward (50 episodes) 86.400000\n",
      "mean length (50 episodes) 8.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 32.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12549\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 251000\n",
      "mean reward (50 episodes) 74.480000\n",
      "mean length (50 episodes) 10.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -300.000000\n",
      "max_episode_length (50 episodes) 90.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12597\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 251500\n",
      "mean reward (50 episodes) 78.960000\n",
      "mean length (50 episodes) 9.040000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -251.000000\n",
      "max_episode_length (50 episodes) 41.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12649\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 252000\n",
      "mean reward (50 episodes) 81.220000\n",
      "mean length (50 episodes) 10.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12693\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 252500\n",
      "mean reward (50 episodes) 75.740000\n",
      "mean length (50 episodes) 12.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12732\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 253000\n",
      "mean reward (50 episodes) 80.660000\n",
      "mean length (50 episodes) 10.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12779\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 253500\n",
      "mean reward (50 episodes) 71.420000\n",
      "mean length (50 episodes) 14.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12815\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 254000\n",
      "mean reward (50 episodes) 72.640000\n",
      "mean length (50 episodes) 14.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12845\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 254500\n",
      "mean reward (50 episodes) 78.240000\n",
      "mean length (50 episodes) 12.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12881\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 255000\n",
      "mean reward (50 episodes) 86.140000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 12942\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 255500\n",
      "mean reward (50 episodes) 86.160000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13003\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 256000\n",
      "mean reward (50 episodes) 81.600000\n",
      "mean length (50 episodes) 8.800000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 27.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13058\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 256500\n",
      "mean reward (50 episodes) 82.620000\n",
      "mean length (50 episodes) 8.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13106\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 257000\n",
      "mean reward (50 episodes) 78.560000\n",
      "mean length (50 episodes) 12.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13148\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 257500\n",
      "mean reward (50 episodes) 84.280000\n",
      "mean length (50 episodes) 8.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 18.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13205\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 258000\n",
      "mean reward (50 episodes) 86.440000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13266\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 258500\n",
      "mean reward (50 episodes) 82.760000\n",
      "mean length (50 episodes) 8.640000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 19.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13325\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 259000\n",
      "mean reward (50 episodes) 85.840000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13386\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 259500\n",
      "mean reward (50 episodes) 86.560000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13446\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 260000\n",
      "mean reward (50 episodes) 83.640000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13509\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 260500\n",
      "mean reward (50 episodes) 83.480000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13570\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 261000\n",
      "mean reward (50 episodes) 79.280000\n",
      "mean length (50 episodes) 10.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13618\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 261500\n",
      "mean reward (50 episodes) 83.960000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13679\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 262000\n",
      "mean reward (50 episodes) 79.940000\n",
      "mean length (50 episodes) 10.060000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13729\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 262500\n",
      "mean reward (50 episodes) 83.740000\n",
      "mean length (50 episodes) 8.460000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13789\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 263000\n",
      "mean reward (50 episodes) 85.820000\n",
      "mean length (50 episodes) 7.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13843\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 263500\n",
      "mean reward (50 episodes) 77.240000\n",
      "mean length (50 episodes) 11.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13890\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 264000\n",
      "mean reward (50 episodes) 84.640000\n",
      "mean length (50 episodes) 8.560000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 21.000000\n",
      "best mean reward 88.800000\n",
      "episodes 13950\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 264500\n",
      "mean reward (50 episodes) 86.120000\n",
      "mean length (50 episodes) 8.480000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 18.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14009\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 265000\n",
      "mean reward (50 episodes) 84.280000\n",
      "mean length (50 episodes) 8.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 18.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14067\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 265500\n",
      "mean reward (50 episodes) 87.760000\n",
      "mean length (50 episodes) 7.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14130\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 266000\n",
      "mean reward (50 episodes) 81.660000\n",
      "mean length (50 episodes) 10.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14176\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 266500\n",
      "mean reward (50 episodes) 80.080000\n",
      "mean length (50 episodes) 10.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14214\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 267000\n",
      "mean reward (50 episodes) 84.520000\n",
      "mean length (50 episodes) 8.480000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 45.000000\n",
      "max_episode_length (50 episodes) 25.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14273\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 267500\n",
      "mean reward (50 episodes) 87.740000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14334\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 268000\n",
      "mean reward (50 episodes) 80.300000\n",
      "mean length (50 episodes) 10.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14384\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 268500\n",
      "mean reward (50 episodes) 85.740000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14445\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 269000\n",
      "mean reward (50 episodes) 86.100000\n",
      "mean length (50 episodes) 7.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14511\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 269500\n",
      "mean reward (50 episodes) 82.760000\n",
      "mean length (50 episodes) 9.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14562\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 270000\n",
      "mean reward (50 episodes) 86.580000\n",
      "mean length (50 episodes) 8.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14621\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 270500\n",
      "mean reward (50 episodes) 84.580000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14682\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 271000\n",
      "mean reward (50 episodes) 82.140000\n",
      "mean length (50 episodes) 10.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14732\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 271500\n",
      "mean reward (50 episodes) 85.440000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14792\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 272000\n",
      "mean reward (50 episodes) 86.060000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 56.000000\n",
      "max_episode_length (50 episodes) 14.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14854\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 272500\n",
      "mean reward (50 episodes) 82.220000\n",
      "mean length (50 episodes) 9.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14905\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 273000\n",
      "mean reward (50 episodes) 82.400000\n",
      "mean length (50 episodes) 9.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 14956\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 273500\n",
      "mean reward (50 episodes) 84.620000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15008\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 274000\n",
      "mean reward (50 episodes) 78.080000\n",
      "mean length (50 episodes) 11.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15051\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 274500\n",
      "mean reward (50 episodes) 79.620000\n",
      "mean length (50 episodes) 11.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15094\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 275000\n",
      "mean reward (50 episodes) 84.480000\n",
      "mean length (50 episodes) 8.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15151\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 275500\n",
      "mean reward (50 episodes) 81.520000\n",
      "mean length (50 episodes) 10.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15200\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 276000\n",
      "mean reward (50 episodes) 85.940000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15260\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 276500\n",
      "mean reward (50 episodes) 84.460000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 40.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15322\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 277000\n",
      "mean reward (50 episodes) 84.620000\n",
      "mean length (50 episodes) 8.180000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15382\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 277500\n",
      "mean reward (50 episodes) 81.860000\n",
      "mean length (50 episodes) 10.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15431\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 278000\n",
      "mean reward (50 episodes) 84.680000\n",
      "mean length (50 episodes) 7.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15493\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 278500\n",
      "mean reward (50 episodes) 82.520000\n",
      "mean length (50 episodes) 10.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15537\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 279000\n",
      "mean reward (50 episodes) 86.560000\n",
      "mean length (50 episodes) 7.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15595\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 279500\n",
      "mean reward (50 episodes) 83.000000\n",
      "mean length (50 episodes) 10.000000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 31.000000\n",
      "max_episode_length (50 episodes) 69.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15645\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 280000\n",
      "mean reward (50 episodes) 86.660000\n",
      "mean length (50 episodes) 7.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15708\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 280500\n",
      "mean reward (50 episodes) 76.940000\n",
      "mean length (50 episodes) 11.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15745\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 281000\n",
      "mean reward (50 episodes) 84.960000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15805\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 281500\n",
      "mean reward (50 episodes) 84.900000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15864\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 282000\n",
      "mean reward (50 episodes) 86.840000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15926\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 282500\n",
      "mean reward (50 episodes) 83.160000\n",
      "mean length (50 episodes) 8.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 15988\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 283000\n",
      "mean reward (50 episodes) 84.680000\n",
      "mean length (50 episodes) 7.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16051\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 283500\n",
      "mean reward (50 episodes) 80.780000\n",
      "mean length (50 episodes) 10.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16096\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 284000\n",
      "mean reward (50 episodes) 82.820000\n",
      "mean length (50 episodes) 9.580000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16149\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 284500\n",
      "mean reward (50 episodes) 86.280000\n",
      "mean length (50 episodes) 7.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16201\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 285000\n",
      "mean reward (50 episodes) 83.100000\n",
      "mean length (50 episodes) 8.700000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16258\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 285500\n",
      "mean reward (50 episodes) 82.180000\n",
      "mean length (50 episodes) 10.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16307\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 286000\n",
      "mean reward (50 episodes) 84.220000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16366\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 286500\n",
      "mean reward (50 episodes) 79.800000\n",
      "mean length (50 episodes) 10.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16405\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 287000\n",
      "mean reward (50 episodes) 79.820000\n",
      "mean length (50 episodes) 11.780000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16455\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 287500\n",
      "mean reward (50 episodes) 86.760000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 52.000000\n",
      "max_episode_length (50 episodes) 28.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16515\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 288000\n",
      "mean reward (50 episodes) 83.980000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16575\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 288500\n",
      "mean reward (50 episodes) 72.560000\n",
      "mean length (50 episodes) 13.640000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16604\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 289000\n",
      "mean reward (50 episodes) 84.720000\n",
      "mean length (50 episodes) 8.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 38.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16666\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 289500\n",
      "mean reward (50 episodes) 80.180000\n",
      "mean length (50 episodes) 9.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16717\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 290000\n",
      "mean reward (50 episodes) 81.960000\n",
      "mean length (50 episodes) 9.840000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16768\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 290500\n",
      "mean reward (50 episodes) 84.540000\n",
      "mean length (50 episodes) 8.460000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16827\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 291000\n",
      "mean reward (50 episodes) 86.480000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16890\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 291500\n",
      "mean reward (50 episodes) 83.560000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.800000\n",
      "episodes 16949\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 292000\n",
      "mean reward (50 episodes) 88.580000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.800000\n",
      "episodes 17010\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 292500\n",
      "mean reward (50 episodes) 82.640000\n",
      "mean length (50 episodes) 9.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17061\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 293000\n",
      "mean reward (50 episodes) 77.500000\n",
      "mean length (50 episodes) 12.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17108\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 293500\n",
      "mean reward (50 episodes) 84.160000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17168\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 294000\n",
      "mean reward (50 episodes) 87.280000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 14.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17229\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 294500\n",
      "mean reward (50 episodes) 82.960000\n",
      "mean length (50 episodes) 10.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17278\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 295000\n",
      "mean reward (50 episodes) 82.280000\n",
      "mean length (50 episodes) 10.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17328\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 295500\n",
      "mean reward (50 episodes) 83.640000\n",
      "mean length (50 episodes) 9.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17378\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 296000\n",
      "mean reward (50 episodes) 84.920000\n",
      "mean length (50 episodes) 8.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17440\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 296500\n",
      "mean reward (50 episodes) 82.800000\n",
      "mean length (50 episodes) 10.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -120.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17490\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 297000\n",
      "mean reward (50 episodes) 77.640000\n",
      "mean length (50 episodes) 11.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -130.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17530\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 297500\n",
      "mean reward (50 episodes) 85.840000\n",
      "mean length (50 episodes) 7.760000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17592\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 298000\n",
      "mean reward (50 episodes) 86.120000\n",
      "mean length (50 episodes) 8.480000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17651\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 298500\n",
      "mean reward (50 episodes) 86.040000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 59.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17713\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 299000\n",
      "mean reward (50 episodes) 84.340000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17772\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 299500\n",
      "mean reward (50 episodes) 86.340000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17832\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 300000\n",
      "mean reward (50 episodes) 80.880000\n",
      "mean length (50 episodes) 10.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17880\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 300500\n",
      "mean reward (50 episodes) 81.520000\n",
      "mean length (50 episodes) 9.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17930\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 301000\n",
      "mean reward (50 episodes) 87.940000\n",
      "mean length (50 episodes) 8.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 52.000000\n",
      "best mean reward 88.840000\n",
      "episodes 17988\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 301500\n",
      "mean reward (50 episodes) 85.300000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18048\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 302000\n",
      "mean reward (50 episodes) 85.540000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18108\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 302500\n",
      "mean reward (50 episodes) 87.140000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18169\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 303000\n",
      "mean reward (50 episodes) 83.440000\n",
      "mean length (50 episodes) 9.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 42.000000\n",
      "max_episode_length (50 episodes) 38.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18225\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 303500\n",
      "mean reward (50 episodes) 86.360000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18286\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 304000\n",
      "mean reward (50 episodes) 88.240000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18346\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 304500\n",
      "mean reward (50 episodes) 83.800000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18402\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 305000\n",
      "mean reward (50 episodes) 84.580000\n",
      "mean length (50 episodes) 8.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 40.000000\n",
      "max_episode_length (50 episodes) 50.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18453\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 305500\n",
      "mean reward (50 episodes) 86.460000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18514\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 306000\n",
      "mean reward (50 episodes) 86.860000\n",
      "mean length (50 episodes) 9.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 28.000000\n",
      "max_episode_length (50 episodes) 62.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18567\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 306500\n",
      "mean reward (50 episodes) 86.600000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18628\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 307000\n",
      "mean reward (50 episodes) 82.800000\n",
      "mean length (50 episodes) 8.400000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18685\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 307500\n",
      "mean reward (50 episodes) 80.740000\n",
      "mean length (50 episodes) 10.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18731\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 308000\n",
      "mean reward (50 episodes) 85.660000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18794\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 308500\n",
      "mean reward (50 episodes) 84.680000\n",
      "mean length (50 episodes) 9.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 6.000000\n",
      "max_episode_length (50 episodes) 84.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18846\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 309000\n",
      "mean reward (50 episodes) 84.720000\n",
      "mean length (50 episodes) 8.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 51.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18907\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 309500\n",
      "mean reward (50 episodes) 86.100000\n",
      "mean length (50 episodes) 8.700000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 37.000000\n",
      "max_episode_length (50 episodes) 53.000000\n",
      "best mean reward 88.840000\n",
      "episodes 18965\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 310000\n",
      "mean reward (50 episodes) 71.580000\n",
      "mean length (50 episodes) 9.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -658.000000\n",
      "max_episode_length (50 episodes) 78.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19016\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 310500\n",
      "mean reward (50 episodes) 87.860000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19075\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 311000\n",
      "mean reward (50 episodes) 86.300000\n",
      "mean length (50 episodes) 9.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 47.000000\n",
      "max_episode_length (50 episodes) 53.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19129\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 311500\n",
      "mean reward (50 episodes) 85.600000\n",
      "mean length (50 episodes) 8.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19191\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 312000\n",
      "mean reward (50 episodes) 85.500000\n",
      "mean length (50 episodes) 8.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19250\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 312500\n",
      "mean reward (50 episodes) 86.500000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 19.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19310\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 313000\n",
      "mean reward (50 episodes) 84.780000\n",
      "mean length (50 episodes) 9.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19366\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 313500\n",
      "mean reward (50 episodes) 85.880000\n",
      "mean length (50 episodes) 8.520000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19425\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 314000\n",
      "mean reward (50 episodes) 86.040000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19485\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 314500\n",
      "mean reward (50 episodes) 86.100000\n",
      "mean length (50 episodes) 9.100000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 57.000000\n",
      "max_episode_length (50 episodes) 43.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19540\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 315000\n",
      "mean reward (50 episodes) 88.540000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 68.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19601\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 315500\n",
      "mean reward (50 episodes) 88.060000\n",
      "mean length (50 episodes) 7.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19665\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 316000\n",
      "mean reward (50 episodes) 85.400000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19726\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 316500\n",
      "mean reward (50 episodes) 83.260000\n",
      "mean length (50 episodes) 10.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -100.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19774\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 317000\n",
      "mean reward (50 episodes) 87.660000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 14.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19834\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 317500\n",
      "mean reward (50 episodes) 85.440000\n",
      "mean length (50 episodes) 8.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 40.000000\n",
      "max_episode_length (50 episodes) 33.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19890\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 318000\n",
      "mean reward (50 episodes) 84.200000\n",
      "mean length (50 episodes) 9.400000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 38.000000\n",
      "max_episode_length (50 episodes) 52.000000\n",
      "best mean reward 88.840000\n",
      "episodes 19944\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 318500\n",
      "mean reward (50 episodes) 86.820000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20004\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 319000\n",
      "mean reward (50 episodes) 86.860000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20065\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 319500\n",
      "mean reward (50 episodes) 85.860000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20126\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 320000\n",
      "mean reward (50 episodes) 86.100000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20184\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 320500\n",
      "mean reward (50 episodes) 84.940000\n",
      "mean length (50 episodes) 8.660000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 57.000000\n",
      "max_episode_length (50 episodes) 23.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20242\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 321000\n",
      "mean reward (50 episodes) 84.380000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20303\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 321500\n",
      "mean reward (50 episodes) 88.480000\n",
      "mean length (50 episodes) 8.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 80.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20363\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 322000\n",
      "mean reward (50 episodes) 83.980000\n",
      "mean length (50 episodes) 8.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20422\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 322500\n",
      "mean reward (50 episodes) 85.800000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20483\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 323000\n",
      "mean reward (50 episodes) 82.400000\n",
      "mean length (50 episodes) 9.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) -110.000000\n",
      "max_episode_length (50 episodes) 100.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20535\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 323500\n",
      "mean reward (50 episodes) 87.640000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20597\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 324000\n",
      "mean reward (50 episodes) 85.060000\n",
      "mean length (50 episodes) 8.540000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20656\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 324500\n",
      "mean reward (50 episodes) 87.940000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20716\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 325000\n",
      "mean reward (50 episodes) 86.440000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20778\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 325500\n",
      "mean reward (50 episodes) 86.580000\n",
      "mean length (50 episodes) 7.820000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20840\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 326000\n",
      "mean reward (50 episodes) 86.260000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20901\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 326500\n",
      "mean reward (50 episodes) 86.800000\n",
      "mean length (50 episodes) 8.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 20964\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 327000\n",
      "mean reward (50 episodes) 84.800000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21024\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 327500\n",
      "mean reward (50 episodes) 85.900000\n",
      "mean length (50 episodes) 8.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 24.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21083\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 328000\n",
      "mean reward (50 episodes) 87.520000\n",
      "mean length (50 episodes) 7.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21147\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 328500\n",
      "mean reward (50 episodes) 86.260000\n",
      "mean length (50 episodes) 7.740000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21210\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 329000\n",
      "mean reward (50 episodes) 85.960000\n",
      "mean length (50 episodes) 8.040000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21272\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 329500\n",
      "mean reward (50 episodes) 86.260000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 56.000000\n",
      "max_episode_length (50 episodes) 14.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21333\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 330000\n",
      "mean reward (50 episodes) 86.880000\n",
      "mean length (50 episodes) 8.520000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21392\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 330500\n",
      "mean reward (50 episodes) 86.220000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21452\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 331000\n",
      "mean reward (50 episodes) 86.700000\n",
      "mean length (50 episodes) 8.500000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21512\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 331500\n",
      "mean reward (50 episodes) 87.120000\n",
      "mean length (50 episodes) 8.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 18.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21572\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 332000\n",
      "mean reward (50 episodes) 86.020000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21633\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 332500\n",
      "mean reward (50 episodes) 86.140000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21694\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 333000\n",
      "mean reward (50 episodes) 86.880000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21756\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 333500\n",
      "mean reward (50 episodes) 85.400000\n",
      "mean length (50 episodes) 8.400000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21815\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 334000\n",
      "mean reward (50 episodes) 86.700000\n",
      "mean length (50 episodes) 8.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21876\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 334500\n",
      "mean reward (50 episodes) 87.200000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21937\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 335000\n",
      "mean reward (50 episodes) 85.580000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 21998\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 335500\n",
      "mean reward (50 episodes) 85.880000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 22060\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 336000\n",
      "mean reward (50 episodes) 87.360000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.840000\n",
      "episodes 22121\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 336500\n",
      "mean reward (50 episodes) 85.280000\n",
      "mean length (50 episodes) 8.320000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 51.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.840000\n",
      "episodes 22181\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 337000\n",
      "mean reward (50 episodes) 86.620000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22241\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 337500\n",
      "mean reward (50 episodes) 84.560000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22302\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 338000\n",
      "mean reward (50 episodes) 86.800000\n",
      "mean length (50 episodes) 7.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22366\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 338500\n",
      "mean reward (50 episodes) 85.740000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22427\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 339000\n",
      "mean reward (50 episodes) 86.180000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22491\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 339500\n",
      "mean reward (50 episodes) 88.180000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 94.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22553\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 340000\n",
      "mean reward (50 episodes) 87.040000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22613\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 340500\n",
      "mean reward (50 episodes) 85.580000\n",
      "mean length (50 episodes) 8.420000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22672\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 341000\n",
      "mean reward (50 episodes) 86.740000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22733\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 341500\n",
      "mean reward (50 episodes) 85.920000\n",
      "mean length (50 episodes) 8.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22794\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 342000\n",
      "mean reward (50 episodes) 85.780000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22856\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 342500\n",
      "mean reward (50 episodes) 87.240000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 66.000000\n",
      "max_episode_length (50 episodes) 14.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22916\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 343000\n",
      "mean reward (50 episodes) 86.340000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 22975\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 343500\n",
      "mean reward (50 episodes) 86.640000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23037\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 344000\n",
      "mean reward (50 episodes) 87.340000\n",
      "mean length (50 episodes) 7.860000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23100\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 344500\n",
      "mean reward (50 episodes) 84.400000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23160\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 345000\n",
      "mean reward (50 episodes) 83.100000\n",
      "mean length (50 episodes) 8.700000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 16.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23218\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 345500\n",
      "mean reward (50 episodes) 84.340000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23278\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 346000\n",
      "mean reward (50 episodes) 85.360000\n",
      "mean length (50 episodes) 8.640000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23336\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 346500\n",
      "mean reward (50 episodes) 87.020000\n",
      "mean length (50 episodes) 7.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23397\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 347000\n",
      "mean reward (50 episodes) 85.040000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23459\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 347500\n",
      "mean reward (50 episodes) 86.620000\n",
      "mean length (50 episodes) 7.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 51.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23519\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 348000\n",
      "mean reward (50 episodes) 86.180000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23580\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 348500\n",
      "mean reward (50 episodes) 85.660000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23641\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 349000\n",
      "mean reward (50 episodes) 86.940000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23701\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 349500\n",
      "mean reward (50 episodes) 84.420000\n",
      "mean length (50 episodes) 8.180000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23763\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 350000\n",
      "mean reward (50 episodes) 86.900000\n",
      "mean length (50 episodes) 8.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23824\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 350500\n",
      "mean reward (50 episodes) 84.020000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 51.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23885\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 351000\n",
      "mean reward (50 episodes) 86.860000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 23946\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 351500\n",
      "mean reward (50 episodes) 85.480000\n",
      "mean length (50 episodes) 7.720000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24010\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 352000\n",
      "mean reward (50 episodes) 84.400000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 14.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24071\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 352500\n",
      "mean reward (50 episodes) 87.440000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24133\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 353000\n",
      "mean reward (50 episodes) 84.640000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24194\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 353500\n",
      "mean reward (50 episodes) 86.840000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24256\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 354000\n",
      "mean reward (50 episodes) 86.220000\n",
      "mean length (50 episodes) 8.180000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 57.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24317\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 354500\n",
      "mean reward (50 episodes) 87.220000\n",
      "mean length (50 episodes) 7.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 18.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24379\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 355000\n",
      "mean reward (50 episodes) 86.240000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 65.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24438\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 355500\n",
      "mean reward (50 episodes) 87.180000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24501\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 356000\n",
      "mean reward (50 episodes) 87.580000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24563\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 356500\n",
      "mean reward (50 episodes) 86.780000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24624\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 357000\n",
      "mean reward (50 episodes) 87.260000\n",
      "mean length (50 episodes) 7.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24688\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 357500\n",
      "mean reward (50 episodes) 87.540000\n",
      "mean length (50 episodes) 8.460000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24747\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 358000\n",
      "mean reward (50 episodes) 85.740000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 16.000000\n",
      "best mean reward 88.860000\n",
      "episodes 24807\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 358500\n",
      "mean reward (50 episodes) 88.560000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.020000\n",
      "episodes 24868\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 359000\n",
      "mean reward (50 episodes) 84.680000\n",
      "mean length (50 episodes) 8.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.020000\n",
      "episodes 24927\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 359500\n",
      "mean reward (50 episodes) 83.960000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.020000\n",
      "episodes 24986\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 360000\n",
      "mean reward (50 episodes) 85.600000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.020000\n",
      "episodes 25046\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 360500\n",
      "mean reward (50 episodes) 87.040000\n",
      "mean length (50 episodes) 8.560000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.020000\n",
      "episodes 25105\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 361000\n",
      "mean reward (50 episodes) 87.540000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.020000\n",
      "episodes 25165\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 361500\n",
      "mean reward (50 episodes) 88.740000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 15.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25227\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 362000\n",
      "mean reward (50 episodes) 86.440000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25289\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 362500\n",
      "mean reward (50 episodes) 88.220000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25348\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 363000\n",
      "mean reward (50 episodes) 86.840000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25410\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 363500\n",
      "mean reward (50 episodes) 86.080000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25471\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 364000\n",
      "mean reward (50 episodes) 86.320000\n",
      "mean length (50 episodes) 7.880000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25534\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 364500\n",
      "mean reward (50 episodes) 87.520000\n",
      "mean length (50 episodes) 8.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25594\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 365000\n",
      "mean reward (50 episodes) 87.160000\n",
      "mean length (50 episodes) 8.640000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 7.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25652\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 365500\n",
      "mean reward (50 episodes) 86.640000\n",
      "mean length (50 episodes) 8.560000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 18.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25710\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 366000\n",
      "mean reward (50 episodes) 85.640000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25772\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 366500\n",
      "mean reward (50 episodes) 87.360000\n",
      "mean length (50 episodes) 8.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25834\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 367000\n",
      "mean reward (50 episodes) 87.220000\n",
      "mean length (50 episodes) 8.180000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25896\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 367500\n",
      "mean reward (50 episodes) 85.680000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 25958\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 368000\n",
      "mean reward (50 episodes) 85.040000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26018\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 368500\n",
      "mean reward (50 episodes) 86.940000\n",
      "mean length (50 episodes) 8.260000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26079\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 369000\n",
      "mean reward (50 episodes) 85.640000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26139\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 369500\n",
      "mean reward (50 episodes) 87.100000\n",
      "mean length (50 episodes) 8.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26200\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 370000\n",
      "mean reward (50 episodes) 87.500000\n",
      "mean length (50 episodes) 7.900000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26264\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 370500\n",
      "mean reward (50 episodes) 85.860000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26324\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 371000\n",
      "mean reward (50 episodes) 86.000000\n",
      "mean length (50 episodes) 8.600000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26382\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 371500\n",
      "mean reward (50 episodes) 86.600000\n",
      "mean length (50 episodes) 8.000000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26444\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 372000\n",
      "mean reward (50 episodes) 85.360000\n",
      "mean length (50 episodes) 8.440000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26503\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 372500\n",
      "mean reward (50 episodes) 86.980000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26565\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 373000\n",
      "mean reward (50 episodes) 88.840000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26626\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 373500\n",
      "mean reward (50 episodes) 84.920000\n",
      "mean length (50 episodes) 8.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26687\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 374000\n",
      "mean reward (50 episodes) 85.520000\n",
      "mean length (50 episodes) 8.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 67.000000\n",
      "max_episode_length (50 episodes) 16.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26745\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 374500\n",
      "mean reward (50 episodes) 86.780000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26806\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 375000\n",
      "mean reward (50 episodes) 86.700000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26867\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 375500\n",
      "mean reward (50 episodes) 86.820000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26928\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 376000\n",
      "mean reward (50 episodes) 87.460000\n",
      "mean length (50 episodes) 7.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 26990\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 376500\n",
      "mean reward (50 episodes) 88.660000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 79.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27051\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 377000\n",
      "mean reward (50 episodes) 87.720000\n",
      "mean length (50 episodes) 8.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27113\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 377500\n",
      "mean reward (50 episodes) 86.720000\n",
      "mean length (50 episodes) 8.080000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27174\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 378000\n",
      "mean reward (50 episodes) 87.760000\n",
      "mean length (50 episodes) 8.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27236\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 378500\n",
      "mean reward (50 episodes) 85.420000\n",
      "mean length (50 episodes) 8.380000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27296\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 379000\n",
      "mean reward (50 episodes) 86.240000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27357\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 379500\n",
      "mean reward (50 episodes) 86.500000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27416\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 380000\n",
      "mean reward (50 episodes) 88.260000\n",
      "mean length (50 episodes) 7.940000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27478\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 380500\n",
      "mean reward (50 episodes) 86.080000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27539\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 381000\n",
      "mean reward (50 episodes) 87.280000\n",
      "mean length (50 episodes) 8.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27599\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 381500\n",
      "mean reward (50 episodes) 88.600000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.260000\n",
      "episodes 27662\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 382000\n",
      "mean reward (50 episodes) 87.180000\n",
      "mean length (50 episodes) 8.220000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 27722\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 382500\n",
      "mean reward (50 episodes) 88.220000\n",
      "mean length (50 episodes) 7.980000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.360000\n",
      "episodes 27785\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 383000\n",
      "mean reward (50 episodes) 85.780000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.360000\n",
      "episodes 27846\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 383500\n",
      "mean reward (50 episodes) 87.900000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 27907\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 384000\n",
      "mean reward (50 episodes) 86.520000\n",
      "mean length (50 episodes) 8.280000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 27967\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 384500\n",
      "mean reward (50 episodes) 88.500000\n",
      "mean length (50 episodes) 8.100000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 13.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28028\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 385000\n",
      "mean reward (50 episodes) 87.280000\n",
      "mean length (50 episodes) 7.920000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28090\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 385500\n",
      "mean reward (50 episodes) 87.640000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28152\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 386000\n",
      "mean reward (50 episodes) 87.040000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28215\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 386500\n",
      "mean reward (50 episodes) 86.940000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 70.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28275\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 387000\n",
      "mean reward (50 episodes) 85.980000\n",
      "mean length (50 episodes) 8.620000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28334\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 387500\n",
      "mean reward (50 episodes) 88.860000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28394\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 388000\n",
      "mean reward (50 episodes) 88.460000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28454\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 388500\n",
      "mean reward (50 episodes) 87.200000\n",
      "mean length (50 episodes) 8.200000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28514\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 389000\n",
      "mean reward (50 episodes) 86.080000\n",
      "mean length (50 episodes) 8.320000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.360000\n",
      "episodes 28574\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 389500\n",
      "mean reward (50 episodes) 88.780000\n",
      "mean length (50 episodes) 8.020000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 28635\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 390000\n",
      "mean reward (50 episodes) 87.040000\n",
      "mean length (50 episodes) 8.360000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.820000\n",
      "episodes 28695\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 390500\n",
      "mean reward (50 episodes) 83.860000\n",
      "mean length (50 episodes) 8.340000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 58.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.820000\n",
      "episodes 28756\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 391000\n",
      "mean reward (50 episodes) 86.760000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.820000\n",
      "episodes 28816\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 391500\n",
      "mean reward (50 episodes) 87.640000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 28879\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 392000\n",
      "mean reward (50 episodes) 88.080000\n",
      "mean length (50 episodes) 8.120000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 49.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.820000\n",
      "episodes 28941\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 392500\n",
      "mean reward (50 episodes) 86.520000\n",
      "mean length (50 episodes) 8.480000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 61.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29001\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 393000\n",
      "mean reward (50 episodes) 87.960000\n",
      "mean length (50 episodes) 8.040000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29063\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 393500\n",
      "mean reward (50 episodes) 87.380000\n",
      "mean length (50 episodes) 8.620000\n",
      "max_episode_reward (50 episodes) 93.000000\n",
      "min_episode_length (50 episodes) 6.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 17.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29121\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 394000\n",
      "mean reward (50 episodes) 87.500000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29181\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 394500\n",
      "mean reward (50 episodes) 87.540000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29243\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 395000\n",
      "mean reward (50 episodes) 85.500000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 60.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29303\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 395500\n",
      "mean reward (50 episodes) 86.520000\n",
      "mean length (50 episodes) 8.680000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 12.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29360\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 396000\n",
      "mean reward (50 episodes) 87.900000\n",
      "mean length (50 episodes) 8.300000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 69.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29420\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 396500\n",
      "mean reward (50 episodes) 87.140000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29481\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 397000\n",
      "mean reward (50 episodes) 87.440000\n",
      "mean length (50 episodes) 7.960000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 71.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29544\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 397500\n",
      "mean reward (50 episodes) 86.860000\n",
      "mean length (50 episodes) 8.140000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29605\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 398000\n",
      "mean reward (50 episodes) 87.640000\n",
      "mean length (50 episodes) 8.160000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 11.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29665\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 398500\n",
      "mean reward (50 episodes) 86.400000\n",
      "mean length (50 episodes) 7.800000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29728\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 399000\n",
      "mean reward (50 episodes) 87.960000\n",
      "mean length (50 episodes) 8.240000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 62.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29788\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Timestep 399500\n",
      "mean reward (50 episodes) 88.540000\n",
      "mean length (50 episodes) 8.060000\n",
      "max_episode_reward (50 episodes) 95.000000\n",
      "min_episode_length (50 episodes) 5.000000\n",
      "min_episode_reward (50 episodes) 73.000000\n",
      "max_episode_length (50 episodes) 10.000000\n",
      "best mean reward 89.820000\n",
      "episodes 29849\n",
      "exploration 0.010000\n",
      "learning_rate 0.000050\n",
      "\n",
      "\n",
      "Model saved in path: option2/dqn_graph.ckpt\n",
      "3722.4170072078705\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a32adfb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a32cdbda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    # Get Atari games.\n",
    "    # benchmark = gym.benchmark_spec('Atari40M')\n",
    "    #\n",
    "    # # Change the index to select a different game.\n",
    "    # task = benchmark.tasks[3]\n",
    "    #\n",
    "    # # Run training\n",
    "#     seed = 0  # Use a seed of zero (you may want to randomize the seed!)\n",
    "#     set_global_seeds(seed)\n",
    "    # env = get_env(task, seed)\n",
    "    env = ArmEnvDQN_2(episode_max_length=100,\n",
    "                 size_x=6,\n",
    "                 size_y=4,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=100,\n",
    "                 tower_target_size=4)\n",
    "    session = get_session()\n",
    "\n",
    "    start = time.time()\n",
    "    ep_rew, ep_len = arm_learn(env, session, num_timesteps=400000)\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=ep_len,\n",
    "        episode_rewards=ep_rew)\n",
    "    plotting.plot_episode_stats(stats)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_observation(frame):\n",
    "    img_h, img_w = frame.shape[1], frame.shape[2]\n",
    "    return frame.transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n",
    "\n",
    "def main():\n",
    "    env = ArmEnvDQN(episode_max_length=200,\n",
    "                 size_x=5,\n",
    "                 size_y=5,\n",
    "                 cubes_cnt=4,\n",
    "                 scaling_coeff=3,\n",
    "                 action_minus_reward=-1,\n",
    "                 finish_reward=400,\n",
    "                 tower_target_size=4)\n",
    "    # print(env.reset())\n",
    "    session = tf.Session()\n",
    "    # First let's load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph('option_go_down.meta')\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    frame_history_len = 1\n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_shape = (img_h, img_w, frame_history_len * img_c)  # size_x, size_y,\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "#     # placeholder for current observation (or state)\n",
    "#     obs_t_ph = tf.placeholder(tf.uint8, [None] + list(input_shape))\n",
    "#     # casting to float on GPU ensures lower data transfer times.\n",
    "#     obs_t_float = tf.cast(obs_t_ph, tf.float32) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "#     pred_q = q_func(obs_t_float, num_actions, scope=\"q_func\", reuse=False)\n",
    "#     pred_ac = tf.argmax(pred_q, axis=1)\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    obs_t_float = graph.get_tensor_by_name(\"obs_t_ph:0\")\n",
    " \n",
    "    ## How to access saved operation\n",
    "    pred_ac = graph.get_tensor_by_name(\"pred_ac:0\")\n",
    "    \n",
    "    \n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    last_obs = env.reset()\n",
    "\n",
    "    for t in itertools.count():\n",
    "\n",
    "        obs = encode_observation(np.array([last_obs]))\n",
    "        action = session.run(pred_ac, {obs_t_float: [obs]})[0]\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done or episode_length == 500:\n",
    "            break\n",
    "\n",
    "        last_obs = next_obs\n",
    "    print(episode_reward, episode_length)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
